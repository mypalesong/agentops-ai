"use strict";(globalThis.webpackChunkagentops_docs=globalThis.webpackChunkagentops_docs||[]).push([[4606],{48(n,e,t){t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"integrations/langchain","title":"LangChain","description":"Integrate AgentOps with LangChain for chain and agent observability","source":"@site/docs/integrations/langchain.md","sourceDirName":"integrations","slug":"/integrations/langchain","permalink":"/agentops-ai/docs/integrations/langchain","draft":false,"unlisted":false,"editUrl":"https://github.com/agentops/agentops-docs/tree/main/docs/integrations/langchain.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"LangChain","description":"Integrate AgentOps with LangChain for chain and agent observability"},"sidebar":"docsSidebar","previous":{"title":"Anthropic","permalink":"/agentops-ai/docs/integrations/anthropic"},"next":{"title":"CrewAI","permalink":"/agentops-ai/docs/integrations/crewai"}}');var r=t(4848),s=t(8453);const i={sidebar_position:4,title:"LangChain",description:"Integrate AgentOps with LangChain for chain and agent observability"},o="LangChain Integration",l={},c=[{value:"Quick Start",id:"quick-start",level:2},{value:"Supported Components",id:"supported-components",level:2},{value:"Chat Models",id:"chat-models",level:2},{value:"OpenAI",id:"openai",level:3},{value:"Anthropic",id:"anthropic",level:3},{value:"Chains",id:"chains",level:2},{value:"Simple Chains",id:"simple-chains",level:3},{value:"Complex Chains",id:"complex-chains",level:3},{value:"Agents",id:"agents",level:2},{value:"ReAct Agent",id:"react-agent",level:3},{value:"Tool Calling Agent",id:"tool-calling-agent",level:3},{value:"Tools",id:"tools",level:2},{value:"Custom Tools",id:"custom-tools",level:3},{value:"Structured Tools",id:"structured-tools",level:3},{value:"Memory",id:"memory",level:2},{value:"RAG (Retrieval-Augmented Generation)",id:"rag-retrieval-augmented-generation",level:2},{value:"Streaming",id:"streaming",level:2},{value:"Async Support",id:"async-support",level:2},{value:"Tracked Data",id:"tracked-data",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Tag Sessions by Chain Type",id:"1-tag-sessions-by-chain-type",level:3},{value:"2. Use Verbose Mode for Debugging",id:"2-use-verbose-mode-for-debugging",level:3},{value:"3. Track Custom Events",id:"3-track-custom-events",level:3},{value:"Next Steps",id:"next-steps",level:2}];function h(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"langchain-integration",children:"LangChain Integration"})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.img,{src:"https://images.unsplash.com/photo-1639322537228-f710d846310a?w=1200&h=300&fit=crop",alt:"LangChain Integration"})}),"\n",(0,r.jsx)(e.p,{children:"AgentOps provides comprehensive integration with LangChain, tracking chains, agents, tools, and all LLM calls automatically."}),"\n",(0,r.jsx)(e.mermaid,{value:'flowchart LR\n    subgraph LangChain["LangChain Components"]\n        Chains[Chains]\n        Agents[Agents]\n        Tools[Tools]\n        Memory[Memory]\n    end\n\n    subgraph AgentOps["AgentOps Tracking"]\n        Events[Event Capture]\n        Metrics[Metrics]\n        Cost[Cost Analysis]\n    end\n\n    Chains --\x3e Events\n    Agents --\x3e Events\n    Tools --\x3e Events\n    Memory --\x3e Events\n\n    Events --\x3e Dashboard[Dashboard]\n    Metrics --\x3e Dashboard\n    Cost --\x3e Dashboard\n\n    style Chains fill:#0066FF,stroke:#3399FF,color:#fff\n    style Agents fill:#22C55E,stroke:#16A34A,color:#fff\n    style Dashboard fill:#8B5CF6,stroke:#7C3AED,color:#fff'}),"\n",(0,r.jsx)(e.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import agentops\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\n# Initialize AgentOps\nagentops.init(api_key="your-agentops-api-key")\n\n# Create LangChain components\nllm = ChatOpenAI(model="gpt-4")\n\n# Use normally - all calls tracked\nresponse = llm.invoke([HumanMessage(content="Hello!")])\nprint(response.content)\n\nagentops.end_session(end_state="Success")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"supported-components",children:"Supported Components"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Component"}),(0,r.jsx)(e.th,{children:"Tracking Level"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Chat Models"}),(0,r.jsx)(e.td,{children:"Full"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"LLMs"}),(0,r.jsx)(e.td,{children:"Full"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Chains"}),(0,r.jsx)(e.td,{children:"Full"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Agents"}),(0,r.jsx)(e.td,{children:"Full"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Tools"}),(0,r.jsx)(e.td,{children:"Full"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Memory"}),(0,r.jsx)(e.td,{children:"Partial"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:"Retrievers"}),(0,r.jsx)(e.td,{children:"Partial"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"chat-models",children:"Chat Models"}),"\n",(0,r.jsx)(e.h3,{id:"openai",children:"OpenAI"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(\n    model="gpt-4",\n    temperature=0.7\n)\n\nresponse = llm.invoke([\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is LangChain?"}\n])\n'})}),"\n",(0,r.jsx)(e.h3,{id:"anthropic",children:"Anthropic"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_anthropic import ChatAnthropic\n\nllm = ChatAnthropic(\n    model="claude-3-sonnet-20240229",\n    temperature=0.7\n)\n\nresponse = llm.invoke("Explain machine learning")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"chains",children:"Chains"}),"\n",(0,r.jsx)(e.h3,{id:"simple-chains",children:"Simple Chains"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n\nllm = ChatOpenAI(model="gpt-4")\nprompt = ChatPromptTemplate.from_template("Tell me about {topic}")\nparser = StrOutputParser()\n\n# Create chain\nchain = prompt | llm | parser\n\n# Run chain - all steps tracked\nresult = chain.invoke({"topic": "artificial intelligence"})\n'})}),"\n",(0,r.jsx)(e.h3,{id:"complex-chains",children:"Complex Chains"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n\n# Parallel execution\nanalysis_chain = RunnableParallel(\n    summary=prompt_summary | llm | parser,\n    sentiment=prompt_sentiment | llm | parser,\n    keywords=prompt_keywords | llm | parser\n)\n\nresult = analysis_chain.invoke({"text": document})\n'})}),"\n",(0,r.jsx)(e.h2,{id:"agents",children:"Agents"}),"\n",(0,r.jsx)(e.h3,{id:"react-agent",children:"ReAct Agent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain.agents import create_react_agent, AgentExecutor\nfrom langchain.tools import Tool\nfrom langchain import hub\n\n# Initialize AgentOps\nagentops.init(api_key="your-agentops-api-key")\n\n# Define tools\ndef search(query):\n    return f"Results for: {query}"\n\ntools = [\n    Tool(\n        name="Search",\n        func=search,\n        description="Search for information"\n    )\n]\n\n# Create agent\nllm = ChatOpenAI(model="gpt-4")\nprompt = hub.pull("hwchase17/react")\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n\n# Run agent - all steps tracked\nresult = agent_executor.invoke({"input": "What is the capital of France?"})\n'})}),"\n",(0,r.jsx)(e.h3,{id:"tool-calling-agent",children:"Tool Calling Agent"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain.agents import create_tool_calling_agent, AgentExecutor\nfrom langchain_core.tools import tool\n\n@tool\ndef multiply(a: int, b: int) -> int:\n    """Multiply two numbers."""\n    return a * b\n\n@tool\ndef add(a: int, b: int) -> int:\n    """Add two numbers."""\n    return a + b\n\ntools = [multiply, add]\nllm = ChatOpenAI(model="gpt-4")\nprompt = ChatPromptTemplate.from_messages([\n    ("system", "You are a helpful math assistant."),\n    ("human", "{input}"),\n    ("placeholder", "{agent_scratchpad}")\n])\n\nagent = create_tool_calling_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools)\n\nresult = agent_executor.invoke({"input": "What is 5 * 3 + 2?"})\n'})}),"\n",(0,r.jsx)(e.h2,{id:"tools",children:"Tools"}),"\n",(0,r.jsx)(e.h3,{id:"custom-tools",children:"Custom Tools"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_core.tools import tool\n\n@tool\ndef get_weather(location: str) -> str:\n    """Get the weather for a location."""\n    # Tool calls are automatically tracked\n    return f"Weather in {location}: Sunny, 72\xb0F"\n\n@tool\ndef search_database(query: str) -> list:\n    """Search the database."""\n    # AgentOps tracks input/output\n    return ["Result 1", "Result 2"]\n'})}),"\n",(0,r.jsx)(e.h3,{id:"structured-tools",children:"Structured Tools"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_core.tools import StructuredTool\nfrom pydantic import BaseModel\n\nclass SearchInput(BaseModel):\n    query: str\n    max_results: int = 10\n\ndef search_func(query: str, max_results: int = 10) -> list:\n    return [f"Result {i}" for i in range(max_results)]\n\nsearch_tool = StructuredTool.from_function(\n    func=search_func,\n    name="Search",\n    description="Search for information",\n    args_schema=SearchInput\n)\n'})}),"\n",(0,r.jsx)(e.h2,{id:"memory",children:"Memory"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nllm = ChatOpenAI(model="gpt-4")\nmemory = ConversationBufferMemory()\n\nconversation = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose=True\n)\n\n# Conversation with memory - all turns tracked\nconversation.predict(input="Hi, I\'m Alice")\nconversation.predict(input="What\'s my name?")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"rag-retrieval-augmented-generation",children:"RAG (Retrieval-Augmented Generation)"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\n\n# Initialize AgentOps\nagentops.init(api_key="your-agentops-api-key")\n\n# Create vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_texts(documents, embeddings)\nretriever = vectorstore.as_retriever()\n\n# Create RAG chain\ntemplate = """Answer based on context:\n{context}\n\nQuestion: {question}\n"""\nprompt = ChatPromptTemplate.from_template(template)\nllm = ChatOpenAI(model="gpt-4")\n\nrag_chain = (\n    {"context": retriever, "question": RunnablePassthrough()}\n    | prompt\n    | llm\n)\n\n# Run RAG - retrieval and generation tracked\nresult = rag_chain.invoke("What is the main topic?")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"streaming",children:"Streaming"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model="gpt-4", streaming=True)\n\n# Stream responses - complete response tracked after streaming\nfor chunk in llm.stream("Tell me a story"):\n    print(chunk.content, end="")\n'})}),"\n",(0,r.jsx)(e.h2,{id:"async-support",children:"Async Support"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'import asyncio\nfrom langchain_openai import ChatOpenAI\n\nasync def main():\n    llm = ChatOpenAI(model="gpt-4")\n\n    # Async calls tracked\n    response = await llm.ainvoke("Hello!")\n    print(response.content)\n\nasyncio.run(main())\n'})}),"\n",(0,r.jsx)(e.h2,{id:"tracked-data",children:"Tracked Data"}),"\n",(0,r.jsx)(e.p,{children:"AgentOps captures for LangChain:"}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Component"}),(0,r.jsx)(e.th,{children:"Data Captured"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Chains"})}),(0,r.jsx)(e.td,{children:"Input, output, intermediate steps"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Agents"})}),(0,r.jsx)(e.td,{children:"Thoughts, actions, observations"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Tools"})}),(0,r.jsx)(e.td,{children:"Name, input, output, duration"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"LLMs"})}),(0,r.jsx)(e.td,{children:"Prompts, responses, tokens, cost"})]})]})]}),"\n",(0,r.jsx)(e.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,r.jsx)(e.h3,{id:"1-tag-sessions-by-chain-type",children:"1. Tag Sessions by Chain Type"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'session = agentops.start_session(tags=[\n    "langchain",\n    "rag-chain",\n    "production"\n])\n'})}),"\n",(0,r.jsx)(e.h3,{id:"2-use-verbose-mode-for-debugging",children:"2. Use Verbose Mode for Debugging"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:"agent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n    verbose=True  # Helps with debugging\n)\n"})}),"\n",(0,r.jsx)(e.h3,{id:"3-track-custom-events",children:"3. Track Custom Events"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# Before chain execution\nagentops.record(Event(\n    event_type="chain_start",\n    params={"chain_type": "rag", "query": query}\n))\n\nresult = chain.invoke({"query": query})\n\n# After chain execution\nagentops.record(Event(\n    event_type="chain_complete",\n    params={"result_length": len(result)}\n))\n'})}),"\n",(0,r.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"/docs/integrations/crewai",children:"CrewAI"})," - Multi-agent framework"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"/docs/advanced/multi-agent",children:"Advanced Chains"})," - Complex chain patterns"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"/docs/guides/best-practices",children:"Best Practices"})," - Optimization tips"]}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(h,{...n})}):h(n)}},8453(n,e,t){t.d(e,{R:()=>i,x:()=>o});var a=t(6540);const r={},s=a.createContext(r);function i(n){const e=a.useContext(s);return a.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:i(n.components),a.createElement(s.Provider,{value:e},n.children)}}}]);