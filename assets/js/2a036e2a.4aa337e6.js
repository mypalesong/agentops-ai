"use strict";(globalThis.webpackChunkagentops_docs=globalThis.webpackChunkagentops_docs||[]).push([[9307],{7674(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"integrations/openai","title":"OpenAI","description":"Integrate AgentOps with OpenAI GPT models","source":"@site/docs/integrations/openai.md","sourceDirName":"integrations","slug":"/integrations/openai","permalink":"/agentops-ai/docs/integrations/openai","draft":false,"unlisted":false,"editUrl":"https://github.com/agentops/agentops-docs/tree/main/docs/integrations/openai.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"OpenAI","description":"Integrate AgentOps with OpenAI GPT models"},"sidebar":"docsSidebar","previous":{"title":"Overview","permalink":"/agentops-ai/docs/integrations/overview"},"next":{"title":"Anthropic","permalink":"/agentops-ai/docs/integrations/anthropic"}}');var i=t(4848),r=t(8453);const o={sidebar_position:2,title:"OpenAI",description:"Integrate AgentOps with OpenAI GPT models"},a="OpenAI Integration",c={},l=[{value:"Quick Start",id:"quick-start",level:2},{value:"Supported Features",id:"supported-features",level:2},{value:"Chat Completions",id:"chat-completions",level:3},{value:"Streaming",id:"streaming",level:3},{value:"Function Calling",id:"function-calling",level:3},{value:"Vision (GPT-4V)",id:"vision-gpt-4v",level:3},{value:"Async Support",id:"async-support",level:3},{value:"Embeddings",id:"embeddings",level:3},{value:"Tracked Data",id:"tracked-data",level:2},{value:"Pricing",id:"pricing",level:2},{value:"Advanced Usage",id:"advanced-usage",level:2},{value:"Conversation Tracking",id:"conversation-tracking",level:3},{value:"Retry with Different Models",id:"retry-with-different-models",level:3},{value:"Batch Processing",id:"batch-processing",level:3},{value:"Error Handling",id:"error-handling",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"1. Use Appropriate Models",id:"1-use-appropriate-models",level:3},{value:"2. Set Token Limits",id:"2-set-token-limits",level:3},{value:"3. Tag Sessions by Purpose",id:"3-tag-sessions-by-purpose",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"openai-integration",children:"OpenAI Integration"})}),"\n",(0,i.jsx)(n.p,{children:"AgentOps provides seamless integration with OpenAI's GPT models, automatically capturing all API calls with zero configuration."}),"\n",(0,i.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import agentops\nfrom openai import OpenAI\n\n# Initialize AgentOps\nagentops.init(api_key="your-agentops-api-key")\n\n# Create OpenAI client\nclient = OpenAI()\n\n# Make API calls - automatically tracked\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[\n        {"role": "system", "content": "You are a helpful assistant."},\n        {"role": "user", "content": "What is the capital of France?"}\n    ]\n)\n\nprint(response.choices[0].message.content)\nagentops.end_session(end_state="Success")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"supported-features",children:"Supported Features"}),"\n",(0,i.jsx)(n.h3,{id:"chat-completions",children:"Chat Completions"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Standard chat completion\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": "Hello!"}],\n    temperature=0.7,\n    max_tokens=500\n)\n\n# With system message\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[\n        {"role": "system", "content": "You are a Python expert."},\n        {"role": "user", "content": "How do I read a file?"}\n    ]\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"streaming",children:"Streaming"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Streaming responses\nstream = client.chat.completions.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": "Tell me a story"}],\n    stream=True\n)\n\nfor chunk in stream:\n    content = chunk.choices[0].delta.content\n    if content:\n        print(content, end="")\n\n# AgentOps records the complete response after streaming ends\n'})}),"\n",(0,i.jsx)(n.h3,{id:"function-calling",children:"Function Calling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Define functions\ntools = [\n    {\n        "type": "function",\n        "function": {\n            "name": "get_weather",\n            "description": "Get the current weather",\n            "parameters": {\n                "type": "object",\n                "properties": {\n                    "location": {\n                        "type": "string",\n                        "description": "City name"\n                    }\n                },\n                "required": ["location"]\n            }\n        }\n    }\n]\n\n# Call with tools\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": "What\'s the weather in Tokyo?"}],\n    tools=tools,\n    tool_choice="auto"\n)\n\n# Tool calls are tracked\nif response.choices[0].message.tool_calls:\n    for tool_call in response.choices[0].message.tool_calls:\n        print(f"Function: {tool_call.function.name}")\n        print(f"Arguments: {tool_call.function.arguments}")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"vision-gpt-4v",children:"Vision (GPT-4V)"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Image analysis\nresponse = client.chat.completions.create(\n    model="gpt-4-vision-preview",\n    messages=[\n        {\n            "role": "user",\n            "content": [\n                {"type": "text", "text": "What\'s in this image?"},\n                {\n                    "type": "image_url",\n                    "image_url": {\n                        "url": "https://example.com/image.jpg"\n                    }\n                }\n            ]\n        }\n    ],\n    max_tokens=300\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"async-support",children:"Async Support"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom openai import AsyncOpenAI\n\nagentops.init(api_key="your-agentops-api-key")\n\nasync def main():\n    client = AsyncOpenAI()\n\n    # Async calls are tracked\n    response = await client.chat.completions.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": "Hello!"}]\n    )\n\n    print(response.choices[0].message.content)\n\nasyncio.run(main())\n'})}),"\n",(0,i.jsx)(n.h3,{id:"embeddings",children:"Embeddings"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Generate embeddings\nresponse = client.embeddings.create(\n    model="text-embedding-3-small",\n    input="The quick brown fox jumps over the lazy dog"\n)\n\n# Embedding requests are tracked\nembedding = response.data[0].embedding\n'})}),"\n",(0,i.jsx)(n.h2,{id:"tracked-data",children:"Tracked Data"}),"\n",(0,i.jsx)(n.p,{children:"For each OpenAI call, AgentOps captures:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Field"}),(0,i.jsx)(n.th,{children:"Description"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"model"})}),(0,i.jsx)(n.td,{children:"Model used (gpt-4, gpt-3.5-turbo, etc.)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"messages"})}),(0,i.jsx)(n.td,{children:"Full conversation context"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"completion"})}),(0,i.jsx)(n.td,{children:"Response content"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"prompt_tokens"})}),(0,i.jsx)(n.td,{children:"Input token count"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"completion_tokens"})}),(0,i.jsx)(n.td,{children:"Output token count"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"total_tokens"})}),(0,i.jsx)(n.td,{children:"Total tokens used"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"cost"})}),(0,i.jsx)(n.td,{children:"Calculated cost in USD"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"latency_ms"})}),(0,i.jsx)(n.td,{children:"Response time"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"finish_reason"})}),(0,i.jsx)(n.td,{children:"Why generation stopped"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"tool_calls"})}),(0,i.jsx)(n.td,{children:"Any function calls made"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"pricing",children:"Pricing"}),"\n",(0,i.jsx)(n.p,{children:"AgentOps automatically calculates costs based on current OpenAI pricing:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Model"}),(0,i.jsx)(n.th,{children:"Input (per 1K)"}),(0,i.jsx)(n.th,{children:"Output (per 1K)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"gpt-4"}),(0,i.jsx)(n.td,{children:"$0.03"}),(0,i.jsx)(n.td,{children:"$0.06"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"gpt-4-turbo"}),(0,i.jsx)(n.td,{children:"$0.01"}),(0,i.jsx)(n.td,{children:"$0.03"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"gpt-4o"}),(0,i.jsx)(n.td,{children:"$0.005"}),(0,i.jsx)(n.td,{children:"$0.015"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"gpt-3.5-turbo"}),(0,i.jsx)(n.td,{children:"$0.0005"}),(0,i.jsx)(n.td,{children:"$0.0015"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"advanced-usage",children:"Advanced Usage"}),"\n",(0,i.jsx)(n.h3,{id:"conversation-tracking",children:"Conversation Tracking"}),"\n",(0,i.jsx)(n.p,{children:"Track multi-turn conversations:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import agentops\nfrom openai import OpenAI\n\nagentops.init(api_key="your-agentops-api-key")\nclient = OpenAI()\n\n# Start session for conversation\nsession = agentops.start_session(tags=["conversation", "support"])\n\nconversation = []\n\ndef chat(user_message):\n    conversation.append({"role": "user", "content": user_message})\n\n    response = client.chat.completions.create(\n        model="gpt-4",\n        messages=conversation\n    )\n\n    assistant_message = response.choices[0].message.content\n    conversation.append({"role": "assistant", "content": assistant_message})\n\n    return assistant_message\n\n# Multi-turn conversation\nchat("Hi, I need help with my order")\nchat("Order number is 12345")\nchat("I want to return it")\n\nsession.end(end_state="Success")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"retry-with-different-models",children:"Retry with Different Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def call_with_fallback(messages):\n    """Try GPT-4, fall back to GPT-3.5 on rate limit."""\n    try:\n        return client.chat.completions.create(\n            model="gpt-4",\n            messages=messages\n        )\n    except openai.RateLimitError:\n        agentops.record(Event(\n            event_type="fallback",\n            params={"from": "gpt-4", "to": "gpt-3.5-turbo", "reason": "rate_limit"}\n        ))\n        return client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=messages\n        )\n'})}),"\n",(0,i.jsx)(n.h3,{id:"batch-processing",children:"Batch Processing"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import agentops\nfrom openai import OpenAI\n\nagentops.init(api_key="your-agentops-api-key", auto_start_session=False)\nclient = OpenAI()\n\nqueries = [\n    "What is Python?",\n    "What is JavaScript?",\n    "What is Rust?"\n]\n\nfor i, query in enumerate(queries):\n    session = agentops.start_session(tags=[f"batch-{i}"])\n\n    response = client.chat.completions.create(\n        model="gpt-3.5-turbo",\n        messages=[{"role": "user", "content": query}]\n    )\n\n    session.end(end_state="Success")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import agentops\nfrom openai import OpenAI, APIError, RateLimitError\n\nagentops.init(api_key="your-agentops-api-key")\nclient = OpenAI()\n\ntry:\n    response = client.chat.completions.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": "Hello!"}]\n    )\n    agentops.end_session(end_state="Success")\n\nexcept RateLimitError as e:\n    agentops.record(ErrorEvent(\n        error_type="RateLimitError",\n        details=str(e)\n    ))\n    agentops.end_session(end_state="Fail", end_state_reason="Rate limited")\n\nexcept APIError as e:\n    agentops.record(ErrorEvent(\n        error_type="APIError",\n        details=str(e)\n    ))\n    agentops.end_session(end_state="Fail", end_state_reason=str(e))\n'})}),"\n",(0,i.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,i.jsx)(n.h3,{id:"1-use-appropriate-models",children:"1. Use Appropriate Models"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Use GPT-4 for complex tasks\ncomplex_response = client.chat.completions.create(\n    model="gpt-4",\n    messages=[{"role": "user", "content": complex_query}]\n)\n\n# Use GPT-3.5 for simple tasks (cheaper)\nsimple_response = client.chat.completions.create(\n    model="gpt-3.5-turbo",\n    messages=[{"role": "user", "content": simple_query}]\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-set-token-limits",children:"2. Set Token Limits"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'response = client.chat.completions.create(\n    model="gpt-4",\n    messages=messages,\n    max_tokens=500  # Prevent runaway costs\n)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-tag-sessions-by-purpose",children:"3. Tag Sessions by Purpose"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'session = agentops.start_session(tags=[\n    "openai",\n    "gpt-4",\n    "customer-support",\n    "production"\n])\n'})}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/advanced/custom-events",children:"Function Calling Guide"})," - Advanced function calling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/features/cost-tracking",children:"Cost Tracking"})," - Monitor OpenAI costs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/guides/best-practices",children:"Best Practices"})," - Optimization tips"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>a});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);