"use strict";(globalThis.webpackChunkagentops_docs=globalThis.webpackChunkagentops_docs||[]).push([[8517],{3341(e){e.exports=JSON.parse('{"permalink":"/docusaurus-guide-3/blog/evaluating-ai-agents-metrics-that-matter","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-07-15-agent-evaluation.md","source":"@site/blog/2024-07-15-agent-evaluation.md","title":"Evaluating AI Agents: The Metrics That Actually Matter","description":"Move beyond accuracy scores. Learn how to evaluate AI agents using metrics that reflect real-world performance and business value.","date":"2024-07-15T00:00:00.000Z","tags":[{"inline":true,"label":"evaluation","permalink":"/docusaurus-guide-3/blog/tags/evaluation"},{"inline":true,"label":"metrics","permalink":"/docusaurus-guide-3/blog/tags/metrics"},{"inline":true,"label":"testing","permalink":"/docusaurus-guide-3/blog/tags/testing"},{"inline":true,"label":"quality","permalink":"/docusaurus-guide-3/blog/tags/quality"}],"readingTime":8.62,"hasTruncateMarker":true,"authors":[{"name":"Michael Torres","title":"Head of AI Research at AgentOps","url":"https://github.com/michaeltorres","image_url":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"evaluating-ai-agents-metrics-that-matter","title":"Evaluating AI Agents: The Metrics That Actually Matter","authors":[{"name":"Michael Torres","title":"Head of AI Research at AgentOps","url":"https://github.com/michaeltorres","image_url":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face"}],"tags":["evaluation","metrics","testing","quality"],"description":"Move beyond accuracy scores. Learn how to evaluate AI agents using metrics that reflect real-world performance and business value.","image":"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"Prompt Engineering for AI Agents: Beyond Basic Prompting","permalink":"/docusaurus-guide-3/blog/prompt-engineering-for-ai-agents"},"nextItem":{"title":"CrewAI vs AutoGen: A Deep Comparison for Multi-Agent Systems","permalink":"/docusaurus-guide-3/blog/crewai-vs-autogen-comparison"}}')},7142(e,n,t){t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>a,toc:()=>c});var a=t(3341),s=t(4848),r=t(8453);const i={slug:"evaluating-ai-agents-metrics-that-matter",title:"Evaluating AI Agents: The Metrics That Actually Matter",authors:[{name:"Michael Torres",title:"Head of AI Research at AgentOps",url:"https://github.com/michaeltorres",image_url:"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face"}],tags:["evaluation","metrics","testing","quality"],description:"Move beyond accuracy scores. Learn how to evaluate AI agents using metrics that reflect real-world performance and business value.",image:"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=1200&h=630&fit=crop"},l="Evaluating AI Agents: The Metrics That Actually Matter",o={authorsImageUrls:[void 0]},c=[{value:"Beyond Accuracy: The Agent Evaluation Framework",id:"beyond-accuracy-the-agent-evaluation-framework",level:2},{value:"Quality Metrics",id:"quality-metrics",level:2},{value:"Task Completion Rate",id:"task-completion-rate",level:3},{value:"Output Quality Score",id:"output-quality-score",level:3},{value:"Reasoning Quality",id:"reasoning-quality",level:3},{value:"Efficiency Metrics",id:"efficiency-metrics",level:2},{value:"Latency Distribution",id:"latency-distribution",level:3},{value:"Token Efficiency",id:"token-efficiency",level:3},{value:"Reliability Metrics",id:"reliability-metrics",level:2},{value:"Success Rate Over Time",id:"success-rate-over-time",level:3},{value:"Error Analysis",id:"error-analysis",level:3},{value:"Business Metrics",id:"business-metrics",level:2},{value:"ROI Calculator",id:"roi-calculator",level:3},{value:"Building an Evaluation Pipeline",id:"building-an-evaluation-pipeline",level:2},{value:"Conclusion",id:"conclusion",level:2}];function u(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=1200&h=400&fit=crop",alt:"Agent Evaluation"})}),"\n",(0,s.jsx)(n.p,{children:"\"Our agent has 95% accuracy!\" Great, but is it actually useful? Traditional ML metrics don't capture the full picture of agent performance. Let's explore the metrics that truly matter."}),"\n",(0,s.jsx)(n.h2,{id:"beyond-accuracy-the-agent-evaluation-framework",children:"Beyond Accuracy: The Agent Evaluation Framework"}),"\n",(0,s.jsx)(n.p,{children:"AI agent evaluation requires a multi-dimensional approach:"}),"\n",(0,s.jsx)(n.mermaid,{value:'graph TB\n    subgraph "Agent Evaluation Framework"\n        A[Quality Metrics] --\x3e E[Overall Score]\n        B[Efficiency Metrics] --\x3e E\n        C[Reliability Metrics] --\x3e E\n        D[Business Metrics] --\x3e E\n\n        A --\x3e A1[Task Completion]\n        A --\x3e A2[Output Quality]\n        A --\x3e A3[Reasoning Quality]\n\n        B --\x3e B1[Latency]\n        B --\x3e B2[Token Efficiency]\n        B --\x3e B3[Cost per Task]\n\n        C --\x3e C1[Success Rate]\n        C --\x3e C2[Error Recovery]\n        C --\x3e C3[Consistency]\n\n        D --\x3e D1[User Satisfaction]\n        D --\x3e D2[Business Impact]\n        D --\x3e D3[ROI]\n    end\n\n    style A fill:#0066FF,stroke:#3399FF,color:#fff\n    style B fill:#22C55E,stroke:#16A34A,color:#fff\n    style C fill:#F59E0B,stroke:#D97706,color:#fff\n    style D fill:#8B5CF6,stroke:#7C3AED,color:#fff'}),"\n",(0,s.jsx)(n.h2,{id:"quality-metrics",children:"Quality Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"task-completion-rate",children:"Task Completion Rate"}),"\n",(0,s.jsx)(n.p,{children:'Not just "did it finish?" but "did it finish correctly?"'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from dataclasses import dataclass\nfrom enum import Enum\nimport agentops\n\nclass CompletionStatus(Enum):\n    COMPLETE = "complete"\n    PARTIAL = "partial"\n    FAILED = "failed"\n    TIMEOUT = "timeout"\n\n@dataclass\nclass TaskResult:\n    status: CompletionStatus\n    output: str\n    required_elements: list[str]\n    found_elements: list[str]\n\nclass TaskEvaluator:\n    """Evaluate task completion quality."""\n\n    def evaluate(self, task: str, result: TaskResult) -> dict:\n        # Calculate completion score\n        if result.required_elements:\n            completion_rate = len(result.found_elements) / len(result.required_elements)\n        else:\n            completion_rate = 1.0 if result.status == CompletionStatus.COMPLETE else 0.0\n\n        evaluation = {\n            "status": result.status.value,\n            "completion_rate": completion_rate,\n            "missing_elements": [\n                e for e in result.required_elements\n                if e not in result.found_elements\n            ]\n        }\n\n        agentops.record(ActionEvent(\n            action_type="task_evaluation",\n            params=evaluation\n        ))\n\n        return evaluation\n'})}),"\n",(0,s.jsx)(n.h3,{id:"output-quality-score",children:"Output Quality Score"}),"\n",(0,s.jsx)(n.p,{children:"Measure the quality of what the agent produces:"}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph "Output Quality Assessment"\n        A[Agent Output] --\x3e B[Factual Accuracy]\n        A --\x3e C[Relevance]\n        A --\x3e D[Completeness]\n        A --\x3e E[Format Compliance]\n\n        B --\x3e F[Quality Score]\n        C --\x3e F\n        D --\x3e F\n        E --\x3e F\n    end\n\n    style F fill:#22C55E,stroke:#16A34A,color:#fff'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class OutputQualityEvaluator:\n    """Evaluate the quality of agent outputs."""\n\n    def __init__(self, evaluator_llm):\n        self.evaluator = evaluator_llm\n\n    async def evaluate(self, task: str, output: str, criteria: dict) -> dict:\n        """\n        Evaluate output quality using LLM-as-judge.\n\n        criteria example:\n        {\n            "factual_accuracy": "Output contains only verifiable facts",\n            "relevance": "Output directly addresses the task",\n            "completeness": "Output covers all aspects of the task",\n            "format": "Output follows the requested format"\n        }\n        """\n        scores = {}\n\n        for criterion, description in criteria.items():\n            prompt = f"""\n            Task: {task}\n            Output: {output}\n\n            Evaluate the output on this criterion:\n            {criterion}: {description}\n\n            Score from 1-5 (5 being best) and explain briefly.\n            Format: SCORE: X\n            REASON: ...\n            """\n\n            result = await self.evaluator.generate(prompt)\n            score = self._extract_score(result)\n            scores[criterion] = score\n\n        # Calculate weighted average\n        weights = {"factual_accuracy": 0.4, "relevance": 0.3,\n                   "completeness": 0.2, "format": 0.1}\n        weighted_score = sum(\n            scores.get(k, 0) * v for k, v in weights.items()\n        ) / sum(weights.values())\n\n        evaluation = {\n            "individual_scores": scores,\n            "weighted_score": weighted_score,\n            "max_score": 5.0\n        }\n\n        agentops.record(ActionEvent(\n            action_type="quality_evaluation",\n            params=evaluation\n        ))\n\n        return evaluation\n'})}),"\n",(0,s.jsx)(n.h3,{id:"reasoning-quality",children:"Reasoning Quality"}),"\n",(0,s.jsx)(n.p,{children:"Evaluate the agent's decision-making process:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ReasoningEvaluator:\n    """Evaluate the quality of agent reasoning."""\n\n    def evaluate_reasoning_chain(self, steps: list[dict]) -> dict:\n        """\n        Evaluate a chain of reasoning steps.\n\n        Each step should have:\n        - thought: The agent\'s reasoning\n        - action: What action was taken\n        - result: The outcome\n        """\n        metrics = {\n            "logical_consistency": self._check_consistency(steps),\n            "efficiency": self._check_efficiency(steps),\n            "grounding": self._check_grounding(steps),\n            "adaptability": self._check_adaptability(steps)\n        }\n\n        agentops.record(ActionEvent(\n            action_type="reasoning_evaluation",\n            params=metrics\n        ))\n\n        return metrics\n\n    def _check_consistency(self, steps: list[dict]) -> float:\n        """Check if reasoning is logically consistent."""\n        # Look for contradictions between steps\n        contradictions = 0\n        for i, step in enumerate(steps[1:], 1):\n            if self._contradicts_previous(step, steps[:i]):\n                contradictions += 1\n\n        return 1.0 - (contradictions / max(len(steps) - 1, 1))\n\n    def _check_efficiency(self, steps: list[dict]) -> float:\n        """Check if the agent took unnecessary steps."""\n        # Compare actual steps to estimated minimum\n        estimated_minimum = self._estimate_minimum_steps(steps)\n        efficiency = estimated_minimum / len(steps)\n        return min(efficiency, 1.0)\n\n    def _check_grounding(self, steps: list[dict]) -> float:\n        """Check if decisions are based on evidence."""\n        grounded_steps = sum(\n            1 for step in steps\n            if step.get("evidence") or step.get("source")\n        )\n        return grounded_steps / len(steps)\n\n    def _check_adaptability(self, steps: list[dict]) -> float:\n        """Check if agent adapted to new information."""\n        adaptations = sum(\n            1 for i, step in enumerate(steps[1:], 1)\n            if self._shows_adaptation(step, steps[i-1])\n        )\n        opportunities = sum(\n            1 for step in steps\n            if step.get("new_information")\n        )\n        return adaptations / max(opportunities, 1)\n'})}),"\n",(0,s.jsx)(n.h2,{id:"efficiency-metrics",children:"Efficiency Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"latency-distribution",children:"Latency Distribution"}),"\n",(0,s.jsx)(n.p,{children:"Track not just average latency, but the full distribution:"}),"\n",(0,s.jsx)(n.mermaid,{value:'xychart-beta\n    title "Agent Response Latency Distribution"\n    x-axis ["p50", "p75", "p90", "p95", "p99"]\n    y-axis "Latency (ms)" 0 --\x3e 5000\n    bar [450, 780, 1200, 2100, 4500]'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom collections import defaultdict\n\nclass LatencyTracker:\n    """Track and analyze agent latency."""\n\n    def __init__(self):\n        self.latencies = defaultdict(list)\n\n    def record(self, operation: str, latency_ms: float):\n        """Record a latency measurement."""\n        self.latencies[operation].append(latency_ms)\n\n        agentops.record(ActionEvent(\n            action_type="latency_recorded",\n            params={\n                "operation": operation,\n                "latency_ms": latency_ms\n            }\n        ))\n\n    def get_percentiles(self, operation: str) -> dict:\n        """Get latency percentiles for an operation."""\n        data = self.latencies[operation]\n        if not data:\n            return {}\n\n        return {\n            "p50": np.percentile(data, 50),\n            "p75": np.percentile(data, 75),\n            "p90": np.percentile(data, 90),\n            "p95": np.percentile(data, 95),\n            "p99": np.percentile(data, 99),\n            "mean": np.mean(data),\n            "std": np.std(data)\n        }\n\n    def detect_anomalies(self, operation: str, threshold_std: float = 2.0) -> list:\n        """Detect anomalous latencies."""\n        data = self.latencies[operation]\n        mean = np.mean(data)\n        std = np.std(data)\n\n        anomalies = [\n            (i, lat) for i, lat in enumerate(data)\n            if abs(lat - mean) > threshold_std * std\n        ]\n\n        return anomalies\n'})}),"\n",(0,s.jsx)(n.h3,{id:"token-efficiency",children:"Token Efficiency"}),"\n",(0,s.jsx)(n.p,{children:"Measure how efficiently the agent uses tokens:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class TokenEfficiencyAnalyzer:\n    """Analyze token usage efficiency."""\n\n    def analyze(self, session_data: dict) -> dict:\n        """\n        Analyze token efficiency for a session.\n\n        session_data should contain:\n        - llm_calls: List of LLM call data\n        - task_complexity: Estimated task complexity\n        - output_quality: Quality score of final output\n        """\n        total_tokens = sum(\n            call["input_tokens"] + call["output_tokens"]\n            for call in session_data["llm_calls"]\n        )\n\n        # Calculate efficiency metrics\n        metrics = {\n            "total_tokens": total_tokens,\n            "tokens_per_quality_point": total_tokens / max(session_data["output_quality"], 0.1),\n            "input_output_ratio": self._calc_io_ratio(session_data["llm_calls"]),\n            "redundancy_score": self._calc_redundancy(session_data["llm_calls"]),\n            "estimated_optimal_tokens": self._estimate_optimal(session_data),\n            "efficiency_percentage": None  # Calculated below\n        }\n\n        metrics["efficiency_percentage"] = (\n            metrics["estimated_optimal_tokens"] / total_tokens * 100\n        )\n\n        agentops.record(ActionEvent(\n            action_type="token_efficiency",\n            params=metrics\n        ))\n\n        return metrics\n\n    def _calc_redundancy(self, llm_calls: list) -> float:\n        """Calculate how much content is repeated across calls."""\n        # Simplified: check for repeated prompt content\n        prompts = [call["prompt"] for call in llm_calls]\n        unique_content = set()\n        total_content = 0\n\n        for prompt in prompts:\n            words = prompt.split()\n            unique_content.update(words)\n            total_content += len(words)\n\n        return 1 - (len(unique_content) / max(total_content, 1))\n'})}),"\n",(0,s.jsx)(n.h2,{id:"reliability-metrics",children:"Reliability Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"success-rate-over-time",children:"Success Rate Over Time"}),"\n",(0,s.jsx)(n.p,{children:"Track how success rate changes:"}),"\n",(0,s.jsx)(n.mermaid,{value:'xychart-beta\n    title "Agent Success Rate Trend"\n    x-axis ["Week 1", "Week 2", "Week 3", "Week 4"]\n    y-axis "Success Rate (%)" 80 --\x3e 100\n    line [92, 94, 93, 96]'}),"\n",(0,s.jsx)(n.h3,{id:"error-analysis",children:"Error Analysis"}),"\n",(0,s.jsx)(n.p,{children:"Categorize and track error patterns:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class ErrorAnalyzer:\n    """Analyze agent error patterns."""\n\n    ERROR_CATEGORIES = {\n        "hallucination": ["incorrect fact", "made up", "fabricated"],\n        "incomplete": ["missing", "partial", "truncated"],\n        "off_topic": ["irrelevant", "unrelated", "tangent"],\n        "format_error": ["wrong format", "invalid json", "parse error"],\n        "timeout": ["timed out", "too slow", "deadline exceeded"],\n        "tool_failure": ["tool error", "api failed", "external service"]\n    }\n\n    def __init__(self):\n        self.error_counts = defaultdict(int)\n        self.error_examples = defaultdict(list)\n\n    def categorize_error(self, error_message: str, context: dict) -> str:\n        """Categorize an error into predefined categories."""\n        error_lower = error_message.lower()\n\n        for category, keywords in self.ERROR_CATEGORIES.items():\n            if any(kw in error_lower for kw in keywords):\n                self.error_counts[category] += 1\n                self.error_examples[category].append({\n                    "message": error_message,\n                    "context": context\n                })\n\n                agentops.record(ActionEvent(\n                    action_type="error_categorized",\n                    params={\n                        "category": category,\n                        "message": error_message[:200]\n                    }\n                ))\n\n                return category\n\n        self.error_counts["unknown"] += 1\n        return "unknown"\n\n    def get_error_report(self) -> dict:\n        """Generate error analysis report."""\n        total_errors = sum(self.error_counts.values())\n\n        return {\n            "total_errors": total_errors,\n            "by_category": dict(self.error_counts),\n            "percentages": {\n                cat: count / total_errors * 100\n                for cat, count in self.error_counts.items()\n            },\n            "top_issues": sorted(\n                self.error_counts.items(),\n                key=lambda x: x[1],\n                reverse=True\n            )[:5]\n        }\n'})}),"\n",(0,s.jsx)(n.h2,{id:"business-metrics",children:"Business Metrics"}),"\n",(0,s.jsx)(n.h3,{id:"roi-calculator",children:"ROI Calculator"}),"\n",(0,s.jsx)(n.p,{children:"Calculate the business value of your agent:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class AgentROICalculator:\n    """Calculate ROI for AI agent deployment."""\n\n    def calculate(\n        self,\n        agent_costs: dict,\n        manual_baseline: dict,\n        volume: int\n    ) -> dict:\n        """\n        Calculate ROI comparing agent to manual process.\n\n        agent_costs: {\n            "llm_cost_per_task": 0.05,\n            "infrastructure_monthly": 500,\n            "maintenance_hours_monthly": 10,\n            "hourly_rate": 50\n        }\n\n        manual_baseline: {\n            "time_per_task_minutes": 15,\n            "hourly_rate": 30,\n            "error_rate": 0.05,\n            "error_cost": 100\n        }\n        """\n        # Agent costs\n        agent_llm_cost = agent_costs["llm_cost_per_task"] * volume\n        agent_infra_cost = agent_costs["infrastructure_monthly"]\n        agent_maintenance = (\n            agent_costs["maintenance_hours_monthly"] *\n            agent_costs["hourly_rate"]\n        )\n        total_agent_cost = agent_llm_cost + agent_infra_cost + agent_maintenance\n\n        # Manual costs\n        manual_time_hours = (manual_baseline["time_per_task_minutes"] * volume) / 60\n        manual_labor_cost = manual_time_hours * manual_baseline["hourly_rate"]\n        manual_error_cost = (\n            volume * manual_baseline["error_rate"] * manual_baseline["error_cost"]\n        )\n        total_manual_cost = manual_labor_cost + manual_error_cost\n\n        # ROI calculation\n        savings = total_manual_cost - total_agent_cost\n        roi_percentage = (savings / total_agent_cost) * 100\n\n        result = {\n            "agent_cost": total_agent_cost,\n            "manual_cost": total_manual_cost,\n            "monthly_savings": savings,\n            "roi_percentage": roi_percentage,\n            "breakeven_volume": self._calc_breakeven(agent_costs, manual_baseline),\n            "time_saved_hours": manual_time_hours\n        }\n\n        agentops.record(ActionEvent(\n            action_type="roi_calculated",\n            params=result\n        ))\n\n        return result\n'})}),"\n",(0,s.jsx)(n.h2,{id:"building-an-evaluation-pipeline",children:"Building an Evaluation Pipeline"}),"\n",(0,s.jsx)(n.p,{children:"Combine all metrics into a comprehensive evaluation:"}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph "Evaluation Pipeline"\n        A[Run Agent] --\x3e B[Collect Outputs]\n        B --\x3e C[Quality Evaluation]\n        B --\x3e D[Efficiency Metrics]\n        B --\x3e E[Reliability Check]\n\n        C --\x3e F[Aggregate Scores]\n        D --\x3e F\n        E --\x3e F\n\n        F --\x3e G{Meets Threshold?}\n        G --\x3e|Yes| H[Deploy/Promote]\n        G --\x3e|No| I[Flag for Review]\n\n        H --\x3e J[Production Monitoring]\n        I --\x3e K[Debug & Improve]\n    end\n\n    style F fill:#0066FF,stroke:#3399FF,color:#fff\n    style H fill:#22C55E,stroke:#16A34A,color:#fff\n    style I fill:#EF4444,stroke:#DC2626,color:#fff'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'class AgentEvaluationPipeline:\n    """Complete evaluation pipeline for AI agents."""\n\n    def __init__(self, thresholds: dict):\n        self.thresholds = thresholds\n        self.quality_evaluator = OutputQualityEvaluator()\n        self.latency_tracker = LatencyTracker()\n        self.error_analyzer = ErrorAnalyzer()\n\n    async def evaluate(self, agent, test_cases: list[dict]) -> dict:\n        """Run full evaluation on an agent."""\n        results = {\n            "test_cases": len(test_cases),\n            "passed": 0,\n            "failed": 0,\n            "scores": []\n        }\n\n        for test_case in test_cases:\n            start = time.time()\n            try:\n                output = await agent.run(test_case["input"])\n                latency = (time.time() - start) * 1000\n\n                # Record latency\n                self.latency_tracker.record("agent_run", latency)\n\n                # Evaluate quality\n                quality = await self.quality_evaluator.evaluate(\n                    test_case["input"],\n                    output,\n                    test_case.get("criteria", {})\n                )\n\n                # Check expected output if provided\n                if "expected" in test_case:\n                    match_score = self._compare_outputs(\n                        output, test_case["expected"]\n                    )\n                    quality["match_score"] = match_score\n\n                results["scores"].append(quality)\n\n                if quality["weighted_score"] >= self.thresholds["quality"]:\n                    results["passed"] += 1\n                else:\n                    results["failed"] += 1\n\n            except Exception as e:\n                results["failed"] += 1\n                self.error_analyzer.categorize_error(str(e), test_case)\n\n        # Aggregate results\n        results["summary"] = self._summarize(results)\n        results["latency"] = self.latency_tracker.get_percentiles("agent_run")\n        results["errors"] = self.error_analyzer.get_error_report()\n        results["recommendation"] = self._make_recommendation(results)\n\n        return results\n\n    def _make_recommendation(self, results: dict) -> str:\n        """Generate deployment recommendation."""\n        success_rate = results["passed"] / results["test_cases"]\n        avg_quality = np.mean([s["weighted_score"] for s in results["scores"]])\n        p95_latency = results["latency"].get("p95", float("inf"))\n\n        if (success_rate >= self.thresholds["success_rate"] and\n            avg_quality >= self.thresholds["quality"] and\n            p95_latency <= self.thresholds["latency_p95"]):\n            return "READY_FOR_PRODUCTION"\n        elif success_rate >= 0.8:\n            return "NEEDS_IMPROVEMENT"\n        else:\n            return "NOT_READY"\n'})}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"Effective agent evaluation goes far beyond simple accuracy metrics. By measuring quality, efficiency, reliability, and business impact, you get a complete picture of agent performance."}),"\n",(0,s.jsx)(n.p,{children:"Key takeaways:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Quality is multi-dimensional"})," - Measure completion, output quality, and reasoning"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficiency matters"})," - Track latency distributions and token usage"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reliability requires tracking"})," - Categorize errors and monitor trends"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Business metrics close the loop"})," - Calculate actual ROI"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"With AgentOps, all these metrics are captured automatically, giving you the data you need to continuously improve your agents."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsxs)(n.em,{children:["Ready to evaluate your agents properly? ",(0,s.jsx)(n.a,{href:"/docs/getting-started/quickstart",children:"Start with AgentOps"})," and get comprehensive metrics out of the box."]})})]})}function d(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>l});var a=t(6540);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);