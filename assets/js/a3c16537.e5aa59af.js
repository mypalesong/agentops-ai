"use strict";(globalThis.webpackChunkagentops_docs=globalThis.webpackChunkagentops_docs||[]).push([[1982],{1626(e,n,s){s.r(n),s.d(n,{assets:()=>i,contentTitle:()=>o,default:()=>p,frontMatter:()=>c,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"advanced/async-operations","title":"Async Operations","description":"Handle asynchronous operations with AgentOps","source":"@site/docs/advanced/async-operations.md","sourceDirName":"advanced","slug":"/advanced/async-operations","permalink":"/agentops-ai/docs/advanced/async-operations","draft":false,"unlisted":false,"editUrl":"https://github.com/agentops/agentops-docs/tree/main/docs/advanced/async-operations.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"Async Operations","description":"Handle asynchronous operations with AgentOps"},"sidebar":"docsSidebar","previous":{"title":"Multi-Agent Systems","permalink":"/agentops-ai/docs/advanced/multi-agent"},"next":{"title":"Performance","permalink":"/agentops-ai/docs/advanced/performance"}}');var a=s(4848),r=s(8453);const c={sidebar_position:3,title:"Async Operations",description:"Handle asynchronous operations with AgentOps"},o="Async Operations",i={},l=[{value:"Async LLM Calls",id:"async-llm-calls",level:2},{value:"OpenAI Async",id:"openai-async",level:3},{value:"Anthropic Async",id:"anthropic-async",level:3},{value:"Concurrent Requests",id:"concurrent-requests",level:2},{value:"Async Context Manager",id:"async-context-manager",level:2},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"async-operations",children:"Async Operations"})}),"\n",(0,a.jsx)(n.p,{children:"AgentOps fully supports asynchronous Python code, ensuring all async LLM calls and operations are properly tracked."}),"\n",(0,a.jsx)(n.h2,{id:"async-llm-calls",children:"Async LLM Calls"}),"\n",(0,a.jsx)(n.h3,{id:"openai-async",children:"OpenAI Async"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import asyncio\nimport agentops\nfrom openai import AsyncOpenAI\n\nagentops.init(api_key="your-api-key")\n\nasync def main():\n    client = AsyncOpenAI()\n\n    response = await client.chat.completions.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": "Hello!"}]\n    )\n\n    return response.choices[0].message.content\n\nresult = asyncio.run(main())\nagentops.end_session(end_state="Success")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"anthropic-async",children:"Anthropic Async"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from anthropic import AsyncAnthropic\n\nasync def main():\n    client = AsyncAnthropic()\n\n    message = await client.messages.create(\n        model="claude-3-sonnet-20240229",\n        max_tokens=1024,\n        messages=[{"role": "user", "content": "Hello!"}]\n    )\n\n    return message.content[0].text\n'})}),"\n",(0,a.jsx)(n.h2,{id:"concurrent-requests",children:"Concurrent Requests"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'async def process_queries(queries):\n    client = AsyncOpenAI()\n\n    tasks = [\n        client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": q}]\n        )\n        for q in queries\n    ]\n\n    # All concurrent calls tracked\n    responses = await asyncio.gather(*tasks)\n    return responses\n\nqueries = ["Question 1", "Question 2", "Question 3"]\nresults = asyncio.run(process_queries(queries))\n'})}),"\n",(0,a.jsx)(n.h2,{id:"async-context-manager",children:"Async Context Manager"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'async def process_with_session():\n    agentops.init(api_key="your-api-key")\n    session = agentops.start_session(tags=["async"])\n\n    try:\n        client = AsyncOpenAI()\n        response = await client.chat.completions.create(...)\n        session.end(end_state="Success")\n        return response\n    except Exception as e:\n        session.end(end_state="Fail", end_state_reason=str(e))\n        raise\n'})}),"\n",(0,a.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Initialize before async"}),": Call ",(0,a.jsx)(n.code,{children:"agentops.init()"})," in sync context"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"End sessions properly"}),": Ensure sessions end even on errors"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use gather for concurrency"}),": Track all parallel requests"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"/docs/advanced/performance",children:"Performance"})," - Optimize async operations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.a,{href:"/docs/guides/best-practices",children:"Best Practices"})," - Production patterns"]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>c,x:()=>o});var t=s(6540);const a={},r=t.createContext(a);function c(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:c(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);