"use strict";(globalThis.webpackChunkagentops_docs=globalThis.webpackChunkagentops_docs||[]).push([[9610],{1597(e){e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"real-time-ai-agent-monitoring","metadata":{"permalink":"/docusaurus-guide-3/blog/real-time-ai-agent-monitoring","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-09-18-real-time-monitoring.md","source":"@site/blog/2024-09-18-real-time-monitoring.md","title":"Real-Time Monitoring for AI Agents: A Complete Implementation Guide","description":"Build a production-grade real-time monitoring system for your AI agents. From metrics collection to alerting, this guide covers it all.","date":"2024-09-18T00:00:00.000Z","tags":[{"inline":true,"label":"monitoring","permalink":"/docusaurus-guide-3/blog/tags/monitoring"},{"inline":true,"label":"real-time","permalink":"/docusaurus-guide-3/blog/tags/real-time"},{"inline":true,"label":"alerting","permalink":"/docusaurus-guide-3/blog/tags/alerting"},{"inline":true,"label":"infrastructure","permalink":"/docusaurus-guide-3/blog/tags/infrastructure"}],"readingTime":8.76,"hasTruncateMarker":true,"authors":[{"name":"Chris Anderson","title":"Platform Engineer at AgentOps","url":"https://github.com/chrisanderson","image_url":"https://images.unsplash.com/photo-1500648767791-00dcc994a43e?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1500648767791-00dcc994a43e?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"real-time-ai-agent-monitoring","title":"Real-Time Monitoring for AI Agents: A Complete Implementation Guide","authors":[{"name":"Chris Anderson","title":"Platform Engineer at AgentOps","url":"https://github.com/chrisanderson","image_url":"https://images.unsplash.com/photo-1500648767791-00dcc994a43e?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1500648767791-00dcc994a43e?w=150&h=150&fit=crop&crop=face"}],"tags":["monitoring","real-time","alerting","infrastructure"],"description":"Build a production-grade real-time monitoring system for your AI agents. From metrics collection to alerting, this guide covers it all.","image":"https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=1200&h=630&fit=crop"},"unlisted":false,"nextItem":{"title":"Prompt Engineering for AI Agents: Beyond Basic Prompting","permalink":"/docusaurus-guide-3/blog/prompt-engineering-for-ai-agents"}},"content":"![Real-Time Monitoring](https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=1200&h=400&fit=crop)\\n\\nWhen your AI agent is handling thousands of requests per minute, you need to know the moment something goes wrong. This guide shows you how to build a comprehensive real-time monitoring system for AI agents.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Monitoring Stack\\n\\nA complete monitoring system has multiple layers:\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"AI Agent Monitoring Stack\\"\\n        A[AI Agents] --\x3e B[Metrics Collection]\\n        B --\x3e C[Stream Processing]\\n        C --\x3e D[Real-time Dashboard]\\n        C --\x3e E[Alerting System]\\n        C --\x3e F[Long-term Storage]\\n\\n        D --\x3e G[Operations Team]\\n        E --\x3e G\\n        F --\x3e H[Analytics & Reports]\\n    end\\n\\n    style B fill:#0066FF,stroke:#3399FF,color:#fff\\n    style D fill:#22C55E,stroke:#16A34A,color:#fff\\n    style E fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n## Essential Metrics to Track\\n\\n### 1. Operational Metrics\\n\\n```mermaid\\ngraph LR\\n    subgraph \\"Operational Metrics\\"\\n        A[Request Rate] --\x3e D[Health Score]\\n        B[Error Rate] --\x3e D\\n        C[Latency] --\x3e D\\n\\n        D --\x3e E{Healthy?}\\n        E --\x3e|Yes| F[Green]\\n        E --\x3e|Warning| G[Yellow]\\n        E --\x3e|Critical| H[Red]\\n    end\\n\\n    style F fill:#22C55E,stroke:#16A34A,color:#fff\\n    style G fill:#F59E0B,stroke:#D97706,color:#fff\\n    style H fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n```python\\nfrom dataclasses import dataclass, field\\nfrom collections import deque\\nimport time\\nimport agentops\\n\\n@dataclass\\nclass OperationalMetrics:\\n    \\"\\"\\"Track operational health metrics.\\"\\"\\"\\n\\n    # Rolling windows for different time periods\\n    requests_1m: deque = field(default_factory=lambda: deque(maxlen=60))\\n    requests_5m: deque = field(default_factory=lambda: deque(maxlen=300))\\n    errors_1m: deque = field(default_factory=lambda: deque(maxlen=60))\\n    latencies_1m: deque = field(default_factory=lambda: deque(maxlen=1000))\\n\\n    def record_request(self, success: bool, latency_ms: float):\\n        \\"\\"\\"Record a request outcome.\\"\\"\\"\\n        now = time.time()\\n\\n        self.requests_1m.append(now)\\n        self.requests_5m.append(now)\\n        self.latencies_1m.append(latency_ms)\\n\\n        if not success:\\n            self.errors_1m.append(now)\\n\\n        # Send to AgentOps for persistence\\n        agentops.record(ActionEvent(\\n            action_type=\\"request_recorded\\",\\n            params={\\n                \\"success\\": success,\\n                \\"latency_ms\\": latency_ms\\n            }\\n        ))\\n\\n    def get_health_score(self) -> dict:\\n        \\"\\"\\"Calculate current health score.\\"\\"\\"\\n        now = time.time()\\n        cutoff_1m = now - 60\\n\\n        # Calculate metrics\\n        request_count = sum(1 for t in self.requests_1m if t > cutoff_1m)\\n        error_count = sum(1 for t in self.errors_1m if t > cutoff_1m)\\n        error_rate = error_count / max(request_count, 1)\\n\\n        latencies = list(self.latencies_1m)\\n        p50 = np.percentile(latencies, 50) if latencies else 0\\n        p99 = np.percentile(latencies, 99) if latencies else 0\\n\\n        # Calculate health score (0-100)\\n        error_penalty = min(error_rate * 100, 50)  # Max 50 points lost\\n        latency_penalty = min((p99 / 5000) * 30, 30)  # Max 30 points for latency\\n        health_score = 100 - error_penalty - latency_penalty\\n\\n        return {\\n            \\"health_score\\": max(health_score, 0),\\n            \\"requests_per_minute\\": request_count,\\n            \\"error_rate\\": error_rate,\\n            \\"latency_p50\\": p50,\\n            \\"latency_p99\\": p99,\\n            \\"status\\": self._get_status(health_score)\\n        }\\n\\n    def _get_status(self, score: float) -> str:\\n        if score >= 90:\\n            return \\"healthy\\"\\n        elif score >= 70:\\n            return \\"warning\\"\\n        else:\\n            return \\"critical\\"\\n```\\n\\n### 2. LLM-Specific Metrics\\n\\nTrack metrics unique to LLM-powered agents:\\n\\n```python\\nclass LLMMetricsCollector:\\n    \\"\\"\\"Collect LLM-specific metrics.\\"\\"\\"\\n\\n    def __init__(self):\\n        self.token_usage = {\\"input\\": 0, \\"output\\": 0}\\n        self.costs = {\\"total\\": 0.0}\\n        self.model_calls = defaultdict(int)\\n        self.cache_stats = {\\"hits\\": 0, \\"misses\\": 0}\\n\\n    def record_llm_call(\\n        self,\\n        model: str,\\n        input_tokens: int,\\n        output_tokens: int,\\n        cost: float,\\n        cached: bool = False\\n    ):\\n        \\"\\"\\"Record an LLM API call.\\"\\"\\"\\n        self.model_calls[model] += 1\\n        self.token_usage[\\"input\\"] += input_tokens\\n        self.token_usage[\\"output\\"] += output_tokens\\n        self.costs[\\"total\\"] += cost\\n\\n        if cached:\\n            self.cache_stats[\\"hits\\"] += 1\\n        else:\\n            self.cache_stats[\\"misses\\"] += 1\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"llm_call\\",\\n            params={\\n                \\"model\\": model,\\n                \\"input_tokens\\": input_tokens,\\n                \\"output_tokens\\": output_tokens,\\n                \\"cost\\": cost,\\n                \\"cached\\": cached\\n            }\\n        ))\\n\\n    def get_summary(self) -> dict:\\n        \\"\\"\\"Get current LLM usage summary.\\"\\"\\"\\n        total_cache_requests = self.cache_stats[\\"hits\\"] + self.cache_stats[\\"misses\\"]\\n        cache_rate = self.cache_stats[\\"hits\\"] / max(total_cache_requests, 1)\\n\\n        return {\\n            \\"total_tokens\\": sum(self.token_usage.values()),\\n            \\"input_tokens\\": self.token_usage[\\"input\\"],\\n            \\"output_tokens\\": self.token_usage[\\"output\\"],\\n            \\"total_cost\\": self.costs[\\"total\\"],\\n            \\"calls_by_model\\": dict(self.model_calls),\\n            \\"cache_hit_rate\\": cache_rate\\n        }\\n```\\n\\n### 3. Agent Behavior Metrics\\n\\nTrack how agents make decisions:\\n\\n```mermaid\\npie showData\\n    title \\"Agent Tool Usage Distribution\\"\\n    \\"search\\" : 35\\n    \\"database\\" : 25\\n    \\"calculator\\" : 15\\n    \\"email\\" : 10\\n    \\"api_call\\" : 15\\n```\\n\\n```python\\nclass AgentBehaviorTracker:\\n    \\"\\"\\"Track agent decision-making patterns.\\"\\"\\"\\n\\n    def __init__(self):\\n        self.tool_usage = defaultdict(int)\\n        self.decision_paths = defaultdict(int)\\n        self.reasoning_lengths = []\\n\\n    def record_tool_use(self, tool_name: str, success: bool, latency_ms: float):\\n        \\"\\"\\"Record a tool invocation.\\"\\"\\"\\n        self.tool_usage[tool_name] += 1\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"tool_used\\",\\n            params={\\n                \\"tool\\": tool_name,\\n                \\"success\\": success,\\n                \\"latency_ms\\": latency_ms\\n            }\\n        ))\\n\\n    def record_decision_path(self, path: list[str]):\\n        \\"\\"\\"Record the path of decisions/tools used.\\"\\"\\"\\n        path_key = \\" -> \\".join(path)\\n        self.decision_paths[path_key] += 1\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"decision_path\\",\\n            params={\\n                \\"path\\": path,\\n                \\"path_length\\": len(path)\\n            }\\n        ))\\n\\n    def get_insights(self) -> dict:\\n        \\"\\"\\"Get behavioral insights.\\"\\"\\"\\n        total_tool_calls = sum(self.tool_usage.values())\\n\\n        return {\\n            \\"tool_distribution\\": {\\n                tool: count / total_tool_calls\\n                for tool, count in self.tool_usage.items()\\n            },\\n            \\"most_common_paths\\": sorted(\\n                self.decision_paths.items(),\\n                key=lambda x: x[1],\\n                reverse=True\\n            )[:10],\\n            \\"avg_path_length\\": np.mean([\\n                len(path.split(\\" -> \\"))\\n                for path in self.decision_paths.keys()\\n            ])\\n        }\\n```\\n\\n## Building the Alerting System\\n\\n### Alert Rules\\n\\nDefine rules for when to alert:\\n\\n```python\\nfrom enum import Enum\\nfrom dataclasses import dataclass\\n\\nclass AlertSeverity(Enum):\\n    INFO = \\"info\\"\\n    WARNING = \\"warning\\"\\n    ERROR = \\"error\\"\\n    CRITICAL = \\"critical\\"\\n\\n@dataclass\\nclass AlertRule:\\n    name: str\\n    condition: str  # Python expression\\n    severity: AlertSeverity\\n    cooldown_seconds: int = 300  # Don\'t re-alert for 5 minutes\\n    message_template: str = \\"\\"\\n\\n# Define alert rules\\nALERT_RULES = [\\n    AlertRule(\\n        name=\\"high_error_rate\\",\\n        condition=\\"metrics[\'error_rate\'] > 0.1\\",\\n        severity=AlertSeverity.ERROR,\\n        message_template=\\"Error rate is {error_rate:.1%}, exceeds 10% threshold\\"\\n    ),\\n    AlertRule(\\n        name=\\"critical_error_rate\\",\\n        condition=\\"metrics[\'error_rate\'] > 0.25\\",\\n        severity=AlertSeverity.CRITICAL,\\n        cooldown_seconds=60,\\n        message_template=\\"CRITICAL: Error rate is {error_rate:.1%}!\\"\\n    ),\\n    AlertRule(\\n        name=\\"high_latency\\",\\n        condition=\\"metrics[\'latency_p99\'] > 5000\\",\\n        severity=AlertSeverity.WARNING,\\n        message_template=\\"P99 latency is {latency_p99:.0f}ms, exceeds 5s threshold\\"\\n    ),\\n    AlertRule(\\n        name=\\"cost_spike\\",\\n        condition=\\"metrics[\'cost_rate\'] > hourly_budget\\",\\n        severity=AlertSeverity.ERROR,\\n        message_template=\\"Hourly cost rate ${cost_rate:.2f} exceeds budget ${hourly_budget:.2f}\\"\\n    ),\\n    AlertRule(\\n        name=\\"agent_loop_detected\\",\\n        condition=\\"metrics[\'avg_path_length\'] > 10\\",\\n        severity=AlertSeverity.WARNING,\\n        message_template=\\"Possible agent loop: avg decision path length is {avg_path_length:.1f}\\"\\n    )\\n]\\n```\\n\\n### Alert Engine\\n\\nProcess rules and send alerts:\\n\\n```mermaid\\nflowchart LR\\n    A[Metrics Stream] --\x3e B[Rule Evaluator]\\n    B --\x3e C{Rule Triggered?}\\n    C --\x3e|No| A\\n    C --\x3e|Yes| D{In Cooldown?}\\n    D --\x3e|Yes| A\\n    D --\x3e|No| E[Generate Alert]\\n    E --\x3e F[Route Alert]\\n    F --\x3e G[Slack]\\n    F --\x3e H[PagerDuty]\\n    F --\x3e I[Email]\\n    F --\x3e J[AgentOps]\\n\\n    style E fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n```python\\nimport asyncio\\nfrom datetime import datetime, timedelta\\n\\nclass AlertEngine:\\n    \\"\\"\\"Process alert rules and send notifications.\\"\\"\\"\\n\\n    def __init__(self, rules: list[AlertRule]):\\n        self.rules = rules\\n        self.last_alerts: dict[str, datetime] = {}\\n        self.alert_handlers = []\\n\\n    def add_handler(self, handler):\\n        \\"\\"\\"Add an alert handler (Slack, PagerDuty, etc.).\\"\\"\\"\\n        self.alert_handlers.append(handler)\\n\\n    async def evaluate(self, metrics: dict):\\n        \\"\\"\\"Evaluate all rules against current metrics.\\"\\"\\"\\n        for rule in self.rules:\\n            try:\\n                # Evaluate condition\\n                triggered = eval(rule.condition, {\\"metrics\\": metrics})\\n\\n                if triggered:\\n                    await self._maybe_alert(rule, metrics)\\n\\n            except Exception as e:\\n                # Log rule evaluation errors but don\'t crash\\n                agentops.record(ActionEvent(\\n                    action_type=\\"alert_rule_error\\",\\n                    params={\\n                        \\"rule\\": rule.name,\\n                        \\"error\\": str(e)\\n                    }\\n                ))\\n\\n    async def _maybe_alert(self, rule: AlertRule, metrics: dict):\\n        \\"\\"\\"Send alert if not in cooldown.\\"\\"\\"\\n        now = datetime.now()\\n        last_alert = self.last_alerts.get(rule.name)\\n\\n        if last_alert:\\n            cooldown_end = last_alert + timedelta(seconds=rule.cooldown_seconds)\\n            if now < cooldown_end:\\n                return  # Still in cooldown\\n\\n        # Generate alert\\n        alert = {\\n            \\"rule\\": rule.name,\\n            \\"severity\\": rule.severity.value,\\n            \\"message\\": rule.message_template.format(**metrics),\\n            \\"timestamp\\": now.isoformat(),\\n            \\"metrics\\": metrics\\n        }\\n\\n        # Record in AgentOps\\n        agentops.record(ActionEvent(\\n            action_type=\\"alert_triggered\\",\\n            params=alert\\n        ))\\n\\n        # Send to all handlers\\n        for handler in self.alert_handlers:\\n            await handler.send(alert)\\n\\n        self.last_alerts[rule.name] = now\\n\\n\\nclass SlackAlertHandler:\\n    \\"\\"\\"Send alerts to Slack.\\"\\"\\"\\n\\n    def __init__(self, webhook_url: str):\\n        self.webhook_url = webhook_url\\n\\n    async def send(self, alert: dict):\\n        \\"\\"\\"Send alert to Slack.\\"\\"\\"\\n        color = {\\n            \\"info\\": \\"#36a64f\\",\\n            \\"warning\\": \\"#FFA500\\",\\n            \\"error\\": \\"#FF6B6B\\",\\n            \\"critical\\": \\"#FF0000\\"\\n        }.get(alert[\\"severity\\"], \\"#808080\\")\\n\\n        payload = {\\n            \\"attachments\\": [{\\n                \\"color\\": color,\\n                \\"title\\": f\\"\ud83d\udea8 {alert[\'rule\']}\\",\\n                \\"text\\": alert[\\"message\\"],\\n                \\"fields\\": [\\n                    {\\"title\\": \\"Severity\\", \\"value\\": alert[\\"severity\\"], \\"short\\": True},\\n                    {\\"title\\": \\"Time\\", \\"value\\": alert[\\"timestamp\\"], \\"short\\": True}\\n                ]\\n            }]\\n        }\\n\\n        async with aiohttp.ClientSession() as session:\\n            await session.post(self.webhook_url, json=payload)\\n```\\n\\n## Real-Time Dashboard\\n\\n### Dashboard Architecture\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Real-Time Dashboard\\"\\n        A[Metrics API] --\x3e B[WebSocket Server]\\n        B --\x3e C[Dashboard Frontend]\\n\\n        C --\x3e D[Health Overview]\\n        C --\x3e E[Request Timeline]\\n        C --\x3e F[Cost Tracker]\\n        C --\x3e G[Error Log]\\n        C --\x3e H[Agent Behavior]\\n    end\\n\\n    style B fill:#0066FF,stroke:#3399FF,color:#fff\\n```\\n\\n### Backend Implementation\\n\\n```python\\nfrom fastapi import FastAPI, WebSocket\\nfrom fastapi.websockets import WebSocketDisconnect\\nimport asyncio\\n\\napp = FastAPI()\\n\\nclass DashboardServer:\\n    \\"\\"\\"Real-time dashboard WebSocket server.\\"\\"\\"\\n\\n    def __init__(self):\\n        self.connections: list[WebSocket] = []\\n        self.metrics = OperationalMetrics()\\n        self.llm_metrics = LLMMetricsCollector()\\n        self.behavior = AgentBehaviorTracker()\\n\\n    async def connect(self, websocket: WebSocket):\\n        \\"\\"\\"Handle new dashboard connection.\\"\\"\\"\\n        await websocket.accept()\\n        self.connections.append(websocket)\\n\\n        # Send initial state\\n        await self._send_full_state(websocket)\\n\\n    async def disconnect(self, websocket: WebSocket):\\n        \\"\\"\\"Handle dashboard disconnection.\\"\\"\\"\\n        self.connections.remove(websocket)\\n\\n    async def broadcast_update(self, update: dict):\\n        \\"\\"\\"Send update to all connected dashboards.\\"\\"\\"\\n        disconnected = []\\n\\n        for ws in self.connections:\\n            try:\\n                await ws.send_json(update)\\n            except:\\n                disconnected.append(ws)\\n\\n        for ws in disconnected:\\n            self.connections.remove(ws)\\n\\n    async def _send_full_state(self, websocket: WebSocket):\\n        \\"\\"\\"Send complete current state to new connection.\\"\\"\\"\\n        state = {\\n            \\"type\\": \\"full_state\\",\\n            \\"health\\": self.metrics.get_health_score(),\\n            \\"llm\\": self.llm_metrics.get_summary(),\\n            \\"behavior\\": self.behavior.get_insights()\\n        }\\n        await websocket.send_json(state)\\n\\n    async def metrics_loop(self):\\n        \\"\\"\\"Continuously broadcast metrics updates.\\"\\"\\"\\n        while True:\\n            update = {\\n                \\"type\\": \\"metrics_update\\",\\n                \\"timestamp\\": time.time(),\\n                \\"health\\": self.metrics.get_health_score(),\\n                \\"llm\\": self.llm_metrics.get_summary()\\n            }\\n            await self.broadcast_update(update)\\n            await asyncio.sleep(1)  # Update every second\\n\\n\\ndashboard = DashboardServer()\\n\\n@app.websocket(\\"/ws/dashboard\\")\\nasync def dashboard_endpoint(websocket: WebSocket):\\n    await dashboard.connect(websocket)\\n    try:\\n        while True:\\n            # Keep connection alive, handle any commands\\n            data = await websocket.receive_json()\\n            if data.get(\\"command\\") == \\"refresh\\":\\n                await dashboard._send_full_state(websocket)\\n    except WebSocketDisconnect:\\n        await dashboard.disconnect(websocket)\\n```\\n\\n## Integrating with AgentOps\\n\\nAgentOps provides built-in monitoring that complements custom solutions:\\n\\n```python\\nimport agentops\\nfrom agentops import track_agent, ActionEvent\\n\\n# Initialize with monitoring options\\nagentops.init(\\n    api_key=\\"your-api-key\\",\\n    default_tags=[\\"production\\", \\"monitored\\"],\\n    auto_start_session=True\\n)\\n\\n@track_agent(name=\\"ProductionAgent\\")\\nclass ProductionAgent:\\n    \\"\\"\\"Agent with full monitoring integration.\\"\\"\\"\\n\\n    def __init__(self, dashboard: DashboardServer):\\n        self.dashboard = dashboard\\n\\n    async def run(self, request: str) -> str:\\n        start_time = time.time()\\n\\n        try:\\n            # Process request\\n            result = await self._process(request)\\n\\n            # Record success\\n            latency = (time.time() - start_time) * 1000\\n            self.dashboard.metrics.record_request(True, latency)\\n\\n            return result\\n\\n        except Exception as e:\\n            # Record failure\\n            latency = (time.time() - start_time) * 1000\\n            self.dashboard.metrics.record_request(False, latency)\\n\\n            agentops.record(ActionEvent(\\n                action_type=\\"agent_error\\",\\n                params={\\n                    \\"error_type\\": type(e).__name__,\\n                    \\"error_message\\": str(e)\\n                }\\n            ))\\n\\n            raise\\n```\\n\\n## Best Practices\\n\\n### 1. Set Meaningful Thresholds\\n\\n```python\\n# Start conservative, adjust based on data\\nINITIAL_THRESHOLDS = {\\n    \\"error_rate_warning\\": 0.05,     # 5%\\n    \\"error_rate_critical\\": 0.15,    # 15%\\n    \\"latency_p99_warning\\": 3000,    # 3 seconds\\n    \\"latency_p99_critical\\": 10000,  # 10 seconds\\n    \\"cost_hourly_warning\\": 10.0,    # $10/hour\\n    \\"cost_hourly_critical\\": 50.0    # $50/hour\\n}\\n```\\n\\n### 2. Implement Gradual Alerting\\n\\n```mermaid\\nstateDiagram-v2\\n    [*] --\x3e Normal\\n    Normal --\x3e Warning: Threshold exceeded\\n    Warning --\x3e Normal: Recovered\\n    Warning --\x3e Error: Worsening\\n    Error --\x3e Warning: Improving\\n    Error --\x3e Critical: Severe\\n    Critical --\x3e Error: Stabilizing\\n    Critical --\x3e [*]: Manual intervention\\n\\n    note right of Warning: Alert team\\n    note right of Critical: Page on-call\\n```\\n\\n### 3. Create Runbooks\\n\\nLink alerts to action:\\n\\n```python\\nRUNBOOKS = {\\n    \\"high_error_rate\\": \\"\\"\\"\\n    ## High Error Rate Runbook\\n\\n    ### Immediate Actions\\n    1. Check service status: `kubectl get pods -n agents`\\n    2. Review recent errors in AgentOps dashboard\\n    3. Check LLM provider status pages\\n\\n    ### Common Causes\\n    - LLM API rate limiting\\n    - Network connectivity issues\\n    - Invalid input patterns\\n\\n    ### Escalation\\n    If not resolved in 15 minutes, page @oncall-ai-platform\\n    \\"\\"\\",\\n\\n    \\"cost_spike\\": \\"\\"\\"\\n    ## Cost Spike Runbook\\n\\n    ### Immediate Actions\\n    1. Check for runaway agents: Look for high token usage\\n    2. Review recent deployments: Were prompts changed?\\n    3. Check for attack patterns: Unusual request volumes?\\n\\n    ### Mitigation\\n    - Enable cost circuit breaker\\n    - Reduce max tokens per request\\n    - Scale down non-critical agents\\n    \\"\\"\\"\\n}\\n```\\n\\n## Conclusion\\n\\nEffective real-time monitoring for AI agents requires:\\n\\n1. **Multi-dimensional metrics** - Track operations, LLM usage, and behavior\\n2. **Intelligent alerting** - Rules with cooldowns and severity levels\\n3. **Real-time visibility** - Dashboards that update instantly\\n4. **Integration** - Connect with AgentOps for comprehensive tracking\\n\\nWith this monitoring stack, you\'ll catch issues before they become incidents and have the data to continuously improve your agents.\\n\\n---\\n\\n*Want instant monitoring for your agents? [Try AgentOps](/docs/getting-started/quickstart) and get real-time dashboards, alerting, and analytics out of the box.*"},{"id":"prompt-engineering-for-ai-agents","metadata":{"permalink":"/docusaurus-guide-3/blog/prompt-engineering-for-ai-agents","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-08-22-prompt-engineering-agents.md","source":"@site/blog/2024-08-22-prompt-engineering-agents.md","title":"Prompt Engineering for AI Agents: Beyond Basic Prompting","description":"Master advanced prompt engineering techniques specifically designed for AI agents. Includes real examples and measurable improvements.","date":"2024-08-22T00:00:00.000Z","tags":[{"inline":true,"label":"prompt-engineering","permalink":"/docusaurus-guide-3/blog/tags/prompt-engineering"},{"inline":true,"label":"optimization","permalink":"/docusaurus-guide-3/blog/tags/optimization"},{"inline":true,"label":"techniques","permalink":"/docusaurus-guide-3/blog/tags/techniques"},{"inline":true,"label":"tutorial","permalink":"/docusaurus-guide-3/blog/tags/tutorial"}],"readingTime":8.85,"hasTruncateMarker":true,"authors":[{"name":"Rachel Liu","title":"AI Research Engineer at AgentOps","url":"https://github.com/rachelliu","image_url":"https://images.unsplash.com/photo-1438761681033-6461ffad8d80?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1438761681033-6461ffad8d80?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"prompt-engineering-for-ai-agents","title":"Prompt Engineering for AI Agents: Beyond Basic Prompting","authors":[{"name":"Rachel Liu","title":"AI Research Engineer at AgentOps","url":"https://github.com/rachelliu","image_url":"https://images.unsplash.com/photo-1438761681033-6461ffad8d80?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1438761681033-6461ffad8d80?w=150&h=150&fit=crop&crop=face"}],"tags":["prompt-engineering","optimization","techniques","tutorial"],"description":"Master advanced prompt engineering techniques specifically designed for AI agents. Includes real examples and measurable improvements.","image":"https://images.unsplash.com/photo-1516321318423-f06f85e504b3?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"Real-Time Monitoring for AI Agents: A Complete Implementation Guide","permalink":"/docusaurus-guide-3/blog/real-time-ai-agent-monitoring"},"nextItem":{"title":"Evaluating AI Agents: The Metrics That Actually Matter","permalink":"/docusaurus-guide-3/blog/evaluating-ai-agents-metrics-that-matter"}},"content":"![Prompt Engineering](https://images.unsplash.com/photo-1516321318423-f06f85e504b3?w=1200&h=400&fit=crop)\\n\\nPrompt engineering for agents is different from prompting for simple Q&A. Agents need to make decisions, use tools, handle errors, and maintain consistency across long interactions. Here\'s how to craft prompts that work.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Agent Prompting Framework\\n\\nAgent prompts need to cover multiple dimensions:\\n\\n```mermaid\\nmindmap\\n    root((Agent Prompt))\\n        Identity\\n            Role definition\\n            Personality traits\\n            Expertise areas\\n        Capabilities\\n            Available tools\\n            Constraints\\n            Boundaries\\n        Behavior\\n            Decision process\\n            Error handling\\n            Output format\\n        Context\\n            Task background\\n            User information\\n            Session state\\n```\\n\\n## The Anatomy of an Agent Prompt\\n\\nA well-structured agent prompt has distinct sections:\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Agent Prompt Structure\\"\\n        A[System Identity] --\x3e B[Capabilities & Tools]\\n        B --\x3e C[Behavioral Guidelines]\\n        C --\x3e D[Output Format]\\n        D --\x3e E[Examples]\\n        E --\x3e F[Error Handling]\\n    end\\n\\n    style A fill:#0066FF,stroke:#3399FF,color:#fff\\n    style B fill:#22C55E,stroke:#16A34A,color:#fff\\n    style C fill:#F59E0B,stroke:#D97706,color:#fff\\n    style D fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n    style E fill:#EC4899,stroke:#DB2777,color:#fff\\n    style F fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n### 1. System Identity\\n\\nDefine who the agent is:\\n\\n```python\\nSYSTEM_IDENTITY = \\"\\"\\"\\nYou are a Senior Technical Support Agent for CloudTech Inc.\\n\\nCore Traits:\\n- Patient and empathetic with frustrated users\\n- Technically precise but explains in simple terms\\n- Proactive in preventing future issues\\n- Admits uncertainty rather than guessing\\n\\nExpertise Areas:\\n- Cloud infrastructure (AWS, GCP, Azure)\\n- Kubernetes and container orchestration\\n- CI/CD pipelines and DevOps practices\\n- Database management and optimization\\n\\"\\"\\"\\n```\\n\\n### 2. Capabilities & Tools\\n\\nBe explicit about what the agent can do:\\n\\n```python\\nCAPABILITIES = \\"\\"\\"\\nAvailable Tools:\\n1. search_knowledge_base(query: str) -> list[Article]\\n   - Search our documentation and past tickets\\n   - Returns relevant articles ranked by relevance\\n\\n2. get_customer_info(customer_id: str) -> CustomerProfile\\n   - Retrieve customer subscription and history\\n   - Includes past tickets and their resolutions\\n\\n3. check_service_status(service: str) -> ServiceStatus\\n   - Check current status of any CloudTech service\\n   - Returns status, recent incidents, and maintenance windows\\n\\n4. create_ticket(title: str, priority: str, description: str) -> Ticket\\n   - Escalate to human support when needed\\n   - Priority: low, medium, high, critical\\n\\nConstraints:\\n- Never share internal system details or credentials\\n- Cannot modify customer accounts directly\\n- Must escalate billing issues to human support\\n- Maximum of 5 tool calls per response\\n\\"\\"\\"\\n```\\n\\n### 3. Behavioral Guidelines\\n\\nDefine how the agent should think and act:\\n\\n```python\\nBEHAVIORAL_GUIDELINES = \\"\\"\\"\\nDecision Process:\\n1. UNDERSTAND: Fully comprehend the user\'s issue before acting\\n2. INVESTIGATE: Use tools to gather relevant information\\n3. ANALYZE: Compare findings with known solutions\\n4. RESPOND: Provide clear, actionable guidance\\n5. VERIFY: Confirm the solution works or escalate\\n\\nCommunication Style:\\n- Start with acknowledgment of the issue\\n- Use numbered steps for multi-step solutions\\n- Include relevant links to documentation\\n- End with a clear next step or confirmation request\\n\\nWhen Uncertain:\\n- Say \\"I\'m not certain, but...\\" rather than guessing\\n- Offer to escalate to a specialist\\n- Provide best-effort guidance with caveats\\n\\"\\"\\"\\n```\\n\\n### 4. Output Format\\n\\nSpecify the expected response structure:\\n\\n```python\\nOUTPUT_FORMAT = \\"\\"\\"\\nResponse Format:\\n{\\n    \\"thought_process\\": \\"Your reasoning (internal, not shown to user)\\",\\n    \\"tools_used\\": [\\"list of tools called\\"],\\n    \\"response_to_user\\": \\"The actual response\\",\\n    \\"confidence\\": 0.0-1.0,\\n    \\"follow_up_needed\\": true/false,\\n    \\"suggested_actions\\": [\\"next steps\\"]\\n}\\n\\nFor troubleshooting responses, use this structure:\\n1. Issue Acknowledgment\\n2. Diagnostic Questions (if needed)\\n3. Solution Steps\\n4. Verification Steps\\n5. Prevention Tips (if applicable)\\n\\"\\"\\"\\n```\\n\\n## Advanced Techniques\\n\\n### Technique 1: Chain-of-Thought for Agents\\n\\nGuide the agent\'s reasoning process:\\n\\n```python\\nCHAIN_OF_THOUGHT_PROMPT = \\"\\"\\"\\nWhen handling a request, think through these steps:\\n\\n<thinking>\\n1. What is the user actually trying to accomplish?\\n2. What information do I need that I don\'t have?\\n3. Which tools can help me get that information?\\n4. What are the possible solutions?\\n5. What\'s the best solution given the context?\\n6. How can I verify this will work?\\n</thinking>\\n\\nThen provide your response based on this analysis.\\n\\"\\"\\"\\n```\\n\\nWith AgentOps, you can track how well this works:\\n\\n```python\\nimport agentops\\n\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# Track reasoning quality\\n@track_agent(name=\\"SupportAgent\\")\\nclass SupportAgent:\\n    def process_request(self, request: str):\\n        # The agent\'s thinking is captured by AgentOps\\n        response = self.llm.generate(\\n            system=SYSTEM_PROMPT + CHAIN_OF_THOUGHT_PROMPT,\\n            user=request\\n        )\\n\\n        # Extract and record thinking quality\\n        thinking = self._extract_thinking(response)\\n        agentops.record(ActionEvent(\\n            action_type=\\"reasoning_trace\\",\\n            params={\\n                \\"steps_completed\\": len(thinking.split(\\"\\\\n\\")),\\n                \\"tools_considered\\": self._count_tool_mentions(thinking)\\n            }\\n        ))\\n\\n        return response\\n```\\n\\n### Technique 2: Few-Shot Examples\\n\\nProvide concrete examples of desired behavior:\\n\\n```python\\nFEW_SHOT_EXAMPLES = \\"\\"\\"\\nExample 1: Simple Question\\n\\nUser: How do I reset my API key?\\n\\n<thinking>\\n- User needs to reset API key\\n- This is a common task with documentation\\n- Should provide steps and link to docs\\n</thinking>\\n\\nResponse:\\nTo reset your API key:\\n\\n1. Log into your CloudTech dashboard\\n2. Navigate to Settings > API Keys\\n3. Click \\"Regenerate\\" next to the key you want to reset\\n4. Confirm the action (note: old key immediately becomes invalid)\\n\\nDocumentation: [API Key Management](https://docs.cloudtech.com/api-keys)\\n\\nWould you like me to guide you through any of these steps?\\n\\n---\\n\\nExample 2: Complex Troubleshooting\\n\\nUser: My deployments keep failing with \\"connection timeout\\" errors\\n\\n<thinking>\\n- This could be multiple causes: network, service issues, configuration\\n- Need to gather more information\\n- Should check service status first\\n</thinking>\\n\\n[Tool call: check_service_status(\\"deployment-service\\")]\\n[Tool call: search_knowledge_base(\\"deployment connection timeout\\")]\\n\\nResponse:\\nI see you\'re experiencing deployment timeouts. Let me help diagnose this.\\n\\nCurrent Status: Deployment service is operational (no known issues).\\n\\nThis error typically occurs due to:\\n1. **Network configuration** - VPC or firewall blocking connections\\n2. **Resource limits** - Insufficient CPU/memory for the build\\n3. **External dependencies** - Services your app connects to during startup\\n\\nTo narrow down the cause:\\n- Are these timeouts happening on all deployments or specific ones?\\n- Did this start recently, or has it been ongoing?\\n- Are you deploying to a specific region?\\n\\nMeanwhile, try these quick checks:\\n1. Verify your VPC security groups allow outbound connections\\n2. Check your deployment logs for the exact timeout point\\n3. Try increasing your deployment timeout in settings\\n\\n[Documentation: Troubleshooting Deployment Timeouts](https://docs.cloudtech.com/deploy-timeout)\\n\\"\\"\\"\\n```\\n\\n### Technique 3: Self-Correction Instructions\\n\\nTeach the agent to catch its own mistakes:\\n\\n```python\\nSELF_CORRECTION_PROMPT = \\"\\"\\"\\nBefore finalizing your response, verify:\\n\\nAccuracy Check:\\n- [ ] All facts are from authoritative sources (tools or documentation)\\n- [ ] No information is assumed or hallucinated\\n- [ ] Technical details are precise and current\\n\\nCompleteness Check:\\n- [ ] User\'s actual question is answered\\n- [ ] All necessary steps are included\\n- [ ] Edge cases or caveats are mentioned\\n\\nTone Check:\\n- [ ] Response is helpful, not condescending\\n- [ ] Complexity matches user\'s apparent technical level\\n- [ ] Clear call-to-action or next step provided\\n\\nIf any check fails, revise before responding.\\n\\"\\"\\"\\n```\\n\\n### Technique 4: Tool Selection Guidance\\n\\nHelp the agent choose the right tools:\\n\\n```mermaid\\nflowchart TD\\n    A[User Request] --\x3e B{Request Type?}\\n\\n    B --\x3e|Information Lookup| C[search_knowledge_base]\\n    B --\x3e|Account Question| D[get_customer_info]\\n    B --\x3e|Service Issue| E[check_service_status]\\n    B --\x3e|Cannot Resolve| F[create_ticket]\\n\\n    C --\x3e G{Found Answer?}\\n    G --\x3e|Yes| H[Respond with Solution]\\n    G --\x3e|No| D\\n\\n    D --\x3e I{Explains Issue?}\\n    I --\x3e|Yes| H\\n    I --\x3e|No| E\\n\\n    E --\x3e J{Service Down?}\\n    J --\x3e|Yes| K[Report Known Issue]\\n    J --\x3e|No| F\\n\\n    style H fill:#22C55E,stroke:#16A34A,color:#fff\\n    style K fill:#F59E0B,stroke:#D97706,color:#fff\\n    style F fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n```python\\nTOOL_SELECTION_GUIDE = \\"\\"\\"\\nTool Selection Logic:\\n\\n1. For \\"how do I\\" questions:\\n   \u2192 First: search_knowledge_base\\n   \u2192 If not found: provide general guidance + create_ticket\\n\\n2. For \\"why is X happening\\" questions:\\n   \u2192 First: check_service_status (rule out known issues)\\n   \u2192 Then: get_customer_info (check for account-specific issues)\\n   \u2192 Then: search_knowledge_base (find similar cases)\\n\\n3. For account-specific questions:\\n   \u2192 First: get_customer_info\\n   \u2192 Adapt response based on their plan/history\\n\\n4. For urgent issues:\\n   \u2192 First: check_service_status\\n   \u2192 If critical: create_ticket immediately\\n   \u2192 Keep user informed of escalation\\n\\nNever call more than 3 tools without providing a response.\\n\\"\\"\\"\\n```\\n\\n## Measuring Prompt Effectiveness\\n\\nTrack how prompt changes affect performance:\\n\\n```python\\nclass PromptExperiment:\\n    \\"\\"\\"A/B test different prompts.\\"\\"\\"\\n\\n    def __init__(self, prompt_a: str, prompt_b: str):\\n        self.prompts = {\\"A\\": prompt_a, \\"B\\": prompt_b}\\n        self.results = {\\"A\\": [], \\"B\\": []}\\n\\n    async def run_experiment(self, test_cases: list[dict], n_runs: int = 100):\\n        for i in range(n_runs):\\n            variant = \\"A\\" if i % 2 == 0 else \\"B\\"\\n            test_case = test_cases[i % len(test_cases)]\\n\\n            with agentops.start_session(tags=[f\\"prompt-variant-{variant}\\"]):\\n                start = time.time()\\n                result = await self.agent.run(\\n                    prompt=self.prompts[variant],\\n                    input=test_case[\\"input\\"]\\n                )\\n                latency = time.time() - start\\n\\n                # Evaluate result\\n                quality = await self.evaluator.score(result, test_case)\\n\\n                self.results[variant].append({\\n                    \\"quality\\": quality,\\n                    \\"latency\\": latency,\\n                    \\"tokens\\": result.token_count\\n                })\\n\\n        return self._analyze_results()\\n\\n    def _analyze_results(self) -> dict:\\n        analysis = {}\\n        for variant in [\\"A\\", \\"B\\"]:\\n            data = self.results[variant]\\n            analysis[variant] = {\\n                \\"avg_quality\\": np.mean([d[\\"quality\\"] for d in data]),\\n                \\"avg_latency\\": np.mean([d[\\"latency\\"] for d in data]),\\n                \\"avg_tokens\\": np.mean([d[\\"tokens\\"] for d in data])\\n            }\\n\\n        # Statistical significance\\n        quality_a = [d[\\"quality\\"] for d in self.results[\\"A\\"]]\\n        quality_b = [d[\\"quality\\"] for d in self.results[\\"B\\"]]\\n        _, p_value = stats.ttest_ind(quality_a, quality_b)\\n\\n        analysis[\\"p_value\\"] = p_value\\n        analysis[\\"winner\\"] = \\"A\\" if analysis[\\"A\\"][\\"avg_quality\\"] > analysis[\\"B\\"][\\"avg_quality\\"] else \\"B\\"\\n        analysis[\\"significant\\"] = p_value < 0.05\\n\\n        return analysis\\n```\\n\\n## Common Pitfalls and Fixes\\n\\n### Pitfall 1: Over-Specified Behavior\\n\\n**Bad:**\\n```\\nAlways respond with exactly 3 paragraphs. Each paragraph must be 2-3 sentences.\\nThe first paragraph summarizes, the second explains, the third provides next steps.\\n```\\n\\n**Good:**\\n```\\nStructure your response based on complexity:\\n- Simple questions: Direct answer with optional elaboration\\n- Complex issues: Summary \u2192 Details \u2192 Next steps\\n- Troubleshooting: Diagnosis \u2192 Steps \u2192 Verification\\n\\nAdapt length to the user\'s need.\\n```\\n\\n### Pitfall 2: Vague Tool Descriptions\\n\\n**Bad:**\\n```\\nsearch_docs(query) - Searches documentation\\n```\\n\\n**Good:**\\n```\\nsearch_docs(query: str) -> list[Document]\\n  Purpose: Find relevant documentation articles\\n  Input: Natural language search query (e.g., \\"kubernetes pod restart\\")\\n  Output: Up to 5 relevant documents with titles, summaries, and URLs\\n  When to use: User asks \\"how to\\" questions or needs reference material\\n  When NOT to use: Account-specific questions or real-time status checks\\n```\\n\\n### Pitfall 3: Missing Error Guidance\\n\\n**Bad:**\\n```\\nUse tools to help the user.\\n```\\n\\n**Good:**\\n```\\nTool Error Handling:\\n\\nIf a tool fails:\\n1. Don\'t mention the internal error to the user\\n2. Try an alternative approach if available\\n3. If no alternative, acknowledge you\'re having difficulty\\n4. Offer to escalate or try again later\\n\\nExample:\\nTool: search_knowledge_base(\\"api rate limits\\")\\nError: \\"Service temporarily unavailable\\"\\n\\nResponse: \\"I\'m having trouble accessing our documentation right now.\\nBased on my training, API rate limits are typically... [provide general guidance].\\nFor the most current information, please check [docs link] or I can create\\na ticket for a specialist to follow up.\\"\\n```\\n\\n## Complete Prompt Template\\n\\nHere\'s a production-ready template:\\n\\n```python\\nAGENT_PROMPT_TEMPLATE = \\"\\"\\"\\n# Identity\\nYou are {agent_name}, {agent_description}.\\n\\nCore Traits: {traits}\\nExpertise: {expertise}\\n\\n# Tools\\nAvailable tools (call only when needed):\\n{tool_descriptions}\\n\\n# Guidelines\\n{behavioral_guidelines}\\n\\n# Response Format\\n{output_format}\\n\\n# Examples\\n{few_shot_examples}\\n\\n# Error Handling\\n{error_handling}\\n\\n# Current Context\\nUser: {user_name}\\nAccount Type: {account_type}\\nPrevious Interactions: {interaction_summary}\\n\\nNow respond to the user\'s request.\\n\\"\\"\\"\\n```\\n\\n## Conclusion\\n\\nEffective agent prompts are:\\n- **Structured** - Clear sections for identity, capabilities, behavior\\n- **Specific** - Concrete examples and tool guidance\\n- **Adaptive** - Handle uncertainty and errors gracefully\\n- **Measurable** - Can be tested and improved\\n\\nWith AgentOps tracking every interaction, you can continuously refine your prompts based on real performance data.\\n\\n---\\n\\n*Want to see how your prompts perform in production? [Try AgentOps](/docs/getting-started/quickstart) and get detailed analytics on agent behavior.*"},{"id":"evaluating-ai-agents-metrics-that-matter","metadata":{"permalink":"/docusaurus-guide-3/blog/evaluating-ai-agents-metrics-that-matter","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-07-15-agent-evaluation.md","source":"@site/blog/2024-07-15-agent-evaluation.md","title":"Evaluating AI Agents: The Metrics That Actually Matter","description":"Move beyond accuracy scores. Learn how to evaluate AI agents using metrics that reflect real-world performance and business value.","date":"2024-07-15T00:00:00.000Z","tags":[{"inline":true,"label":"evaluation","permalink":"/docusaurus-guide-3/blog/tags/evaluation"},{"inline":true,"label":"metrics","permalink":"/docusaurus-guide-3/blog/tags/metrics"},{"inline":true,"label":"testing","permalink":"/docusaurus-guide-3/blog/tags/testing"},{"inline":true,"label":"quality","permalink":"/docusaurus-guide-3/blog/tags/quality"}],"readingTime":8.62,"hasTruncateMarker":true,"authors":[{"name":"Michael Torres","title":"Head of AI Research at AgentOps","url":"https://github.com/michaeltorres","image_url":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"evaluating-ai-agents-metrics-that-matter","title":"Evaluating AI Agents: The Metrics That Actually Matter","authors":[{"name":"Michael Torres","title":"Head of AI Research at AgentOps","url":"https://github.com/michaeltorres","image_url":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1506794778202-cad84cf45f1d?w=150&h=150&fit=crop&crop=face"}],"tags":["evaluation","metrics","testing","quality"],"description":"Move beyond accuracy scores. Learn how to evaluate AI agents using metrics that reflect real-world performance and business value.","image":"https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"Prompt Engineering for AI Agents: Beyond Basic Prompting","permalink":"/docusaurus-guide-3/blog/prompt-engineering-for-ai-agents"},"nextItem":{"title":"CrewAI vs AutoGen: A Deep Comparison for Multi-Agent Systems","permalink":"/docusaurus-guide-3/blog/crewai-vs-autogen-comparison"}},"content":"![Agent Evaluation](https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=1200&h=400&fit=crop)\\n\\n\\"Our agent has 95% accuracy!\\" Great, but is it actually useful? Traditional ML metrics don\'t capture the full picture of agent performance. Let\'s explore the metrics that truly matter.\\n\\n\x3c!--truncate--\x3e\\n\\n## Beyond Accuracy: The Agent Evaluation Framework\\n\\nAI agent evaluation requires a multi-dimensional approach:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"Agent Evaluation Framework\\"\\n        A[Quality Metrics] --\x3e E[Overall Score]\\n        B[Efficiency Metrics] --\x3e E\\n        C[Reliability Metrics] --\x3e E\\n        D[Business Metrics] --\x3e E\\n\\n        A --\x3e A1[Task Completion]\\n        A --\x3e A2[Output Quality]\\n        A --\x3e A3[Reasoning Quality]\\n\\n        B --\x3e B1[Latency]\\n        B --\x3e B2[Token Efficiency]\\n        B --\x3e B3[Cost per Task]\\n\\n        C --\x3e C1[Success Rate]\\n        C --\x3e C2[Error Recovery]\\n        C --\x3e C3[Consistency]\\n\\n        D --\x3e D1[User Satisfaction]\\n        D --\x3e D2[Business Impact]\\n        D --\x3e D3[ROI]\\n    end\\n\\n    style A fill:#0066FF,stroke:#3399FF,color:#fff\\n    style B fill:#22C55E,stroke:#16A34A,color:#fff\\n    style C fill:#F59E0B,stroke:#D97706,color:#fff\\n    style D fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n```\\n\\n## Quality Metrics\\n\\n### Task Completion Rate\\n\\nNot just \\"did it finish?\\" but \\"did it finish correctly?\\"\\n\\n```python\\nfrom dataclasses import dataclass\\nfrom enum import Enum\\nimport agentops\\n\\nclass CompletionStatus(Enum):\\n    COMPLETE = \\"complete\\"\\n    PARTIAL = \\"partial\\"\\n    FAILED = \\"failed\\"\\n    TIMEOUT = \\"timeout\\"\\n\\n@dataclass\\nclass TaskResult:\\n    status: CompletionStatus\\n    output: str\\n    required_elements: list[str]\\n    found_elements: list[str]\\n\\nclass TaskEvaluator:\\n    \\"\\"\\"Evaluate task completion quality.\\"\\"\\"\\n\\n    def evaluate(self, task: str, result: TaskResult) -> dict:\\n        # Calculate completion score\\n        if result.required_elements:\\n            completion_rate = len(result.found_elements) / len(result.required_elements)\\n        else:\\n            completion_rate = 1.0 if result.status == CompletionStatus.COMPLETE else 0.0\\n\\n        evaluation = {\\n            \\"status\\": result.status.value,\\n            \\"completion_rate\\": completion_rate,\\n            \\"missing_elements\\": [\\n                e for e in result.required_elements\\n                if e not in result.found_elements\\n            ]\\n        }\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"task_evaluation\\",\\n            params=evaluation\\n        ))\\n\\n        return evaluation\\n```\\n\\n### Output Quality Score\\n\\nMeasure the quality of what the agent produces:\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Output Quality Assessment\\"\\n        A[Agent Output] --\x3e B[Factual Accuracy]\\n        A --\x3e C[Relevance]\\n        A --\x3e D[Completeness]\\n        A --\x3e E[Format Compliance]\\n\\n        B --\x3e F[Quality Score]\\n        C --\x3e F\\n        D --\x3e F\\n        E --\x3e F\\n    end\\n\\n    style F fill:#22C55E,stroke:#16A34A,color:#fff\\n```\\n\\n```python\\nclass OutputQualityEvaluator:\\n    \\"\\"\\"Evaluate the quality of agent outputs.\\"\\"\\"\\n\\n    def __init__(self, evaluator_llm):\\n        self.evaluator = evaluator_llm\\n\\n    async def evaluate(self, task: str, output: str, criteria: dict) -> dict:\\n        \\"\\"\\"\\n        Evaluate output quality using LLM-as-judge.\\n\\n        criteria example:\\n        {\\n            \\"factual_accuracy\\": \\"Output contains only verifiable facts\\",\\n            \\"relevance\\": \\"Output directly addresses the task\\",\\n            \\"completeness\\": \\"Output covers all aspects of the task\\",\\n            \\"format\\": \\"Output follows the requested format\\"\\n        }\\n        \\"\\"\\"\\n        scores = {}\\n\\n        for criterion, description in criteria.items():\\n            prompt = f\\"\\"\\"\\n            Task: {task}\\n            Output: {output}\\n\\n            Evaluate the output on this criterion:\\n            {criterion}: {description}\\n\\n            Score from 1-5 (5 being best) and explain briefly.\\n            Format: SCORE: X\\n            REASON: ...\\n            \\"\\"\\"\\n\\n            result = await self.evaluator.generate(prompt)\\n            score = self._extract_score(result)\\n            scores[criterion] = score\\n\\n        # Calculate weighted average\\n        weights = {\\"factual_accuracy\\": 0.4, \\"relevance\\": 0.3,\\n                   \\"completeness\\": 0.2, \\"format\\": 0.1}\\n        weighted_score = sum(\\n            scores.get(k, 0) * v for k, v in weights.items()\\n        ) / sum(weights.values())\\n\\n        evaluation = {\\n            \\"individual_scores\\": scores,\\n            \\"weighted_score\\": weighted_score,\\n            \\"max_score\\": 5.0\\n        }\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"quality_evaluation\\",\\n            params=evaluation\\n        ))\\n\\n        return evaluation\\n```\\n\\n### Reasoning Quality\\n\\nEvaluate the agent\'s decision-making process:\\n\\n```python\\nclass ReasoningEvaluator:\\n    \\"\\"\\"Evaluate the quality of agent reasoning.\\"\\"\\"\\n\\n    def evaluate_reasoning_chain(self, steps: list[dict]) -> dict:\\n        \\"\\"\\"\\n        Evaluate a chain of reasoning steps.\\n\\n        Each step should have:\\n        - thought: The agent\'s reasoning\\n        - action: What action was taken\\n        - result: The outcome\\n        \\"\\"\\"\\n        metrics = {\\n            \\"logical_consistency\\": self._check_consistency(steps),\\n            \\"efficiency\\": self._check_efficiency(steps),\\n            \\"grounding\\": self._check_grounding(steps),\\n            \\"adaptability\\": self._check_adaptability(steps)\\n        }\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"reasoning_evaluation\\",\\n            params=metrics\\n        ))\\n\\n        return metrics\\n\\n    def _check_consistency(self, steps: list[dict]) -> float:\\n        \\"\\"\\"Check if reasoning is logically consistent.\\"\\"\\"\\n        # Look for contradictions between steps\\n        contradictions = 0\\n        for i, step in enumerate(steps[1:], 1):\\n            if self._contradicts_previous(step, steps[:i]):\\n                contradictions += 1\\n\\n        return 1.0 - (contradictions / max(len(steps) - 1, 1))\\n\\n    def _check_efficiency(self, steps: list[dict]) -> float:\\n        \\"\\"\\"Check if the agent took unnecessary steps.\\"\\"\\"\\n        # Compare actual steps to estimated minimum\\n        estimated_minimum = self._estimate_minimum_steps(steps)\\n        efficiency = estimated_minimum / len(steps)\\n        return min(efficiency, 1.0)\\n\\n    def _check_grounding(self, steps: list[dict]) -> float:\\n        \\"\\"\\"Check if decisions are based on evidence.\\"\\"\\"\\n        grounded_steps = sum(\\n            1 for step in steps\\n            if step.get(\\"evidence\\") or step.get(\\"source\\")\\n        )\\n        return grounded_steps / len(steps)\\n\\n    def _check_adaptability(self, steps: list[dict]) -> float:\\n        \\"\\"\\"Check if agent adapted to new information.\\"\\"\\"\\n        adaptations = sum(\\n            1 for i, step in enumerate(steps[1:], 1)\\n            if self._shows_adaptation(step, steps[i-1])\\n        )\\n        opportunities = sum(\\n            1 for step in steps\\n            if step.get(\\"new_information\\")\\n        )\\n        return adaptations / max(opportunities, 1)\\n```\\n\\n## Efficiency Metrics\\n\\n### Latency Distribution\\n\\nTrack not just average latency, but the full distribution:\\n\\n```mermaid\\nxychart-beta\\n    title \\"Agent Response Latency Distribution\\"\\n    x-axis [\\"p50\\", \\"p75\\", \\"p90\\", \\"p95\\", \\"p99\\"]\\n    y-axis \\"Latency (ms)\\" 0 --\x3e 5000\\n    bar [450, 780, 1200, 2100, 4500]\\n```\\n\\n```python\\nimport numpy as np\\nfrom collections import defaultdict\\n\\nclass LatencyTracker:\\n    \\"\\"\\"Track and analyze agent latency.\\"\\"\\"\\n\\n    def __init__(self):\\n        self.latencies = defaultdict(list)\\n\\n    def record(self, operation: str, latency_ms: float):\\n        \\"\\"\\"Record a latency measurement.\\"\\"\\"\\n        self.latencies[operation].append(latency_ms)\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"latency_recorded\\",\\n            params={\\n                \\"operation\\": operation,\\n                \\"latency_ms\\": latency_ms\\n            }\\n        ))\\n\\n    def get_percentiles(self, operation: str) -> dict:\\n        \\"\\"\\"Get latency percentiles for an operation.\\"\\"\\"\\n        data = self.latencies[operation]\\n        if not data:\\n            return {}\\n\\n        return {\\n            \\"p50\\": np.percentile(data, 50),\\n            \\"p75\\": np.percentile(data, 75),\\n            \\"p90\\": np.percentile(data, 90),\\n            \\"p95\\": np.percentile(data, 95),\\n            \\"p99\\": np.percentile(data, 99),\\n            \\"mean\\": np.mean(data),\\n            \\"std\\": np.std(data)\\n        }\\n\\n    def detect_anomalies(self, operation: str, threshold_std: float = 2.0) -> list:\\n        \\"\\"\\"Detect anomalous latencies.\\"\\"\\"\\n        data = self.latencies[operation]\\n        mean = np.mean(data)\\n        std = np.std(data)\\n\\n        anomalies = [\\n            (i, lat) for i, lat in enumerate(data)\\n            if abs(lat - mean) > threshold_std * std\\n        ]\\n\\n        return anomalies\\n```\\n\\n### Token Efficiency\\n\\nMeasure how efficiently the agent uses tokens:\\n\\n```python\\nclass TokenEfficiencyAnalyzer:\\n    \\"\\"\\"Analyze token usage efficiency.\\"\\"\\"\\n\\n    def analyze(self, session_data: dict) -> dict:\\n        \\"\\"\\"\\n        Analyze token efficiency for a session.\\n\\n        session_data should contain:\\n        - llm_calls: List of LLM call data\\n        - task_complexity: Estimated task complexity\\n        - output_quality: Quality score of final output\\n        \\"\\"\\"\\n        total_tokens = sum(\\n            call[\\"input_tokens\\"] + call[\\"output_tokens\\"]\\n            for call in session_data[\\"llm_calls\\"]\\n        )\\n\\n        # Calculate efficiency metrics\\n        metrics = {\\n            \\"total_tokens\\": total_tokens,\\n            \\"tokens_per_quality_point\\": total_tokens / max(session_data[\\"output_quality\\"], 0.1),\\n            \\"input_output_ratio\\": self._calc_io_ratio(session_data[\\"llm_calls\\"]),\\n            \\"redundancy_score\\": self._calc_redundancy(session_data[\\"llm_calls\\"]),\\n            \\"estimated_optimal_tokens\\": self._estimate_optimal(session_data),\\n            \\"efficiency_percentage\\": None  # Calculated below\\n        }\\n\\n        metrics[\\"efficiency_percentage\\"] = (\\n            metrics[\\"estimated_optimal_tokens\\"] / total_tokens * 100\\n        )\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"token_efficiency\\",\\n            params=metrics\\n        ))\\n\\n        return metrics\\n\\n    def _calc_redundancy(self, llm_calls: list) -> float:\\n        \\"\\"\\"Calculate how much content is repeated across calls.\\"\\"\\"\\n        # Simplified: check for repeated prompt content\\n        prompts = [call[\\"prompt\\"] for call in llm_calls]\\n        unique_content = set()\\n        total_content = 0\\n\\n        for prompt in prompts:\\n            words = prompt.split()\\n            unique_content.update(words)\\n            total_content += len(words)\\n\\n        return 1 - (len(unique_content) / max(total_content, 1))\\n```\\n\\n## Reliability Metrics\\n\\n### Success Rate Over Time\\n\\nTrack how success rate changes:\\n\\n```mermaid\\nxychart-beta\\n    title \\"Agent Success Rate Trend\\"\\n    x-axis [\\"Week 1\\", \\"Week 2\\", \\"Week 3\\", \\"Week 4\\"]\\n    y-axis \\"Success Rate (%)\\" 80 --\x3e 100\\n    line [92, 94, 93, 96]\\n```\\n\\n### Error Analysis\\n\\nCategorize and track error patterns:\\n\\n```python\\nclass ErrorAnalyzer:\\n    \\"\\"\\"Analyze agent error patterns.\\"\\"\\"\\n\\n    ERROR_CATEGORIES = {\\n        \\"hallucination\\": [\\"incorrect fact\\", \\"made up\\", \\"fabricated\\"],\\n        \\"incomplete\\": [\\"missing\\", \\"partial\\", \\"truncated\\"],\\n        \\"off_topic\\": [\\"irrelevant\\", \\"unrelated\\", \\"tangent\\"],\\n        \\"format_error\\": [\\"wrong format\\", \\"invalid json\\", \\"parse error\\"],\\n        \\"timeout\\": [\\"timed out\\", \\"too slow\\", \\"deadline exceeded\\"],\\n        \\"tool_failure\\": [\\"tool error\\", \\"api failed\\", \\"external service\\"]\\n    }\\n\\n    def __init__(self):\\n        self.error_counts = defaultdict(int)\\n        self.error_examples = defaultdict(list)\\n\\n    def categorize_error(self, error_message: str, context: dict) -> str:\\n        \\"\\"\\"Categorize an error into predefined categories.\\"\\"\\"\\n        error_lower = error_message.lower()\\n\\n        for category, keywords in self.ERROR_CATEGORIES.items():\\n            if any(kw in error_lower for kw in keywords):\\n                self.error_counts[category] += 1\\n                self.error_examples[category].append({\\n                    \\"message\\": error_message,\\n                    \\"context\\": context\\n                })\\n\\n                agentops.record(ActionEvent(\\n                    action_type=\\"error_categorized\\",\\n                    params={\\n                        \\"category\\": category,\\n                        \\"message\\": error_message[:200]\\n                    }\\n                ))\\n\\n                return category\\n\\n        self.error_counts[\\"unknown\\"] += 1\\n        return \\"unknown\\"\\n\\n    def get_error_report(self) -> dict:\\n        \\"\\"\\"Generate error analysis report.\\"\\"\\"\\n        total_errors = sum(self.error_counts.values())\\n\\n        return {\\n            \\"total_errors\\": total_errors,\\n            \\"by_category\\": dict(self.error_counts),\\n            \\"percentages\\": {\\n                cat: count / total_errors * 100\\n                for cat, count in self.error_counts.items()\\n            },\\n            \\"top_issues\\": sorted(\\n                self.error_counts.items(),\\n                key=lambda x: x[1],\\n                reverse=True\\n            )[:5]\\n        }\\n```\\n\\n## Business Metrics\\n\\n### ROI Calculator\\n\\nCalculate the business value of your agent:\\n\\n```python\\nclass AgentROICalculator:\\n    \\"\\"\\"Calculate ROI for AI agent deployment.\\"\\"\\"\\n\\n    def calculate(\\n        self,\\n        agent_costs: dict,\\n        manual_baseline: dict,\\n        volume: int\\n    ) -> dict:\\n        \\"\\"\\"\\n        Calculate ROI comparing agent to manual process.\\n\\n        agent_costs: {\\n            \\"llm_cost_per_task\\": 0.05,\\n            \\"infrastructure_monthly\\": 500,\\n            \\"maintenance_hours_monthly\\": 10,\\n            \\"hourly_rate\\": 50\\n        }\\n\\n        manual_baseline: {\\n            \\"time_per_task_minutes\\": 15,\\n            \\"hourly_rate\\": 30,\\n            \\"error_rate\\": 0.05,\\n            \\"error_cost\\": 100\\n        }\\n        \\"\\"\\"\\n        # Agent costs\\n        agent_llm_cost = agent_costs[\\"llm_cost_per_task\\"] * volume\\n        agent_infra_cost = agent_costs[\\"infrastructure_monthly\\"]\\n        agent_maintenance = (\\n            agent_costs[\\"maintenance_hours_monthly\\"] *\\n            agent_costs[\\"hourly_rate\\"]\\n        )\\n        total_agent_cost = agent_llm_cost + agent_infra_cost + agent_maintenance\\n\\n        # Manual costs\\n        manual_time_hours = (manual_baseline[\\"time_per_task_minutes\\"] * volume) / 60\\n        manual_labor_cost = manual_time_hours * manual_baseline[\\"hourly_rate\\"]\\n        manual_error_cost = (\\n            volume * manual_baseline[\\"error_rate\\"] * manual_baseline[\\"error_cost\\"]\\n        )\\n        total_manual_cost = manual_labor_cost + manual_error_cost\\n\\n        # ROI calculation\\n        savings = total_manual_cost - total_agent_cost\\n        roi_percentage = (savings / total_agent_cost) * 100\\n\\n        result = {\\n            \\"agent_cost\\": total_agent_cost,\\n            \\"manual_cost\\": total_manual_cost,\\n            \\"monthly_savings\\": savings,\\n            \\"roi_percentage\\": roi_percentage,\\n            \\"breakeven_volume\\": self._calc_breakeven(agent_costs, manual_baseline),\\n            \\"time_saved_hours\\": manual_time_hours\\n        }\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"roi_calculated\\",\\n            params=result\\n        ))\\n\\n        return result\\n```\\n\\n## Building an Evaluation Pipeline\\n\\nCombine all metrics into a comprehensive evaluation:\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Evaluation Pipeline\\"\\n        A[Run Agent] --\x3e B[Collect Outputs]\\n        B --\x3e C[Quality Evaluation]\\n        B --\x3e D[Efficiency Metrics]\\n        B --\x3e E[Reliability Check]\\n\\n        C --\x3e F[Aggregate Scores]\\n        D --\x3e F\\n        E --\x3e F\\n\\n        F --\x3e G{Meets Threshold?}\\n        G --\x3e|Yes| H[Deploy/Promote]\\n        G --\x3e|No| I[Flag for Review]\\n\\n        H --\x3e J[Production Monitoring]\\n        I --\x3e K[Debug & Improve]\\n    end\\n\\n    style F fill:#0066FF,stroke:#3399FF,color:#fff\\n    style H fill:#22C55E,stroke:#16A34A,color:#fff\\n    style I fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n```python\\nclass AgentEvaluationPipeline:\\n    \\"\\"\\"Complete evaluation pipeline for AI agents.\\"\\"\\"\\n\\n    def __init__(self, thresholds: dict):\\n        self.thresholds = thresholds\\n        self.quality_evaluator = OutputQualityEvaluator()\\n        self.latency_tracker = LatencyTracker()\\n        self.error_analyzer = ErrorAnalyzer()\\n\\n    async def evaluate(self, agent, test_cases: list[dict]) -> dict:\\n        \\"\\"\\"Run full evaluation on an agent.\\"\\"\\"\\n        results = {\\n            \\"test_cases\\": len(test_cases),\\n            \\"passed\\": 0,\\n            \\"failed\\": 0,\\n            \\"scores\\": []\\n        }\\n\\n        for test_case in test_cases:\\n            start = time.time()\\n            try:\\n                output = await agent.run(test_case[\\"input\\"])\\n                latency = (time.time() - start) * 1000\\n\\n                # Record latency\\n                self.latency_tracker.record(\\"agent_run\\", latency)\\n\\n                # Evaluate quality\\n                quality = await self.quality_evaluator.evaluate(\\n                    test_case[\\"input\\"],\\n                    output,\\n                    test_case.get(\\"criteria\\", {})\\n                )\\n\\n                # Check expected output if provided\\n                if \\"expected\\" in test_case:\\n                    match_score = self._compare_outputs(\\n                        output, test_case[\\"expected\\"]\\n                    )\\n                    quality[\\"match_score\\"] = match_score\\n\\n                results[\\"scores\\"].append(quality)\\n\\n                if quality[\\"weighted_score\\"] >= self.thresholds[\\"quality\\"]:\\n                    results[\\"passed\\"] += 1\\n                else:\\n                    results[\\"failed\\"] += 1\\n\\n            except Exception as e:\\n                results[\\"failed\\"] += 1\\n                self.error_analyzer.categorize_error(str(e), test_case)\\n\\n        # Aggregate results\\n        results[\\"summary\\"] = self._summarize(results)\\n        results[\\"latency\\"] = self.latency_tracker.get_percentiles(\\"agent_run\\")\\n        results[\\"errors\\"] = self.error_analyzer.get_error_report()\\n        results[\\"recommendation\\"] = self._make_recommendation(results)\\n\\n        return results\\n\\n    def _make_recommendation(self, results: dict) -> str:\\n        \\"\\"\\"Generate deployment recommendation.\\"\\"\\"\\n        success_rate = results[\\"passed\\"] / results[\\"test_cases\\"]\\n        avg_quality = np.mean([s[\\"weighted_score\\"] for s in results[\\"scores\\"]])\\n        p95_latency = results[\\"latency\\"].get(\\"p95\\", float(\\"inf\\"))\\n\\n        if (success_rate >= self.thresholds[\\"success_rate\\"] and\\n            avg_quality >= self.thresholds[\\"quality\\"] and\\n            p95_latency <= self.thresholds[\\"latency_p95\\"]):\\n            return \\"READY_FOR_PRODUCTION\\"\\n        elif success_rate >= 0.8:\\n            return \\"NEEDS_IMPROVEMENT\\"\\n        else:\\n            return \\"NOT_READY\\"\\n```\\n\\n## Conclusion\\n\\nEffective agent evaluation goes far beyond simple accuracy metrics. By measuring quality, efficiency, reliability, and business impact, you get a complete picture of agent performance.\\n\\nKey takeaways:\\n1. **Quality is multi-dimensional** - Measure completion, output quality, and reasoning\\n2. **Efficiency matters** - Track latency distributions and token usage\\n3. **Reliability requires tracking** - Categorize errors and monitor trends\\n4. **Business metrics close the loop** - Calculate actual ROI\\n\\nWith AgentOps, all these metrics are captured automatically, giving you the data you need to continuously improve your agents.\\n\\n---\\n\\n*Ready to evaluate your agents properly? [Start with AgentOps](/docs/getting-started/quickstart) and get comprehensive metrics out of the box.*"},{"id":"crewai-vs-autogen-comparison","metadata":{"permalink":"/docusaurus-guide-3/blog/crewai-vs-autogen-comparison","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-06-08-crewai-vs-autogen.md","source":"@site/blog/2024-06-08-crewai-vs-autogen.md","title":"CrewAI vs AutoGen: A Deep Comparison for Multi-Agent Systems","description":"An in-depth comparison of CrewAI and AutoGen for building multi-agent AI systems. Includes code examples, performance benchmarks, and recommendations.","date":"2024-06-08T00:00:00.000Z","tags":[{"inline":true,"label":"crewai","permalink":"/docusaurus-guide-3/blog/tags/crewai"},{"inline":true,"label":"autogen","permalink":"/docusaurus-guide-3/blog/tags/autogen"},{"inline":true,"label":"multi-agent","permalink":"/docusaurus-guide-3/blog/tags/multi-agent"},{"inline":true,"label":"comparison","permalink":"/docusaurus-guide-3/blog/tags/comparison"}],"readingTime":7.11,"hasTruncateMarker":true,"authors":[{"name":"Jessica Wong","title":"AI Solutions Engineer at AgentOps","url":"https://github.com/jessicawong","image_url":"https://images.unsplash.com/photo-1573496359142-b8d87734a5a2?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1573496359142-b8d87734a5a2?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"crewai-vs-autogen-comparison","title":"CrewAI vs AutoGen: A Deep Comparison for Multi-Agent Systems","authors":[{"name":"Jessica Wong","title":"AI Solutions Engineer at AgentOps","url":"https://github.com/jessicawong","image_url":"https://images.unsplash.com/photo-1573496359142-b8d87734a5a2?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1573496359142-b8d87734a5a2?w=150&h=150&fit=crop&crop=face"}],"tags":["crewai","autogen","multi-agent","comparison"],"description":"An in-depth comparison of CrewAI and AutoGen for building multi-agent AI systems. Includes code examples, performance benchmarks, and recommendations.","image":"https://images.unsplash.com/photo-1485827404703-89b55fcc595e?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"Evaluating AI Agents: The Metrics That Actually Matter","permalink":"/docusaurus-guide-3/blog/evaluating-ai-agents-metrics-that-matter"},"nextItem":{"title":"Building Production-Ready AI Agents: A Complete Checklist","permalink":"/docusaurus-guide-3/blog/building-production-ready-ai-agents"}},"content":"![Multi-Agent Systems](https://images.unsplash.com/photo-1485827404703-89b55fcc595e?w=1200&h=400&fit=crop)\\n\\nChoosing the right multi-agent framework can make or break your AI project. We\'ve extensively tested both CrewAI and AutoGen with AgentOps instrumentation. Here\'s what we learned.\\n\\n\x3c!--truncate--\x3e\\n\\n## Overview\\n\\nBoth frameworks enable multi-agent AI systems, but they take fundamentally different approaches:\\n\\n```mermaid\\nflowchart TB\\n    subgraph CrewAI[\\"CrewAI Philosophy\\"]\\n        direction TB\\n        A1[Role-Based Design] --\x3e A2[Structured Workflows]\\n        A2 --\x3e A3[Task-Centric]\\n        A3 --\x3e A4[Human-Inspired Teams]\\n    end\\n\\n    subgraph AutoGen[\\"AutoGen Philosophy\\"]\\n        direction TB\\n        B1[Conversational Design] --\x3e B2[Flexible Interactions]\\n        B2 --\x3e B3[Message-Centric]\\n        B3 --\x3e B4[Emergent Collaboration]\\n    end\\n\\n    style A1 fill:#0066FF,stroke:#3399FF,color:#fff\\n    style B1 fill:#22C55E,stroke:#16A34A,color:#fff\\n```\\n\\n| Aspect | CrewAI | AutoGen |\\n|--------|--------|---------|\\n| **Design Philosophy** | Role-based crews | Conversation-based agents |\\n| **Learning Curve** | Easier | Steeper |\\n| **Flexibility** | Structured | Highly flexible |\\n| **Best For** | Defined workflows | Research & experimentation |\\n| **Organization** | Crews with tasks | Agent conversations |\\n| **Developed By** | CrewAI (startup) | Microsoft Research |\\n\\n## Architecture Comparison\\n\\n### CrewAI Architecture\\n\\nCrewAI uses a hierarchical structure inspired by human teams:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"CrewAI Structure\\"\\n        Crew[Crew] --\x3e M[Manager Agent]\\n        Crew --\x3e T1[Task 1]\\n        Crew --\x3e T2[Task 2]\\n        Crew --\x3e T3[Task 3]\\n\\n        M --\x3e A1[Research Agent]\\n        M --\x3e A2[Writer Agent]\\n        M --\x3e A3[Reviewer Agent]\\n\\n        A1 --\x3e T1\\n        A2 --\x3e T2\\n        A3 --\x3e T3\\n\\n        T1 --\x3e|Output| T2\\n        T2 --\x3e|Output| T3\\n    end\\n\\n    style Crew fill:#0066FF,stroke:#3399FF,color:#fff\\n    style M fill:#F59E0B,stroke:#D97706,color:#fff\\n```\\n\\n```python\\nfrom crewai import Agent, Task, Crew, Process\\nimport agentops\\n\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# Define agents with specific roles\\nresearcher = Agent(\\n    role=\\"Research Analyst\\",\\n    goal=\\"Find and analyze the most relevant information\\",\\n    backstory=\\"You are a senior research analyst with expertise in market research\\",\\n    verbose=True,\\n    allow_delegation=False,\\n    tools=[search_tool, web_scraper]\\n)\\n\\nwriter = Agent(\\n    role=\\"Content Writer\\",\\n    goal=\\"Create compelling content based on research\\",\\n    backstory=\\"You are an experienced technical writer\\",\\n    verbose=True\\n)\\n\\nreviewer = Agent(\\n    role=\\"Quality Reviewer\\",\\n    goal=\\"Ensure content accuracy and quality\\",\\n    backstory=\\"You are a meticulous editor with high standards\\",\\n    verbose=True\\n)\\n\\n# Define tasks\\nresearch_task = Task(\\n    description=\\"Research the latest trends in AI observability\\",\\n    agent=researcher,\\n    expected_output=\\"A detailed research report with key findings\\"\\n)\\n\\nwriting_task = Task(\\n    description=\\"Write a blog post based on the research\\",\\n    agent=writer,\\n    expected_output=\\"A 1500-word blog post\\",\\n    context=[research_task]  # Depends on research\\n)\\n\\nreview_task = Task(\\n    description=\\"Review and improve the blog post\\",\\n    agent=reviewer,\\n    expected_output=\\"A polished, publication-ready blog post\\",\\n    context=[writing_task]  # Depends on writing\\n)\\n\\n# Create and run crew\\ncrew = Crew(\\n    agents=[researcher, writer, reviewer],\\n    tasks=[research_task, writing_task, review_task],\\n    process=Process.sequential,  # or Process.hierarchical\\n    verbose=True\\n)\\n\\nresult = crew.kickoff()\\n```\\n\\n### AutoGen Architecture\\n\\nAutoGen uses a conversation-based approach:\\n\\n```mermaid\\nsequenceDiagram\\n    participant U as User Proxy\\n    participant A as Assistant\\n    participant C as Critic\\n    participant E as Executor\\n\\n    U->>A: \\"Write code for X\\"\\n    A->>A: Generate code\\n    A->>C: \\"Review this code\\"\\n    C->>C: Analyze code\\n    C->>A: \\"Found issues: ...\\"\\n    A->>A: Fix issues\\n    A->>E: \\"Execute this\\"\\n    E->>E: Run code\\n    E->>A: \\"Results: ...\\"\\n    A->>U: \\"Here\'s the solution\\"\\n```\\n\\n```python\\nimport autogen\\nimport agentops\\n\\nagentops.init(api_key=\\"your-api-key\\")\\n\\nconfig_list = [{\\"model\\": \\"gpt-4\\", \\"api_key\\": \\"your-openai-key\\"}]\\n\\n# Define agents\\nuser_proxy = autogen.UserProxyAgent(\\n    name=\\"User\\",\\n    system_message=\\"A human user who gives tasks and feedback.\\",\\n    code_execution_config={\\"work_dir\\": \\"workspace\\"},\\n    human_input_mode=\\"TERMINATE\\"\\n)\\n\\nassistant = autogen.AssistantAgent(\\n    name=\\"Assistant\\",\\n    llm_config={\\"config_list\\": config_list},\\n    system_message=\\"You are a helpful AI assistant that writes code.\\"\\n)\\n\\ncritic = autogen.AssistantAgent(\\n    name=\\"Critic\\",\\n    llm_config={\\"config_list\\": config_list},\\n    system_message=\\"You review code for bugs and improvements.\\"\\n)\\n\\n# Create group chat for multi-agent conversation\\ngroupchat = autogen.GroupChat(\\n    agents=[user_proxy, assistant, critic],\\n    messages=[],\\n    max_round=10\\n)\\n\\nmanager = autogen.GroupChatManager(\\n    groupchat=groupchat,\\n    llm_config={\\"config_list\\": config_list}\\n)\\n\\n# Start conversation\\nuser_proxy.initiate_chat(\\n    manager,\\n    message=\\"Create a Python function that calculates fibonacci numbers efficiently\\"\\n)\\n```\\n\\n## Performance Comparison\\n\\nWe ran both frameworks on identical tasks with AgentOps tracking:\\n\\n```mermaid\\nxychart-beta\\n    title \\"Average Completion Time (seconds)\\"\\n    x-axis [Simple, Medium, Complex, Multi-Step]\\n    y-axis \\"Time (s)\\" 0 --\x3e 120\\n    bar [15, 35, 65, 95]\\n    bar [20, 40, 55, 80]\\n```\\n\\n### Benchmark Results\\n\\n| Metric | CrewAI | AutoGen | Winner |\\n|--------|--------|---------|--------|\\n| Simple Tasks (avg) | 15s | 20s | CrewAI |\\n| Complex Tasks (avg) | 65s | 55s | AutoGen |\\n| Token Efficiency | 85% | 78% | CrewAI |\\n| Success Rate | 92% | 89% | CrewAI |\\n| Flexibility Score | 7/10 | 9/10 | AutoGen |\\n| Learning Curve | Easy | Medium | CrewAI |\\n\\n### Cost Analysis\\n\\nWith AgentOps cost tracking:\\n\\n```python\\n# Example cost breakdown from AgentOps dashboard\\ncrewai_costs = {\\n    \\"research_task\\": {\\"tokens\\": 4500, \\"cost\\": 0.135},\\n    \\"writing_task\\": {\\"tokens\\": 6200, \\"cost\\": 0.186},\\n    \\"review_task\\": {\\"tokens\\": 3100, \\"cost\\": 0.093},\\n    \\"total\\": {\\"tokens\\": 13800, \\"cost\\": 0.414}\\n}\\n\\nautogen_costs = {\\n    \\"conversation_rounds\\": 8,\\n    \\"total_tokens\\": 15200,\\n    \\"total_cost\\": 0.456\\n}\\n```\\n\\nCrewAI tends to be more token-efficient for structured workflows, while AutoGen\'s conversational approach can use more tokens but provides more flexibility.\\n\\n## When to Choose CrewAI\\n\\nChoose CrewAI when you have:\\n\\n```mermaid\\nmindmap\\n    root((CrewAI))\\n        Defined Workflows\\n            Known task sequence\\n            Clear agent roles\\n            Predictable outputs\\n        Team Metaphor Fits\\n            Research teams\\n            Content creation\\n            Business processes\\n        Production Focus\\n            Reliability needed\\n            Cost efficiency\\n            Consistent results\\n        Simpler Requirements\\n            Linear workflows\\n            Limited agent count\\n            Clear delegation\\n```\\n\\n### CrewAI Strengths\\n\\n1. **Clear role definitions** - Each agent has a specific job\\n2. **Structured task flow** - Tasks execute in defined order\\n3. **Easier debugging** - AgentOps shows clear task boundaries\\n4. **Lower token usage** - More efficient for structured work\\n\\n### CrewAI Example: Content Team\\n\\n```python\\n# Perfect CrewAI use case: Content production pipeline\\ncontent_crew = Crew(\\n    agents=[\\n        Agent(role=\\"SEO Researcher\\", ...),\\n        Agent(role=\\"Content Writer\\", ...),\\n        Agent(role=\\"Editor\\", ...),\\n        Agent(role=\\"Social Media Manager\\", ...)\\n    ],\\n    tasks=[\\n        Task(description=\\"Research keywords\\", ...),\\n        Task(description=\\"Write article\\", ...),\\n        Task(description=\\"Edit and polish\\", ...),\\n        Task(description=\\"Create social posts\\", ...)\\n    ],\\n    process=Process.sequential\\n)\\n```\\n\\n## When to Choose AutoGen\\n\\nChoose AutoGen when you need:\\n\\n```mermaid\\nmindmap\\n    root((AutoGen))\\n        Flexibility\\n            Dynamic conversations\\n            Emergent behavior\\n            Adaptive workflows\\n        Complex Reasoning\\n            Multi-turn dialogue\\n            Iterative refinement\\n            Code execution\\n        Research & Experimentation\\n            Exploring approaches\\n            Testing hypotheses\\n            Novel architectures\\n        Code-Heavy Tasks\\n            Development workflows\\n            Testing & debugging\\n            Code review\\n```\\n\\n### AutoGen Strengths\\n\\n1. **Flexible conversations** - Agents can interact naturally\\n2. **Code execution** - Built-in sandboxed execution\\n3. **Emergent behavior** - Agents can self-organize\\n4. **Research-friendly** - Great for experimentation\\n\\n### AutoGen Example: Code Review System\\n\\n```python\\n# Perfect AutoGen use case: Iterative code development\\ndeveloper = autogen.AssistantAgent(\\n    name=\\"Developer\\",\\n    system_message=\\"Write clean, efficient code.\\"\\n)\\n\\nreviewer = autogen.AssistantAgent(\\n    name=\\"Reviewer\\",\\n    system_message=\\"Review code for bugs, security issues, and improvements.\\"\\n)\\n\\ntester = autogen.AssistantAgent(\\n    name=\\"Tester\\",\\n    system_message=\\"Write comprehensive tests for the code.\\"\\n)\\n\\n# Natural conversation flow allows iterative improvement\\ngroupchat = autogen.GroupChat(\\n    agents=[user_proxy, developer, reviewer, tester],\\n    messages=[],\\n    max_round=15\\n)\\n```\\n\\n## Observability with AgentOps\\n\\nBoth frameworks work seamlessly with AgentOps:\\n\\n### CrewAI + AgentOps\\n\\n```python\\nimport agentops\\nfrom crewai import Crew\\n\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# AgentOps automatically tracks:\\n# - Each agent\'s actions\\n# - Task transitions\\n# - LLM calls per agent\\n# - Total crew execution time\\n# - Costs per task and agent\\n\\ncrew = Crew(agents=agents, tasks=tasks)\\nresult = crew.kickoff()\\n\\n# View in AgentOps dashboard:\\n# - Task waterfall view\\n# - Agent performance comparison\\n# - Token usage breakdown\\n```\\n\\n### AutoGen + AgentOps\\n\\n```python\\nimport agentops\\nimport autogen\\n\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# AgentOps automatically tracks:\\n# - Each conversation turn\\n# - Agent-to-agent messages\\n# - Code execution events\\n# - Group chat dynamics\\n# - Cost per conversation\\n\\nuser_proxy.initiate_chat(manager, message=\\"...\\")\\n\\n# View in AgentOps dashboard:\\n# - Conversation timeline\\n# - Message flow diagram\\n# - Agent participation metrics\\n```\\n\\n## Hybrid Approach\\n\\nSometimes the best solution combines both:\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Hybrid Architecture\\"\\n        A[User Request] --\x3e B[Router]\\n        B --\x3e|Structured Task| C[CrewAI Pipeline]\\n        B --\x3e|Open-ended Task| D[AutoGen Conversation]\\n        C --\x3e E[Result]\\n        D --\x3e E\\n        E --\x3e F[Response]\\n    end\\n\\n    style B fill:#F59E0B,stroke:#D97706,color:#fff\\n    style C fill:#0066FF,stroke:#3399FF,color:#fff\\n    style D fill:#22C55E,stroke:#16A34A,color:#fff\\n```\\n\\n```python\\nclass HybridOrchestrator:\\n    def __init__(self):\\n        self.crewai_crew = self._setup_crewai()\\n        self.autogen_chat = self._setup_autogen()\\n        agentops.init(api_key=\\"your-api-key\\")\\n\\n    def process(self, task: str) -> str:\\n        task_type = self._classify_task(task)\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"task_routed\\",\\n            params={\\n                \\"task_type\\": task_type,\\n                \\"framework\\": \\"crewai\\" if task_type == \\"structured\\" else \\"autogen\\"\\n            }\\n        ))\\n\\n        if task_type == \\"structured\\":\\n            return self.crewai_crew.kickoff(inputs={\\"task\\": task})\\n        else:\\n            return self._run_autogen(task)\\n\\n    def _classify_task(self, task: str) -> str:\\n        # Simple classification logic\\n        structured_keywords = [\\"write\\", \\"create\\", \\"generate\\", \\"summarize\\"]\\n        if any(kw in task.lower() for kw in structured_keywords):\\n            return \\"structured\\"\\n        return \\"open-ended\\"\\n```\\n\\n## Recommendations\\n\\n### Choose CrewAI if:\\n- You have well-defined workflows\\n- Team metaphors fit your use case\\n- You need predictable, efficient execution\\n- Production reliability is critical\\n\\n### Choose AutoGen if:\\n- Tasks require iterative refinement\\n- You need code execution capabilities\\n- Flexibility is more important than efficiency\\n- You\'re doing research or experimentation\\n\\n### Use Both if:\\n- You have diverse task types\\n- Some workflows are structured, others open-ended\\n- You want the best of both worlds\\n\\n## Conclusion\\n\\nBoth CrewAI and AutoGen are excellent frameworks for multi-agent AI systems. The right choice depends on your specific needs:\\n\\n- **CrewAI** excels at structured, production workflows\\n- **AutoGen** shines in flexible, research-oriented scenarios\\n\\nWith AgentOps, you get full visibility into both, making it easy to compare, debug, and optimize regardless of your choice.\\n\\n---\\n\\n*Want to see both frameworks in action? [Try AgentOps free](/docs/getting-started/quickstart) and get instant visibility into your multi-agent systems.*"},{"id":"building-production-ready-ai-agents","metadata":{"permalink":"/docusaurus-guide-3/blog/building-production-ready-ai-agents","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-05-20-building-production-agents.md","source":"@site/blog/2024-05-20-building-production-agents.md","title":"Building Production-Ready AI Agents: A Complete Checklist","description":"The definitive checklist for taking your AI agents from prototype to production. Covers reliability, security, monitoring, and scale.","date":"2024-05-20T00:00:00.000Z","tags":[{"inline":true,"label":"production","permalink":"/docusaurus-guide-3/blog/tags/production"},{"inline":true,"label":"best-practices","permalink":"/docusaurus-guide-3/blog/tags/best-practices"},{"inline":true,"label":"architecture","permalink":"/docusaurus-guide-3/blog/tags/architecture"},{"inline":true,"label":"reliability","permalink":"/docusaurus-guide-3/blog/tags/reliability"}],"readingTime":7.75,"hasTruncateMarker":true,"authors":[{"name":"David Park","title":"Staff Engineer at AgentOps","url":"https://github.com/davidpark","image_url":"https://images.unsplash.com/photo-1519085360753-af0119f7cbe7?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1519085360753-af0119f7cbe7?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"building-production-ready-ai-agents","title":"Building Production-Ready AI Agents: A Complete Checklist","authors":[{"name":"David Park","title":"Staff Engineer at AgentOps","url":"https://github.com/davidpark","image_url":"https://images.unsplash.com/photo-1519085360753-af0119f7cbe7?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1519085360753-af0119f7cbe7?w=150&h=150&fit=crop&crop=face"}],"tags":["production","best-practices","architecture","reliability"],"description":"The definitive checklist for taking your AI agents from prototype to production. Covers reliability, security, monitoring, and scale.","image":"https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"CrewAI vs AutoGen: A Deep Comparison for Multi-Agent Systems","permalink":"/docusaurus-guide-3/blog/crewai-vs-autogen-comparison"},"nextItem":{"title":"LangChain + AgentOps: Complete Observability Guide","permalink":"/docusaurus-guide-3/blog/langchain-observability-complete-guide"}},"content":"![Production AI Agents](https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=1200&h=400&fit=crop)\\n\\nThe gap between a prototype AI agent and a production-ready system is enormous. This guide provides a comprehensive checklist developed from helping hundreds of teams deploy agents to production.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Production Readiness Pyramid\\n\\nMoving to production requires addressing multiple layers:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"Production Readiness Pyramid\\"\\n        A[Scale & Performance] --\x3e B[Security & Compliance]\\n        B --\x3e C[Reliability & Recovery]\\n        C --\x3e D[Observability & Debugging]\\n        D --\x3e E[Core Functionality]\\n    end\\n\\n    style A fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n    style B fill:#EF4444,stroke:#DC2626,color:#fff\\n    style C fill:#F59E0B,stroke:#D97706,color:#fff\\n    style D fill:#0066FF,stroke:#3399FF,color:#fff\\n    style E fill:#22C55E,stroke:#16A34A,color:#fff\\n```\\n\\nYou can\'t focus on scale until security is solid. You can\'t ensure reliability without observability. Let\'s go through each layer.\\n\\n## Layer 1: Core Functionality\\n\\nBefore anything else, your agent must work correctly.\\n\\n### Functional Requirements Checklist\\n\\n- [ ] **Clear task definition** - What exactly should the agent do?\\n- [ ] **Input validation** - All inputs are validated before processing\\n- [ ] **Output verification** - Outputs meet expected format and quality\\n- [ ] **Edge case handling** - Unusual inputs don\'t cause failures\\n- [ ] **Timeout handling** - Long-running operations have appropriate timeouts\\n\\n```python\\nfrom pydantic import BaseModel, validator\\nfrom typing import Literal\\nimport agentops\\n\\nclass AgentInput(BaseModel):\\n    \\"\\"\\"Validated input for the agent.\\"\\"\\"\\n    query: str\\n    context: dict = {}\\n    priority: Literal[\\"low\\", \\"medium\\", \\"high\\"] = \\"medium\\"\\n\\n    @validator(\\"query\\")\\n    def query_not_empty(cls, v):\\n        if not v or not v.strip():\\n            raise ValueError(\\"Query cannot be empty\\")\\n        if len(v) > 10000:\\n            raise ValueError(\\"Query too long (max 10000 chars)\\")\\n        return v.strip()\\n\\nclass AgentOutput(BaseModel):\\n    \\"\\"\\"Validated output from the agent.\\"\\"\\"\\n    response: str\\n    confidence: float\\n    sources: list[str] = []\\n\\n    @validator(\\"confidence\\")\\n    def confidence_in_range(cls, v):\\n        if not 0 <= v <= 1:\\n            raise ValueError(\\"Confidence must be between 0 and 1\\")\\n        return v\\n\\nclass ProductionAgent:\\n    def __init__(self):\\n        agentops.init(api_key=\\"your-api-key\\")\\n\\n    async def run(self, input_data: dict) -> dict:\\n        # Validate input\\n        validated_input = AgentInput(**input_data)\\n\\n        # Process with timeout\\n        try:\\n            result = await asyncio.wait_for(\\n                self._process(validated_input),\\n                timeout=30.0\\n            )\\n        except asyncio.TimeoutError:\\n            agentops.record(ActionEvent(\\n                action_type=\\"timeout\\",\\n                params={\\"query_length\\": len(validated_input.query)}\\n            ))\\n            raise\\n\\n        # Validate output\\n        validated_output = AgentOutput(**result)\\n        return validated_output.dict()\\n```\\n\\n## Layer 2: Observability & Debugging\\n\\nYou can\'t fix what you can\'t see. This is where AgentOps shines.\\n\\n### Observability Checklist\\n\\n- [ ] **Session tracking** - Every interaction is captured\\n- [ ] **LLM call logging** - All prompts and responses recorded\\n- [ ] **Tool execution tracking** - Tool inputs, outputs, and timing\\n- [ ] **Error capturing** - Full context on failures\\n- [ ] **Cost tracking** - Real-time spend monitoring\\n- [ ] **Performance metrics** - Latency percentiles, throughput\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Observability Stack\\"\\n        A[Agent] --\x3e B[AgentOps SDK]\\n        B --\x3e C[Event Collection]\\n        C --\x3e D[Real-time Processing]\\n        D --\x3e E[Dashboard]\\n        D --\x3e F[Alerts]\\n        D --\x3e G[Analytics]\\n    end\\n\\n    style B fill:#0066FF,stroke:#3399FF,color:#fff\\n    style E fill:#22C55E,stroke:#16A34A,color:#fff\\n    style F fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n```python\\nimport agentops\\nfrom agentops import ActionEvent\\nimport structlog\\n\\n# Initialize comprehensive tracking\\nagentops.init(\\n    api_key=\\"your-api-key\\",\\n    default_tags=[\\"production\\", \\"v2.1.0\\"],\\n    instrument_llm_calls=True,\\n    auto_start_session=True\\n)\\n\\n# Set up structured logging alongside AgentOps\\nlogger = structlog.get_logger()\\n\\nclass ObservableAgent:\\n    def __init__(self, name: str):\\n        self.name = name\\n        self.logger = logger.bind(agent=name)\\n\\n    async def run(self, query: str):\\n        session = agentops.start_session(tags=[\\n            f\\"agent-{self.name}\\",\\n            f\\"query-type-{self._classify_query(query)}\\"\\n        ])\\n\\n        self.logger.info(\\"agent_started\\", query_length=len(query))\\n\\n        try:\\n            result = await self._process(query)\\n\\n            agentops.record(ActionEvent(\\n                action_type=\\"agent_success\\",\\n                params={\\n                    \\"response_length\\": len(result),\\n                    \\"confidence\\": self._calculate_confidence(result)\\n                }\\n            ))\\n\\n            agentops.end_session(end_state=\\"Success\\")\\n            return result\\n\\n        except Exception as e:\\n            self.logger.error(\\"agent_failed\\", error=str(e))\\n\\n            agentops.record(ActionEvent(\\n                action_type=\\"agent_error\\",\\n                params={\\n                    \\"error_type\\": type(e).__name__,\\n                    \\"error_message\\": str(e)\\n                }\\n            ))\\n\\n            agentops.end_session(\\n                end_state=\\"Fail\\",\\n                end_state_reason=str(e)\\n            )\\n            raise\\n```\\n\\n## Layer 3: Reliability & Recovery\\n\\nProduction systems must handle failures gracefully.\\n\\n### Reliability Checklist\\n\\n- [ ] **Retry logic** - Transient failures are retried with backoff\\n- [ ] **Circuit breakers** - Failing dependencies are isolated\\n- [ ] **Fallback responses** - Graceful degradation when components fail\\n- [ ] **State recovery** - Sessions can resume after interruption\\n- [ ] **Idempotency** - Repeated calls produce same result\\n- [ ] **Rate limiting** - Protect against traffic spikes\\n\\n```mermaid\\nstateDiagram-v2\\n    [*] --\x3e Healthy\\n    Healthy --\x3e Degraded: Errors > threshold\\n    Degraded --\x3e Healthy: Errors decrease\\n    Degraded --\x3e Failed: Errors critical\\n    Failed --\x3e Degraded: Recovery detected\\n    Failed --\x3e [*]: Manual intervention\\n\\n    note right of Healthy: All systems operational\\n    note right of Degraded: Fallback mode active\\n    note right of Failed: Circuit breaker open\\n```\\n\\n```python\\nfrom circuitbreaker import circuit\\nfrom tenacity import retry, stop_after_attempt, wait_exponential\\n\\nclass ResilientAgent:\\n    def __init__(self):\\n        self.circuit_breaker_state = \\"closed\\"\\n\\n    @retry(\\n        stop=stop_after_attempt(3),\\n        wait=wait_exponential(multiplier=1, min=1, max=10)\\n    )\\n    @circuit(failure_threshold=5, recovery_timeout=30)\\n    async def call_llm(self, prompt: str) -> str:\\n        \\"\\"\\"LLM call with retry and circuit breaker.\\"\\"\\"\\n        try:\\n            response = await self.llm_client.generate(prompt)\\n            return response\\n        except RateLimitError:\\n            # Record for analysis\\n            agentops.record(ActionEvent(\\n                action_type=\\"rate_limited\\",\\n                params={\\"provider\\": \\"openai\\"}\\n            ))\\n            raise\\n        except APIError as e:\\n            agentops.record(ActionEvent(\\n                action_type=\\"api_error\\",\\n                params={\\"error\\": str(e)}\\n            ))\\n            raise\\n\\n    async def run_with_fallback(self, query: str) -> str:\\n        \\"\\"\\"Run with graceful degradation.\\"\\"\\"\\n        try:\\n            return await self.call_llm(query)\\n        except CircuitBreakerError:\\n            agentops.record(ActionEvent(\\n                action_type=\\"circuit_breaker_open\\",\\n                params={\\"fallback\\": \\"cached_response\\"}\\n            ))\\n            # Return cached or simplified response\\n            return self._get_fallback_response(query)\\n\\n    def _get_fallback_response(self, query: str) -> str:\\n        \\"\\"\\"Provide degraded but useful response.\\"\\"\\"\\n        # Check cache first\\n        cached = self.cache.get(query)\\n        if cached:\\n            return cached\\n\\n        # Return helpful error message\\n        return (\\n            \\"I\'m currently experiencing high demand. \\"\\n            \\"Please try again in a few moments or contact support.\\"\\n        )\\n```\\n\\n## Layer 4: Security & Compliance\\n\\nAI agents have unique security considerations.\\n\\n### Security Checklist\\n\\n- [ ] **Input sanitization** - Prevent prompt injection\\n- [ ] **Output filtering** - Block sensitive data exposure\\n- [ ] **API key management** - Secure credential storage\\n- [ ] **Access control** - Proper authentication/authorization\\n- [ ] **Audit logging** - All actions are logged for compliance\\n- [ ] **Data retention** - Policies for data lifecycle\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Security Layers\\"\\n        A[User Input] --\x3e B[Input Sanitization]\\n        B --\x3e C[Prompt Injection Detection]\\n        C --\x3e D[Agent Processing]\\n        D --\x3e E[Output Filtering]\\n        E --\x3e F[PII Detection]\\n        F --\x3e G[Safe Response]\\n    end\\n\\n    B -.->|Log| H[Audit Trail]\\n    C -.->|Alert| I[Security Team]\\n    F -.->|Log| H\\n\\n    style C fill:#EF4444,stroke:#DC2626,color:#fff\\n    style F fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n```python\\nimport re\\nfrom typing import Optional\\n\\nclass SecureAgent:\\n    \\"\\"\\"Agent with security hardening.\\"\\"\\"\\n\\n    # Patterns that might indicate prompt injection\\n    INJECTION_PATTERNS = [\\n        r\\"ignore\\\\s+(previous|all)\\\\s+instructions\\",\\n        r\\"disregard\\\\s+(previous|all)\\\\s+instructions\\",\\n        r\\"you\\\\s+are\\\\s+now\\\\s+\\",\\n        r\\"new\\\\s+instructions:\\",\\n        r\\"system\\\\s+prompt:\\",\\n    ]\\n\\n    # PII patterns to detect and redact\\n    PII_PATTERNS = {\\n        \\"ssn\\": r\\"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\\",\\n        \\"credit_card\\": r\\"\\\\b\\\\d{4}[-\\\\s]?\\\\d{4}[-\\\\s]?\\\\d{4}[-\\\\s]?\\\\d{4}\\\\b\\",\\n        \\"email\\": r\\"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\\",\\n        \\"phone\\": r\\"\\\\b\\\\d{3}[-.]?\\\\d{3}[-.]?\\\\d{4}\\\\b\\",\\n    }\\n\\n    def sanitize_input(self, user_input: str) -> str:\\n        \\"\\"\\"Check for prompt injection attempts.\\"\\"\\"\\n        lower_input = user_input.lower()\\n\\n        for pattern in self.INJECTION_PATTERNS:\\n            if re.search(pattern, lower_input, re.IGNORECASE):\\n                agentops.record(ActionEvent(\\n                    action_type=\\"security_alert\\",\\n                    params={\\n                        \\"type\\": \\"prompt_injection_attempt\\",\\n                        \\"pattern_matched\\": pattern\\n                    }\\n                ))\\n                raise SecurityException(\\"Potentially malicious input detected\\")\\n\\n        return user_input\\n\\n    def filter_output(self, response: str) -> str:\\n        \\"\\"\\"Remove or redact sensitive information from output.\\"\\"\\"\\n        filtered = response\\n\\n        for pii_type, pattern in self.PII_PATTERNS.items():\\n            matches = re.findall(pattern, filtered)\\n            if matches:\\n                agentops.record(ActionEvent(\\n                    action_type=\\"pii_detected\\",\\n                    params={\\n                        \\"type\\": pii_type,\\n                        \\"count\\": len(matches)\\n                    }\\n                ))\\n                filtered = re.sub(pattern, f\\"[REDACTED_{pii_type.upper()}]\\", filtered)\\n\\n        return filtered\\n\\n    async def run(self, user_input: str) -> str:\\n        \\"\\"\\"Run agent with security checks.\\"\\"\\"\\n        # Sanitize input\\n        safe_input = self.sanitize_input(user_input)\\n\\n        # Process\\n        response = await self._process(safe_input)\\n\\n        # Filter output\\n        safe_output = self.filter_output(response)\\n\\n        return safe_output\\n```\\n\\n## Layer 5: Scale & Performance\\n\\nFinally, ensure your agent can handle production load.\\n\\n### Scale Checklist\\n\\n- [ ] **Load testing** - Verified behavior under expected load\\n- [ ] **Horizontal scaling** - Can add instances as needed\\n- [ ] **Caching strategy** - Reduce redundant LLM calls\\n- [ ] **Queue management** - Handle traffic bursts\\n- [ ] **Resource limits** - Memory and CPU bounds\\n- [ ] **Cost projections** - Understood cost at scale\\n\\n```mermaid\\ngraph LR\\n    subgraph \\"Scaled Architecture\\"\\n        LB[Load Balancer] --\x3e A1[Agent 1]\\n        LB --\x3e A2[Agent 2]\\n        LB --\x3e A3[Agent 3]\\n\\n        A1 --\x3e Cache[(Redis Cache)]\\n        A2 --\x3e Cache\\n        A3 --\x3e Cache\\n\\n        A1 --\x3e Q[Task Queue]\\n        A2 --\x3e Q\\n        A3 --\x3e Q\\n\\n        Q --\x3e W1[Worker 1]\\n        Q --\x3e W2[Worker 2]\\n\\n        W1 --\x3e LLM[LLM API]\\n        W2 --\x3e LLM\\n    end\\n\\n    style LB fill:#0066FF,stroke:#3399FF,color:#fff\\n    style Cache fill:#22C55E,stroke:#16A34A,color:#fff\\n    style Q fill:#F59E0B,stroke:#D97706,color:#fff\\n```\\n\\n```python\\nimport asyncio\\nfrom redis import Redis\\nimport hashlib\\n\\nclass ScalableAgent:\\n    \\"\\"\\"Agent designed for horizontal scaling.\\"\\"\\"\\n\\n    def __init__(self):\\n        self.redis = Redis(host=\\"redis\\", port=6379)\\n        self.semaphore = asyncio.Semaphore(10)  # Limit concurrent LLM calls\\n\\n    async def run(self, query: str) -> str:\\n        # Check cache first\\n        cache_key = self._make_cache_key(query)\\n        cached = self.redis.get(cache_key)\\n\\n        if cached:\\n            agentops.record(ActionEvent(\\n                action_type=\\"cache_hit\\",\\n                params={\\"cache_key\\": cache_key[:16]}\\n            ))\\n            return cached.decode()\\n\\n        # Rate limit LLM calls\\n        async with self.semaphore:\\n            result = await self._process(query)\\n\\n        # Cache result\\n        self.redis.setex(cache_key, 3600, result)  # 1 hour TTL\\n\\n        return result\\n\\n    def _make_cache_key(self, query: str) -> str:\\n        \\"\\"\\"Create deterministic cache key.\\"\\"\\"\\n        return f\\"agent:v2:{hashlib.sha256(query.encode()).hexdigest()[:16]}\\"\\n```\\n\\n## Production Deployment Checklist\\n\\nUse this checklist before going live:\\n\\n### Pre-Launch\\n\\n- [ ] All functional tests passing\\n- [ ] Load test completed successfully\\n- [ ] Security audit completed\\n- [ ] Observability configured and verified\\n- [ ] Alerting rules set up\\n- [ ] Runbooks documented\\n- [ ] Rollback plan ready\\n\\n### Launch Day\\n\\n- [ ] Canary deployment (5% traffic)\\n- [ ] Monitor error rates\\n- [ ] Monitor latency\\n- [ ] Monitor costs\\n- [ ] Gradually increase traffic\\n- [ ] Full deployment\\n\\n### Post-Launch\\n\\n- [ ] Daily metrics review\\n- [ ] Weekly cost analysis\\n- [ ] Monthly security review\\n- [ ] Quarterly load testing\\n- [ ] Continuous improvement\\n\\n## Summary\\n\\nBuilding production-ready AI agents requires attention to multiple dimensions:\\n\\n```mermaid\\nmindmap\\n    root((Production<br/>Ready))\\n        Functionality\\n            Input Validation\\n            Output Verification\\n            Edge Cases\\n        Observability\\n            Session Tracking\\n            Cost Monitoring\\n            Performance Metrics\\n        Reliability\\n            Retry Logic\\n            Circuit Breakers\\n            Fallbacks\\n        Security\\n            Input Sanitization\\n            Output Filtering\\n            Audit Logging\\n        Scale\\n            Caching\\n            Load Balancing\\n            Queue Management\\n```\\n\\nTake it one layer at a time. Each improvement makes your agent more robust and trustworthy.\\n\\n---\\n\\n*Ready to monitor your production agents? [Get started with AgentOps](/docs/getting-started/quickstart) and gain full visibility into your AI systems.*"},{"id":"langchain-observability-complete-guide","metadata":{"permalink":"/docusaurus-guide-3/blog/langchain-observability-complete-guide","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-04-12-langchain-observability.md","source":"@site/blog/2024-04-12-langchain-observability.md","title":"LangChain + AgentOps: Complete Observability Guide","description":"Learn how to add full observability to your LangChain agents with AgentOps. Step-by-step guide with examples.","date":"2024-04-12T00:00:00.000Z","tags":[{"inline":true,"label":"langchain","permalink":"/docusaurus-guide-3/blog/tags/langchain"},{"inline":true,"label":"integration","permalink":"/docusaurus-guide-3/blog/tags/integration"},{"inline":true,"label":"tutorial","permalink":"/docusaurus-guide-3/blog/tags/tutorial"},{"inline":true,"label":"observability","permalink":"/docusaurus-guide-3/blog/tags/observability"}],"readingTime":7.05,"hasTruncateMarker":true,"authors":[{"name":"Emily Zhang","title":"Developer Advocate at AgentOps","url":"https://github.com/emilyzhang","image_url":"https://images.unsplash.com/photo-1580489944761-15a19d654956?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1580489944761-15a19d654956?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"langchain-observability-complete-guide","title":"LangChain + AgentOps: Complete Observability Guide","authors":[{"name":"Emily Zhang","title":"Developer Advocate at AgentOps","url":"https://github.com/emilyzhang","image_url":"https://images.unsplash.com/photo-1580489944761-15a19d654956?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1580489944761-15a19d654956?w=150&h=150&fit=crop&crop=face"}],"tags":["langchain","integration","tutorial","observability"],"description":"Learn how to add full observability to your LangChain agents with AgentOps. Step-by-step guide with examples.","image":"https://images.unsplash.com/photo-1639322537228-f710d846310a?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"Building Production-Ready AI Agents: A Complete Checklist","permalink":"/docusaurus-guide-3/blog/building-production-ready-ai-agents"},"nextItem":{"title":"The Complete Guide to LLM Cost Optimization for AI Agents","permalink":"/docusaurus-guide-3/blog/cost-optimization-llm-agents"}},"content":"![LangChain Observability](https://images.unsplash.com/photo-1639322537228-f710d846310a?w=1200&h=400&fit=crop)\\n\\nLangChain is one of the most popular frameworks for building AI agents, but debugging LangChain applications can be challenging. This guide shows you how to add comprehensive observability to your LangChain agents using AgentOps.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why Observability Matters for LangChain\\n\\nLangChain applications involve multiple components working together:\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"LangChain Application\\"\\n        A[User Input] --\x3e B[Prompt Template]\\n        B --\x3e C[LLM]\\n        C --\x3e D{Agent Decision}\\n        D --\x3e|Tool Call| E[Tool Executor]\\n        E --\x3e F[Tool 1: Search]\\n        E --\x3e G[Tool 2: Calculator]\\n        E --\x3e H[Tool 3: Database]\\n        F --\x3e D\\n        G --\x3e D\\n        H --\x3e D\\n        D --\x3e|Final Answer| I[Output Parser]\\n        I --\x3e J[Response]\\n    end\\n\\n    style C fill:#0066FF,stroke:#3399FF,color:#fff\\n    style E fill:#22C55E,stroke:#16A34A,color:#fff\\n    style D fill:#F59E0B,stroke:#D97706,color:#fff\\n```\\n\\nWithout observability, you\'re flying blind. You can\'t see:\\n- Which tools are being called and why\\n- How much each LLM call costs\\n- Where bottlenecks occur in the chain\\n- Why the agent made certain decisions\\n\\n## Quick Setup\\n\\nGetting started takes just two lines of code:\\n\\n```python\\nimport agentops\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# Your existing LangChain code works unchanged!\\nfrom langchain.agents import create_openai_functions_agent, AgentExecutor\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.tools import tool\\n\\n@tool\\ndef search_web(query: str) -> str:\\n    \\"\\"\\"Search the web for information.\\"\\"\\"\\n    # Your search implementation\\n    return f\\"Results for: {query}\\"\\n\\nllm = ChatOpenAI(model=\\"gpt-4\\")\\nagent = create_openai_functions_agent(llm, [search_web], prompt)\\nexecutor = AgentExecutor(agent=agent, tools=[search_web])\\n\\n# AgentOps automatically tracks everything!\\nresult = executor.invoke({\\"input\\": \\"What\'s the weather in Tokyo?\\"})\\n```\\n\\nThat\'s it! AgentOps automatically instruments:\\n- All LLM calls with prompts and responses\\n- Tool invocations with inputs and outputs\\n- Agent reasoning steps\\n- Token usage and costs\\n\\n## Deep Dive: What Gets Tracked\\n\\n### LLM Calls\\n\\nEvery call to the LLM is captured with full context:\\n\\n```mermaid\\nsequenceDiagram\\n    participant App as LangChain App\\n    participant AO as AgentOps\\n    participant LLM as OpenAI\\n\\n    App->>AO: Start tracking\\n    App->>LLM: chat.completions.create()\\n    Note over AO: Captures: model, messages,<br/>temperature, tokens\\n    LLM--\x3e>App: Response\\n    Note over AO: Captures: response content,<br/>finish reason, usage\\n    AO->>AO: Calculate cost\\n    App->>AO: End tracking\\n```\\n\\nIn the AgentOps dashboard, you see:\\n\\n| Metric | Example Value |\\n|--------|---------------|\\n| Model | gpt-4-0125-preview |\\n| Input Tokens | 1,234 |\\n| Output Tokens | 567 |\\n| Latency | 2.3s |\\n| Cost | $0.0456 |\\n| Prompt | \\"You are a helpful assistant...\\" |\\n| Response | \\"Based on my search...\\" |\\n\\n### Tool Executions\\n\\nTool calls are tracked with timing and results:\\n\\n```python\\nfrom langchain.tools import tool\\nfrom agentops import ActionEvent\\n\\n@tool\\ndef database_query(query: str) -> str:\\n    \\"\\"\\"Execute a database query and return results.\\"\\"\\"\\n    # AgentOps automatically tracks this tool execution\\n    # But you can add custom events for more detail:\\n\\n    agentops.record(ActionEvent(\\n        action_type=\\"database_query\\",\\n        params={\\n            \\"query\\": query,\\n            \\"table\\": extract_table_name(query)\\n        }\\n    ))\\n\\n    results = execute_query(query)\\n\\n    agentops.record(ActionEvent(\\n        action_type=\\"database_result\\",\\n        params={\\n            \\"row_count\\": len(results),\\n            \\"columns\\": list(results.columns)\\n        }\\n    ))\\n\\n    return results.to_string()\\n```\\n\\n### Agent Reasoning\\n\\nSee exactly how your agent thinks:\\n\\n```mermaid\\nflowchart TD\\n    A[\\"User: What\'s the capital of France<br/>and its population?\\"] --\x3e B[Agent Reasoning]\\n\\n    B --\x3e C[\\"Thought: I need to find two pieces<br/>of information\\"]\\n    C --\x3e D[\\"Action: search_web\\"]\\n    D --\x3e E[\\"Input: capital of France\\"]\\n    E --\x3e F[\\"Observation: Paris is the capital\\"]\\n\\n    F --\x3e G[\\"Thought: Now I need the population\\"]\\n    G --\x3e H[\\"Action: search_web\\"]\\n    H --\x3e I[\\"Input: population of Paris\\"]\\n    I --\x3e J[\\"Observation: 2.1 million\\"]\\n\\n    J --\x3e K[\\"Thought: I have both answers\\"]\\n    K --\x3e L[\\"Final Answer: Paris, 2.1 million\\"]\\n\\n    style A fill:#0066FF,stroke:#3399FF,color:#fff\\n    style L fill:#22C55E,stroke:#16A34A,color:#fff\\n```\\n\\n## Advanced Integration Patterns\\n\\n### Custom Callbacks\\n\\nFor fine-grained control, use custom callbacks:\\n\\n```python\\nfrom langchain.callbacks.base import BaseCallbackHandler\\nfrom agentops import ActionEvent\\n\\nclass AgentOpsCallback(BaseCallbackHandler):\\n    \\"\\"\\"Custom callback for detailed AgentOps integration.\\"\\"\\"\\n\\n    def on_chain_start(self, serialized, inputs, **kwargs):\\n        agentops.record(ActionEvent(\\n            action_type=\\"chain_start\\",\\n            params={\\n                \\"chain_type\\": serialized.get(\\"name\\", \\"unknown\\"),\\n                \\"inputs\\": str(inputs)[:500]  # Truncate for storage\\n            }\\n        ))\\n\\n    def on_chain_end(self, outputs, **kwargs):\\n        agentops.record(ActionEvent(\\n            action_type=\\"chain_end\\",\\n            params={\\"outputs\\": str(outputs)[:500]}\\n        ))\\n\\n    def on_agent_action(self, action, **kwargs):\\n        agentops.record(ActionEvent(\\n            action_type=\\"agent_action\\",\\n            params={\\n                \\"tool\\": action.tool,\\n                \\"tool_input\\": str(action.tool_input)[:500],\\n                \\"log\\": action.log[:200]\\n            }\\n        ))\\n\\n    def on_tool_error(self, error, **kwargs):\\n        agentops.record(ActionEvent(\\n            action_type=\\"tool_error\\",\\n            params={\\n                \\"error_type\\": type(error).__name__,\\n                \\"error_message\\": str(error)\\n            }\\n        ))\\n\\n# Use the callback\\nexecutor = AgentExecutor(\\n    agent=agent,\\n    tools=tools,\\n    callbacks=[AgentOpsCallback()]\\n)\\n```\\n\\n### Tracking RAG Pipelines\\n\\nFor Retrieval-Augmented Generation:\\n\\n```python\\nfrom langchain.vectorstores import Chroma\\nfrom langchain.embeddings import OpenAIEmbeddings\\nfrom langchain.chains import RetrievalQA\\n\\n# Initialize with AgentOps tracking\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# Create your RAG pipeline\\nembeddings = OpenAIEmbeddings()\\nvectorstore = Chroma(embedding_function=embeddings)\\n\\n# Track retrieval metrics\\nclass TrackedRetriever:\\n    def __init__(self, retriever):\\n        self.retriever = retriever\\n\\n    def get_relevant_documents(self, query: str):\\n        start_time = time.time()\\n\\n        docs = self.retriever.get_relevant_documents(query)\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"retrieval\\",\\n            params={\\n                \\"query\\": query,\\n                \\"num_docs_retrieved\\": len(docs),\\n                \\"latency_ms\\": (time.time() - start_time) * 1000,\\n                \\"doc_sources\\": [doc.metadata.get(\\"source\\") for doc in docs]\\n            }\\n        ))\\n\\n        return docs\\n\\n# Use in your chain\\nqa_chain = RetrievalQA.from_chain_type(\\n    llm=ChatOpenAI(),\\n    retriever=TrackedRetriever(vectorstore.as_retriever())\\n)\\n```\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"RAG Pipeline with Observability\\"\\n        A[Query] --\x3e B[Embedding]\\n        B --\x3e C[Vector Search]\\n        C --\x3e D[Retrieved Docs]\\n        D --\x3e E[Context Assembly]\\n        E --\x3e F[LLM]\\n        F --\x3e G[Response]\\n\\n        B -.->|Track| H[AgentOps]\\n        C -.->|Track| H\\n        F -.->|Track| H\\n    end\\n\\n    style H fill:#0066FF,stroke:#3399FF,color:#fff\\n```\\n\\n### Multi-Chain Workflows\\n\\nTrack complex workflows with multiple chains:\\n\\n```python\\nfrom langchain.chains import SequentialChain\\n\\n# Create multiple chains\\nresearch_chain = LLMChain(llm=llm, prompt=research_prompt, output_key=\\"research\\")\\nanalysis_chain = LLMChain(llm=llm, prompt=analysis_prompt, output_key=\\"analysis\\")\\nsummary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key=\\"summary\\")\\n\\n# Combine into sequential chain\\nfull_pipeline = SequentialChain(\\n    chains=[research_chain, analysis_chain, summary_chain],\\n    input_variables=[\\"topic\\"],\\n    output_variables=[\\"research\\", \\"analysis\\", \\"summary\\"]\\n)\\n\\n# Track with custom session tags\\nwith agentops.start_session(tags=[\\"multi-chain\\", \\"research-pipeline\\"]):\\n    result = full_pipeline.invoke({\\"topic\\": \\"AI in healthcare\\"})\\n\\n    # Add session-level metrics\\n    agentops.record(ActionEvent(\\n        action_type=\\"pipeline_complete\\",\\n        params={\\n            \\"topic\\": \\"AI in healthcare\\",\\n            \\"research_length\\": len(result[\\"research\\"]),\\n            \\"analysis_length\\": len(result[\\"analysis\\"]),\\n            \\"summary_length\\": len(result[\\"summary\\"])\\n        }\\n    ))\\n```\\n\\n## Debugging Common Issues\\n\\n### Issue 1: Agent Loops\\n\\nWhen your agent gets stuck in a loop:\\n\\n```mermaid\\nflowchart LR\\n    A[Detect Loop] --\x3e B{Loop Count > 3?}\\n    B --\x3e|Yes| C[Alert in AgentOps]\\n    B --\x3e|No| D[Continue]\\n    C --\x3e E[Review in Dashboard]\\n    E --\x3e F[Identify Pattern]\\n    F --\x3e G[Fix Prompt/Tools]\\n\\n    style C fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\nAgentOps shows you:\\n- The exact sequence of tool calls\\n- What inputs triggered the loop\\n- How many iterations occurred\\n\\n### Issue 2: High Latency\\n\\nIdentify bottlenecks in your chain:\\n\\n```python\\n# AgentOps automatically tracks timing, but you can add custom spans\\nwith agentops.start_span(\\"custom_processing\\"):\\n    # Your slow operation\\n    processed_data = expensive_operation(data)\\n\\n# In the dashboard, you\'ll see timing breakdown:\\n# - LLM calls: 2.5s\\n# - Tool executions: 1.2s\\n# - Custom processing: 0.8s\\n# - Other: 0.1s\\n```\\n\\n### Issue 3: Unexpected Costs\\n\\nTrack costs in real-time:\\n\\n```python\\n# Set up cost alerts\\nagentops.init(\\n    api_key=\\"your-api-key\\",\\n    tags=[\\"production\\"],\\n    # Get alerted if session cost exceeds $1\\n    max_session_cost=1.0\\n)\\n```\\n\\n## Best Practices\\n\\n### 1. Use Meaningful Tags\\n\\n```python\\nagentops.init(api_key=\\"your-api-key\\")\\n\\n# Start sessions with descriptive tags\\nwith agentops.start_session(tags=[\\n    \\"customer-support\\",\\n    \\"tier-enterprise\\",\\n    f\\"user-{user_id}\\",\\n    f\\"version-{app_version}\\"\\n]):\\n    result = agent.run(user_query)\\n```\\n\\n### 2. Record Business Events\\n\\n```python\\n# Track business-relevant events, not just technical ones\\nagentops.record(ActionEvent(\\n    action_type=\\"customer_intent\\",\\n    params={\\n        \\"intent\\": \\"refund_request\\",\\n        \\"sentiment\\": \\"frustrated\\",\\n        \\"priority\\": \\"high\\"\\n    }\\n))\\n```\\n\\n### 3. Set Up Alerts\\n\\nConfigure alerts for:\\n- Cost exceeding thresholds\\n- Error rate spikes\\n- Latency increases\\n- Agent loop detection\\n\\n## Real-World Example\\n\\nHere\'s a complete example of a production-ready LangChain agent with full observability:\\n\\n```python\\nimport agentops\\nfrom langchain.agents import create_openai_functions_agent, AgentExecutor\\nfrom langchain_openai import ChatOpenAI\\nfrom langchain.tools import tool\\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\\n\\n# Initialize AgentOps\\nagentops.init(\\n    api_key=\\"your-api-key\\",\\n    default_tags=[\\"production\\", \\"customer-support\\"]\\n)\\n\\n# Define tools with clear descriptions\\n@tool\\ndef search_knowledge_base(query: str) -> str:\\n    \\"\\"\\"Search our knowledge base for relevant articles.\\"\\"\\"\\n    results = kb_client.search(query, limit=3)\\n    return \\"\\\\n\\".join([r.content for r in results])\\n\\n@tool\\ndef get_customer_info(customer_id: str) -> str:\\n    \\"\\"\\"Get customer information including plan and history.\\"\\"\\"\\n    customer = db.get_customer(customer_id)\\n    return f\\"Name: {customer.name}, Plan: {customer.plan}, Since: {customer.created_at}\\"\\n\\n@tool\\ndef create_support_ticket(title: str, description: str, priority: str) -> str:\\n    \\"\\"\\"Create a support ticket for issues that need human attention.\\"\\"\\"\\n    ticket = ticketing.create(title=title, description=description, priority=priority)\\n\\n    agentops.record(ActionEvent(\\n        action_type=\\"ticket_created\\",\\n        params={\\"ticket_id\\": ticket.id, \\"priority\\": priority}\\n    ))\\n\\n    return f\\"Ticket created: {ticket.id}\\"\\n\\n# Create agent\\ntools = [search_knowledge_base, get_customer_info, create_support_ticket]\\nprompt = ChatPromptTemplate.from_messages([\\n    (\\"system\\", \\"You are a helpful customer support agent.\\"),\\n    (\\"human\\", \\"{input}\\"),\\n    MessagesPlaceholder(variable_name=\\"agent_scratchpad\\")\\n])\\n\\nllm = ChatOpenAI(model=\\"gpt-4\\", temperature=0)\\nagent = create_openai_functions_agent(llm, tools, prompt)\\nexecutor = AgentExecutor(agent=agent, tools=tools, verbose=True)\\n\\n# Run with session tracking\\ndef handle_customer_query(customer_id: str, query: str):\\n    with agentops.start_session(tags=[f\\"customer-{customer_id}\\"]):\\n        try:\\n            result = executor.invoke({\\n                \\"input\\": f\\"Customer ID: {customer_id}\\\\nQuery: {query}\\"\\n            })\\n\\n            agentops.record(ActionEvent(\\n                action_type=\\"query_resolved\\",\\n                params={\\n                    \\"customer_id\\": customer_id,\\n                    \\"resolution_type\\": \\"automated\\"\\n                }\\n            ))\\n\\n            agentops.end_session(end_state=\\"Success\\")\\n            return result[\\"output\\"]\\n\\n        except Exception as e:\\n            agentops.end_session(end_state=\\"Fail\\", end_state_reason=str(e))\\n            raise\\n```\\n\\n---\\n\\n*Ready to add observability to your LangChain agents? [Get started free](/docs/getting-started/quickstart) and see what your agents are really doing.*"},{"id":"cost-optimization-llm-agents","metadata":{"permalink":"/docusaurus-guide-3/blog/cost-optimization-llm-agents","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-03-05-cost-optimization-llm.md","source":"@site/blog/2024-03-05-cost-optimization-llm.md","title":"The Complete Guide to LLM Cost Optimization for AI Agents","description":"Cut your AI agent LLM costs by up to 80% with these proven optimization strategies. Real examples and code included.","date":"2024-03-05T00:00:00.000Z","tags":[{"inline":true,"label":"cost-optimization","permalink":"/docusaurus-guide-3/blog/tags/cost-optimization"},{"inline":true,"label":"llm","permalink":"/docusaurus-guide-3/blog/tags/llm"},{"inline":true,"label":"performance","permalink":"/docusaurus-guide-3/blog/tags/performance"},{"inline":true,"label":"tutorial","permalink":"/docusaurus-guide-3/blog/tags/tutorial"}],"readingTime":7.58,"hasTruncateMarker":true,"authors":[{"name":"Marcus Johnson","title":"Solutions Architect at AgentOps","url":"https://github.com/marcusj","image_url":"https://images.unsplash.com/photo-1472099645785-5658abf4ff4e?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1472099645785-5658abf4ff4e?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"cost-optimization-llm-agents","title":"The Complete Guide to LLM Cost Optimization for AI Agents","authors":[{"name":"Marcus Johnson","title":"Solutions Architect at AgentOps","url":"https://github.com/marcusj","image_url":"https://images.unsplash.com/photo-1472099645785-5658abf4ff4e?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1472099645785-5658abf4ff4e?w=150&h=150&fit=crop&crop=face"}],"tags":["cost-optimization","llm","performance","tutorial"],"description":"Cut your AI agent LLM costs by up to 80% with these proven optimization strategies. Real examples and code included.","image":"https://images.unsplash.com/photo-1554224155-6726b3ff858f?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"LangChain + AgentOps: Complete Observability Guide","permalink":"/docusaurus-guide-3/blog/langchain-observability-complete-guide"},"nextItem":{"title":"Debugging Multi-Agent Systems: A Practical Guide","permalink":"/docusaurus-guide-3/blog/debugging-multi-agent-systems"}},"content":"![Cost Optimization](https://images.unsplash.com/photo-1554224155-6726b3ff858f?w=1200&h=400&fit=crop)\\n\\nRunning AI agents in production can get expensive fast. We\'ve seen teams spend $100,000+ per month on LLM API calls without realizing how much they could save. This guide shares the strategies that have helped our customers reduce costs by 50-80%.\\n\\n\x3c!--truncate--\x3e\\n\\n## Understanding LLM Costs\\n\\nBefore optimizing, you need to understand where your money goes:\\n\\n```mermaid\\npie showData\\n    title Typical Agent LLM Cost Breakdown\\n    \\"Input Tokens\\" : 35\\n    \\"Output Tokens\\" : 45\\n    \\"Redundant Calls\\" : 15\\n    \\"Failed Retries\\" : 5\\n```\\n\\nMost teams are surprised to learn that **15-20% of their costs come from redundant or unnecessary calls**. That\'s the low-hanging fruit.\\n\\n## The Cost Optimization Framework\\n\\nWe use a three-tier approach to cost optimization:\\n\\n```mermaid\\nflowchart TB\\n    subgraph Tier1[\\"Tier 1: Quick Wins (Day 1)\\"]\\n        A[Remove Redundant Calls]\\n        B[Fix Retry Logic]\\n        C[Implement Basic Caching]\\n    end\\n\\n    subgraph Tier2[\\"Tier 2: Architecture (Week 1)\\"]\\n        D[Model Selection Strategy]\\n        E[Prompt Optimization]\\n        F[Response Streaming]\\n    end\\n\\n    subgraph Tier3[\\"Tier 3: Advanced (Month 1)\\"]\\n        G[Semantic Caching]\\n        H[Fine-tuned Models]\\n        I[Hybrid Architectures]\\n    end\\n\\n    Tier1 --\x3e Tier2 --\x3e Tier3\\n\\n    style A fill:#22C55E,stroke:#16A34A,color:#fff\\n    style B fill:#22C55E,stroke:#16A34A,color:#fff\\n    style C fill:#22C55E,stroke:#16A34A,color:#fff\\n    style D fill:#0066FF,stroke:#3399FF,color:#fff\\n    style E fill:#0066FF,stroke:#3399FF,color:#fff\\n    style F fill:#0066FF,stroke:#3399FF,color:#fff\\n    style G fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n    style H fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n    style I fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n```\\n\\n## Tier 1: Quick Wins\\n\\n### 1. Remove Redundant Calls\\n\\nThe first step is identifying and eliminating duplicate API calls:\\n\\n```python\\nimport agentops\\nfrom functools import lru_cache\\nimport hashlib\\n\\nclass OptimizedAgent:\\n    def __init__(self):\\n        self.call_cache = {}\\n        agentops.init(api_key=\\"your-api-key\\")\\n\\n    def _create_cache_key(self, prompt: str, model: str) -> str:\\n        \\"\\"\\"Create a deterministic cache key.\\"\\"\\"\\n        content = f\\"{model}:{prompt}\\"\\n        return hashlib.sha256(content.encode()).hexdigest()\\n\\n    async def call_llm(self, prompt: str, model: str = \\"gpt-4\\"):\\n        cache_key = self._create_cache_key(prompt, model)\\n\\n        # Check cache first\\n        if cache_key in self.call_cache:\\n            agentops.record(ActionEvent(\\n                action_type=\\"cache_hit\\",\\n                params={\\"cache_key\\": cache_key[:8]}\\n            ))\\n            return self.call_cache[cache_key]\\n\\n        # Make the actual call\\n        response = await self.client.chat.completions.create(\\n            model=model,\\n            messages=[{\\"role\\": \\"user\\", \\"content\\": prompt}]\\n        )\\n\\n        # Cache the result\\n        self.call_cache[cache_key] = response.choices[0].message.content\\n        return self.call_cache[cache_key]\\n```\\n\\n**Impact:** 15-25% cost reduction from caching alone.\\n\\n### 2. Fix Retry Logic\\n\\nExponential backoff with jitter prevents costly retry storms:\\n\\n```python\\nimport asyncio\\nimport random\\n\\nasync def call_with_smart_retry(func, max_retries=3, base_delay=1.0):\\n    \\"\\"\\"Smart retry with exponential backoff and jitter.\\"\\"\\"\\n    for attempt in range(max_retries):\\n        try:\\n            return await func()\\n        except RateLimitError:\\n            if attempt == max_retries - 1:\\n                raise\\n\\n            # Exponential backoff with jitter\\n            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\\n\\n            agentops.record(ActionEvent(\\n                action_type=\\"rate_limit_retry\\",\\n                params={\\n                    \\"attempt\\": attempt + 1,\\n                    \\"delay\\": delay\\n                }\\n            ))\\n\\n            await asyncio.sleep(delay)\\n        except Exception as e:\\n            # Don\'t retry on non-retryable errors\\n            agentops.record(ActionEvent(\\n                action_type=\\"non_retryable_error\\",\\n                params={\\"error\\": str(e)}\\n            ))\\n            raise\\n```\\n\\n### 3. Implement Basic Caching\\n\\nUse TTL-based caching for semi-dynamic content:\\n\\n```python\\nfrom datetime import datetime, timedelta\\nfrom dataclasses import dataclass\\n\\n@dataclass\\nclass CacheEntry:\\n    value: str\\n    expires_at: datetime\\n    hit_count: int = 0\\n\\nclass TTLCache:\\n    def __init__(self, default_ttl: timedelta = timedelta(hours=1)):\\n        self.cache: dict[str, CacheEntry] = {}\\n        self.default_ttl = default_ttl\\n\\n    def get(self, key: str) -> str | None:\\n        if key in self.cache:\\n            entry = self.cache[key]\\n            if datetime.now() < entry.expires_at:\\n                entry.hit_count += 1\\n                return entry.value\\n            else:\\n                del self.cache[key]\\n        return None\\n\\n    def set(self, key: str, value: str, ttl: timedelta = None):\\n        self.cache[key] = CacheEntry(\\n            value=value,\\n            expires_at=datetime.now() + (ttl or self.default_ttl)\\n        )\\n```\\n\\n## Tier 2: Architecture Optimization\\n\\n### 4. Model Selection Strategy\\n\\nNot every task needs GPT-4. Use the right model for the job:\\n\\n```mermaid\\nflowchart TD\\n    A[Incoming Task] --\x3e B{Task Complexity?}\\n\\n    B --\x3e|Simple| C[GPT-3.5 Turbo<br/>$0.0015/1K tokens]\\n    B --\x3e|Medium| D[GPT-4 Turbo<br/>$0.01/1K tokens]\\n    B --\x3e|Complex| E[GPT-4<br/>$0.03/1K tokens]\\n    B --\x3e|Code| F[Claude 3.5 Sonnet<br/>$0.003/1K tokens]\\n\\n    C --\x3e G[Classification<br/>Simple Q&A<br/>Formatting]\\n    D --\x3e H[Analysis<br/>Summarization<br/>Standard Generation]\\n    E --\x3e I[Reasoning<br/>Complex Planning<br/>Multi-step Tasks]\\n    F --\x3e J[Code Generation<br/>Code Review<br/>Technical Writing]\\n\\n    style C fill:#22C55E,stroke:#16A34A,color:#fff\\n    style D fill:#F59E0B,stroke:#D97706,color:#fff\\n    style E fill:#EF4444,stroke:#DC2626,color:#fff\\n    style F fill:#0066FF,stroke:#3399FF,color:#fff\\n```\\n\\nImplement automatic model selection:\\n\\n```python\\nclass ModelRouter:\\n    \\"\\"\\"Routes tasks to the most cost-effective model.\\"\\"\\"\\n\\n    COMPLEXITY_MODELS = {\\n        \\"simple\\": \\"gpt-3.5-turbo\\",\\n        \\"medium\\": \\"gpt-4-turbo-preview\\",\\n        \\"complex\\": \\"gpt-4\\",\\n        \\"code\\": \\"claude-3-sonnet-20240229\\"\\n    }\\n\\n    def __init__(self):\\n        self.classifier = self._load_classifier()\\n\\n    def route(self, task: str) -> str:\\n        \\"\\"\\"Determine the best model for a task.\\"\\"\\"\\n        complexity = self._classify_complexity(task)\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"model_routed\\",\\n            params={\\n                \\"complexity\\": complexity,\\n                \\"model\\": self.COMPLEXITY_MODELS[complexity]\\n            }\\n        ))\\n\\n        return self.COMPLEXITY_MODELS[complexity]\\n\\n    def _classify_complexity(self, task: str) -> str:\\n        \\"\\"\\"Classify task complexity using a lightweight model.\\"\\"\\"\\n        # Use a simple heuristic or lightweight classifier\\n        if any(kw in task.lower() for kw in [\\"list\\", \\"format\\", \\"convert\\"]):\\n            return \\"simple\\"\\n        elif any(kw in task.lower() for kw in [\\"code\\", \\"function\\", \\"implement\\"]):\\n            return \\"code\\"\\n        elif any(kw in task.lower() for kw in [\\"analyze\\", \\"compare\\", \\"explain\\"]):\\n            return \\"medium\\"\\n        else:\\n            return \\"complex\\"\\n```\\n\\n### 5. Prompt Optimization\\n\\nShorter prompts = lower costs. But don\'t sacrifice quality:\\n\\n```python\\nclass PromptOptimizer:\\n    \\"\\"\\"Optimizes prompts for cost without sacrificing quality.\\"\\"\\"\\n\\n    def optimize(self, prompt: str) -> str:\\n        \\"\\"\\"Apply optimization techniques to a prompt.\\"\\"\\"\\n        optimized = prompt\\n\\n        # Remove redundant whitespace\\n        optimized = \\" \\".join(optimized.split())\\n\\n        # Use abbreviations for common phrases\\n        replacements = {\\n            \\"Please provide\\": \\"Provide\\",\\n            \\"I would like you to\\": \\"\\",\\n            \\"Can you please\\": \\"\\",\\n            \\"It would be great if you could\\": \\"\\",\\n        }\\n\\n        for old, new in replacements.items():\\n            optimized = optimized.replace(old, new)\\n\\n        # Calculate savings\\n        original_tokens = len(prompt.split()) * 1.3  # Rough estimate\\n        optimized_tokens = len(optimized.split()) * 1.3\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"prompt_optimized\\",\\n            params={\\n                \\"original_tokens\\": int(original_tokens),\\n                \\"optimized_tokens\\": int(optimized_tokens),\\n                \\"savings_percent\\": round((1 - optimized_tokens/original_tokens) * 100, 1)\\n            }\\n        ))\\n\\n        return optimized.strip()\\n```\\n\\n### 6. Response Streaming\\n\\nStream responses to fail fast and reduce wasted tokens:\\n\\n```python\\nasync def stream_with_early_stop(prompt: str, stop_conditions: list[str]):\\n    \\"\\"\\"Stream response and stop early if conditions are met.\\"\\"\\"\\n    collected_text = \\"\\"\\n    tokens_saved = 0\\n\\n    async for chunk in client.chat.completions.create(\\n        model=\\"gpt-4\\",\\n        messages=[{\\"role\\": \\"user\\", \\"content\\": prompt}],\\n        stream=True\\n    ):\\n        content = chunk.choices[0].delta.content or \\"\\"\\n        collected_text += content\\n\\n        # Check stop conditions\\n        for condition in stop_conditions:\\n            if condition in collected_text:\\n                agentops.record(ActionEvent(\\n                    action_type=\\"early_stop\\",\\n                    params={\\n                        \\"condition\\": condition,\\n                        \\"tokens_collected\\": len(collected_text.split())\\n                    }\\n                ))\\n                return collected_text\\n\\n    return collected_text\\n```\\n\\n## Tier 3: Advanced Optimization\\n\\n### 7. Semantic Caching\\n\\nCache based on meaning, not exact matches:\\n\\n```mermaid\\nflowchart LR\\n    A[New Query] --\x3e B[Generate Embedding]\\n    B --\x3e C{Similar Query<br/>in Cache?}\\n    C --\x3e|Yes, >95% similar| D[Return Cached Response]\\n    C --\x3e|No| E[Call LLM]\\n    E --\x3e F[Cache Response]\\n    F --\x3e G[Return Response]\\n    D --\x3e H[Log Cache Hit]\\n\\n    style D fill:#22C55E,stroke:#16A34A,color:#fff\\n    style E fill:#F59E0B,stroke:#D97706,color:#fff\\n```\\n\\n```python\\nimport numpy as np\\nfrom openai import OpenAI\\n\\nclass SemanticCache:\\n    \\"\\"\\"Cache responses based on semantic similarity.\\"\\"\\"\\n\\n    def __init__(self, similarity_threshold: float = 0.95):\\n        self.threshold = similarity_threshold\\n        self.embeddings: list[np.ndarray] = []\\n        self.responses: list[str] = []\\n        self.queries: list[str] = []\\n\\n    def _get_embedding(self, text: str) -> np.ndarray:\\n        \\"\\"\\"Get embedding for text.\\"\\"\\"\\n        response = OpenAI().embeddings.create(\\n            model=\\"text-embedding-3-small\\",\\n            input=text\\n        )\\n        return np.array(response.data[0].embedding)\\n\\n    def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:\\n        \\"\\"\\"Calculate cosine similarity between two vectors.\\"\\"\\"\\n        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\\n\\n    def get(self, query: str) -> str | None:\\n        \\"\\"\\"Find semantically similar cached response.\\"\\"\\"\\n        query_embedding = self._get_embedding(query)\\n\\n        for i, cached_embedding in enumerate(self.embeddings):\\n            similarity = self._cosine_similarity(query_embedding, cached_embedding)\\n\\n            if similarity >= self.threshold:\\n                agentops.record(ActionEvent(\\n                    action_type=\\"semantic_cache_hit\\",\\n                    params={\\n                        \\"similarity\\": round(similarity, 3),\\n                        \\"original_query\\": self.queries[i][:50]\\n                    }\\n                ))\\n                return self.responses[i]\\n\\n        return None\\n\\n    def set(self, query: str, response: str):\\n        \\"\\"\\"Add query-response pair to cache.\\"\\"\\"\\n        embedding = self._get_embedding(query)\\n        self.embeddings.append(embedding)\\n        self.responses.append(response)\\n        self.queries.append(query)\\n```\\n\\n### 8. Cost Budgets and Alerts\\n\\nPrevent runaway costs with hard limits:\\n\\n```python\\nfrom dataclasses import dataclass\\nfrom datetime import datetime\\n\\n@dataclass\\nclass CostBudget:\\n    daily_limit: float\\n    monthly_limit: float\\n    alert_threshold: float = 0.8\\n\\nclass CostGuard:\\n    \\"\\"\\"Enforce cost budgets and send alerts.\\"\\"\\"\\n\\n    def __init__(self, budget: CostBudget):\\n        self.budget = budget\\n        self.daily_spend = 0.0\\n        self.monthly_spend = 0.0\\n        self.last_reset = datetime.now()\\n\\n    def check_and_record(self, cost: float) -> bool:\\n        \\"\\"\\"Check if cost is within budget, record spend.\\"\\"\\"\\n        self._maybe_reset()\\n\\n        # Check daily limit\\n        if self.daily_spend + cost > self.budget.daily_limit:\\n            agentops.record(ActionEvent(\\n                action_type=\\"budget_exceeded\\",\\n                params={\\n                    \\"type\\": \\"daily\\",\\n                    \\"limit\\": self.budget.daily_limit,\\n                    \\"current\\": self.daily_spend\\n                }\\n            ))\\n            return False\\n\\n        # Record the spend\\n        self.daily_spend += cost\\n        self.monthly_spend += cost\\n\\n        # Check alert thresholds\\n        if self.daily_spend > self.budget.daily_limit * self.budget.alert_threshold:\\n            self._send_alert(\\"daily\\", self.daily_spend, self.budget.daily_limit)\\n\\n        return True\\n\\n    def _send_alert(self, period: str, current: float, limit: float):\\n        \\"\\"\\"Send cost alert.\\"\\"\\"\\n        agentops.record(ActionEvent(\\n            action_type=\\"cost_alert\\",\\n            params={\\n                \\"period\\": period,\\n                \\"current_spend\\": current,\\n                \\"limit\\": limit,\\n                \\"percent_used\\": round(current / limit * 100, 1)\\n            }\\n        ))\\n```\\n\\n## Real-World Results\\n\\nHere\'s what optimization looks like in practice:\\n\\n| Optimization | Before | After | Savings |\\n|-------------|--------|-------|---------|\\n| Basic Caching | $10,000/mo | $7,500/mo | 25% |\\n| Model Routing | $7,500/mo | $4,500/mo | 40% |\\n| Prompt Optimization | $4,500/mo | $3,600/mo | 20% |\\n| Semantic Caching | $3,600/mo | $2,500/mo | 30% |\\n| **Total** | **$10,000/mo** | **$2,500/mo** | **75%** |\\n\\n## Quick Start Checklist\\n\\nStart optimizing today:\\n\\n1. **[ ] Enable AgentOps cost tracking** to see where money goes\\n2. **[ ] Implement basic caching** for repeated queries\\n3. **[ ] Add model routing** to use cheaper models for simple tasks\\n4. **[ ] Optimize prompts** to reduce token count\\n5. **[ ] Set up budget alerts** to catch runaway costs early\\n6. **[ ] Review weekly** and iterate on optimizations\\n\\n---\\n\\n*Want to see your actual cost breakdown? [Set up AgentOps](/docs/getting-started/quickstart) and get detailed cost analytics in minutes.*"},{"id":"debugging-multi-agent-systems","metadata":{"permalink":"/docusaurus-guide-3/blog/debugging-multi-agent-systems","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-02-10-debugging-multi-agent-systems.md","source":"@site/blog/2024-02-10-debugging-multi-agent-systems.md","title":"Debugging Multi-Agent Systems: A Practical Guide","description":"Learn battle-tested techniques for debugging complex multi-agent AI systems, from session replay to distributed tracing.","date":"2024-02-10T00:00:00.000Z","tags":[{"inline":true,"label":"debugging","permalink":"/docusaurus-guide-3/blog/tags/debugging"},{"inline":true,"label":"multi-agent","permalink":"/docusaurus-guide-3/blog/tags/multi-agent"},{"inline":true,"label":"tutorial","permalink":"/docusaurus-guide-3/blog/tags/tutorial"},{"inline":true,"label":"best-practices","permalink":"/docusaurus-guide-3/blog/tags/best-practices"}],"readingTime":6.45,"hasTruncateMarker":true,"authors":[{"name":"Sarah Kim","title":"Lead Engineer at AgentOps","url":"https://github.com/sarahkim","image_url":"https://images.unsplash.com/photo-1494790108377-be9c29b29330?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1494790108377-be9c29b29330?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"debugging-multi-agent-systems","title":"Debugging Multi-Agent Systems: A Practical Guide","authors":[{"name":"Sarah Kim","title":"Lead Engineer at AgentOps","url":"https://github.com/sarahkim","image_url":"https://images.unsplash.com/photo-1494790108377-be9c29b29330?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1494790108377-be9c29b29330?w=150&h=150&fit=crop&crop=face"}],"tags":["debugging","multi-agent","tutorial","best-practices"],"description":"Learn battle-tested techniques for debugging complex multi-agent AI systems, from session replay to distributed tracing.","image":"https://images.unsplash.com/photo-1555949963-ff9fe0c870eb?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"The Complete Guide to LLM Cost Optimization for AI Agents","permalink":"/docusaurus-guide-3/blog/cost-optimization-llm-agents"},"nextItem":{"title":"The Future of AI Agents: Why Observability is Non-Negotiable","permalink":"/docusaurus-guide-3/blog/future-of-ai-agents"}},"content":"![Debugging Multi-Agent Systems](https://images.unsplash.com/photo-1555949963-ff9fe0c870eb?w=1200&h=400&fit=crop)\\n\\nMulti-agent systems are powerful but notoriously difficult to debug. When Agent A tells Agent B something, and Agent B misinterprets it, causing Agent C to fail... where do you even start?\\n\\nThis guide shares hard-won lessons from helping hundreds of teams debug their multi-agent architectures.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Unique Challenges of Multi-Agent Debugging\\n\\nUnlike traditional software, multi-agent systems introduce several debugging challenges:\\n\\n```mermaid\\nflowchart TD\\n    subgraph Challenges[\\"Multi-Agent Debugging Challenges\\"]\\n        A[Non-deterministic Behavior] --\x3e D[Difficult Reproduction]\\n        B[Emergent Interactions] --\x3e E[Unexpected Failures]\\n        C[Cascading Failures] --\x3e F[Root Cause Obscured]\\n        G[State Complexity] --\x3e H[Context Loss]\\n    end\\n\\n    style A fill:#EF4444,stroke:#DC2626,color:#fff\\n    style B fill:#F59E0B,stroke:#D97706,color:#fff\\n    style C fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n    style G fill:#0066FF,stroke:#3399FF,color:#fff\\n```\\n\\n### 1. Non-Deterministic Behavior\\n\\nLLMs are inherently non-deterministic. The same input can produce different outputs, making bugs hard to reproduce.\\n\\n### 2. Emergent Interactions\\n\\nBehaviors emerge from agent interactions that weren\'t explicitly programmed. These emergent behaviors can be beneficial\u2014or catastrophic.\\n\\n### 3. Cascading Failures\\n\\nOne agent\'s mistake propagates through the system, transforming a small error into a major failure.\\n\\n### 4. State Complexity\\n\\nWith multiple agents maintaining their own state, understanding the full system state at any point becomes challenging.\\n\\n## Setting Up for Success\\n\\nBefore diving into debugging techniques, let\'s set up proper instrumentation:\\n\\n```python\\nimport agentops\\nfrom agentops import track_agent, ActionEvent\\n\\n# Initialize with debugging-friendly settings\\nagentops.init(\\n    api_key=\\"your-api-key\\",\\n    default_tags=[\\"debug-mode\\"],\\n    instrument_llm_calls=True,\\n    auto_start_session=True\\n)\\n\\n@track_agent(name=\\"CoordinatorAgent\\")\\nclass CoordinatorAgent:\\n    def __init__(self):\\n        self.subordinates = {}\\n\\n    def delegate_task(self, task: str, agent_name: str):\\n        # Record the delegation for debugging\\n        agentops.record(ActionEvent(\\n            action_type=\\"delegation\\",\\n            params={\\n                \\"task\\": task,\\n                \\"delegated_to\\": agent_name,\\n                \\"coordinator_state\\": self.get_state_snapshot()\\n            }\\n        ))\\n        return self.subordinates[agent_name].execute(task)\\n\\n    def get_state_snapshot(self) -> dict:\\n        \\"\\"\\"Capture current state for debugging.\\"\\"\\"\\n        return {\\n            \\"active_tasks\\": self.active_tasks,\\n            \\"completed_tasks\\": self.completed_tasks,\\n            \\"subordinate_status\\": {\\n                name: agent.status\\n                for name, agent in self.subordinates.items()\\n            }\\n        }\\n```\\n\\n## The Debugging Toolkit\\n\\n### Technique 1: Session Replay\\n\\nThe most powerful debugging tool is being able to replay exactly what happened:\\n\\n```mermaid\\nsequenceDiagram\\n    participant U as User\\n    participant C as Coordinator\\n    participant R as Research Agent\\n    participant W as Writer Agent\\n\\n    U->>C: \\"Write a blog post about AI\\"\\n    Note over C: Session starts\\n    C->>R: \\"Research AI trends\\"\\n    R->>R: Search web\\n    R->>R: Analyze results\\n    R--\x3e>C: Research findings\\n    Note over R: Error: Timeout\\n    C->>W: \\"Write based on research\\"\\n    W->>W: Generate draft\\n    Note over W: Using incomplete data\\n    W--\x3e>C: Draft (low quality)\\n    C--\x3e>U: Final response\\n\\n    rect rgb(239, 68, 68)\\n        Note over R,W: Bug: Writer used incomplete research\\n    end\\n```\\n\\nWith AgentOps session replay, you can:\\n- See the exact sequence of events\\n- Inspect the data passed between agents\\n- Identify where things went wrong\\n- Understand the timing of operations\\n\\n### Technique 2: State Diffing\\n\\nCompare agent states before and after operations:\\n\\n```python\\nfrom agentops.debugging import StateDiffer\\n\\nclass DebuggableAgent:\\n    def __init__(self):\\n        self.state_history = []\\n\\n    def checkpoint_state(self, label: str):\\n        \\"\\"\\"Save state checkpoint for later comparison.\\"\\"\\"\\n        self.state_history.append({\\n            \\"label\\": label,\\n            \\"timestamp\\": datetime.now(),\\n            \\"state\\": copy.deepcopy(self.__dict__)\\n        })\\n\\n    def diff_states(self, label1: str, label2: str) -> dict:\\n        \\"\\"\\"Compare two state checkpoints.\\"\\"\\"\\n        state1 = next(s for s in self.state_history if s[\\"label\\"] == label1)\\n        state2 = next(s for s in self.state_history if s[\\"label\\"] == label2)\\n        return StateDiffer.diff(state1[\\"state\\"], state2[\\"state\\"])\\n```\\n\\n### Technique 3: Correlation IDs\\n\\nTrack requests across agent boundaries:\\n\\n```python\\nimport uuid\\nfrom contextvars import ContextVar\\n\\n# Global correlation ID for request tracking\\ncorrelation_id: ContextVar[str] = ContextVar(\'correlation_id\', default=\'\')\\n\\ndef start_request():\\n    \\"\\"\\"Initialize correlation ID for a new request.\\"\\"\\"\\n    request_id = str(uuid.uuid4())[:8]\\n    correlation_id.set(request_id)\\n    return request_id\\n\\nclass TracedAgent:\\n    def execute(self, task: str):\\n        cid = correlation_id.get()\\n        agentops.record(ActionEvent(\\n            action_type=\\"execution\\",\\n            params={\\n                \\"correlation_id\\": cid,\\n                \\"agent\\": self.name,\\n                \\"task\\": task\\n            }\\n        ))\\n        # ... execute task\\n```\\n\\nThis lets you trace a single user request through all agents:\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Request: abc123\\"\\n        direction TB\\n        A[Coordinator<br/>cid: abc123] --\x3e B[Research<br/>cid: abc123]\\n        A --\x3e C[Writer<br/>cid: abc123]\\n        B --\x3e D[Summarizer<br/>cid: abc123]\\n    end\\n\\n    style A fill:#0066FF,stroke:#3399FF,color:#fff\\n    style B fill:#22C55E,stroke:#16A34A,color:#fff\\n    style C fill:#F59E0B,stroke:#D97706,color:#fff\\n    style D fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n```\\n\\n### Technique 4: Error Boundary Agents\\n\\nWrap risky operations with error boundaries:\\n\\n```python\\nclass ErrorBoundaryAgent:\\n    \\"\\"\\"Wraps another agent with error handling and logging.\\"\\"\\"\\n\\n    def __init__(self, wrapped_agent, fallback_response=None):\\n        self.wrapped = wrapped_agent\\n        self.fallback = fallback_response\\n\\n    async def execute(self, task: str):\\n        try:\\n            with agentops.start_span(f\\"error_boundary_{self.wrapped.name}\\"):\\n                result = await self.wrapped.execute(task)\\n                return result\\n        except Exception as e:\\n            agentops.record(ActionEvent(\\n                action_type=\\"error_caught\\",\\n                params={\\n                    \\"agent\\": self.wrapped.name,\\n                    \\"error_type\\": type(e).__name__,\\n                    \\"error_message\\": str(e),\\n                    \\"task\\": task,\\n                    \\"stack_trace\\": traceback.format_exc()\\n                }\\n            ))\\n            if self.fallback:\\n                return self.fallback\\n            raise\\n```\\n\\n## Common Bugs and Solutions\\n\\n### Bug 1: The Infinite Loop\\n\\n**Symptom:** Agents keep calling each other indefinitely.\\n\\n```mermaid\\nflowchart LR\\n    A[Agent A] --\x3e|\\"Need help\\"| B[Agent B]\\n    B --\x3e|\\"Need clarification\\"| A\\n    A --\x3e|\\"Here\'s more context\\"| B\\n    B --\x3e|\\"Still unclear\\"| A\\n\\n    style A fill:#EF4444,stroke:#DC2626,color:#fff\\n    style B fill:#EF4444,stroke:#DC2626,color:#fff\\n```\\n\\n**Solution:** Implement depth limits and loop detection:\\n\\n```python\\nclass LoopProtectedAgent:\\n    MAX_DEPTH = 5\\n\\n    def execute(self, task: str, depth: int = 0):\\n        if depth >= self.MAX_DEPTH:\\n            agentops.record(ActionEvent(\\n                action_type=\\"loop_detected\\",\\n                params={\\"depth\\": depth, \\"task\\": task}\\n            ))\\n            return \\"Maximum recursion depth reached\\"\\n\\n        # Pass depth to any sub-agents\\n        return self.delegate(task, depth=depth + 1)\\n```\\n\\n### Bug 2: The Silent Failure\\n\\n**Symptom:** Agents return empty or default responses without errors.\\n\\n**Solution:** Validate outputs at every step:\\n\\n```python\\ndef validate_output(output, agent_name: str, expected_type: type):\\n    \\"\\"\\"Validate agent output and log issues.\\"\\"\\"\\n    if output is None:\\n        agentops.record(ActionEvent(\\n            action_type=\\"validation_failed\\",\\n            params={\\n                \\"agent\\": agent_name,\\n                \\"issue\\": \\"null_output\\"\\n            }\\n        ))\\n        raise ValueError(f\\"{agent_name} returned null output\\")\\n\\n    if not isinstance(output, expected_type):\\n        agentops.record(ActionEvent(\\n            action_type=\\"validation_failed\\",\\n            params={\\n                \\"agent\\": agent_name,\\n                \\"expected\\": expected_type.__name__,\\n                \\"actual\\": type(output).__name__\\n            }\\n        ))\\n        raise TypeError(f\\"{agent_name} returned wrong type\\")\\n\\n    if hasattr(output, \'__len__\') and len(output) == 0:\\n        agentops.record(ActionEvent(\\n            action_type=\\"validation_warning\\",\\n            params={\\n                \\"agent\\": agent_name,\\n                \\"issue\\": \\"empty_output\\"\\n            }\\n        ))\\n```\\n\\n### Bug 3: The Context Loss\\n\\n**Symptom:** Agent responses don\'t match the conversation context.\\n\\n**Solution:** Implement context checksums:\\n\\n```python\\nimport hashlib\\n\\nclass ContextAwareAgent:\\n    def create_context_hash(self, context: dict) -> str:\\n        \\"\\"\\"Create hash of context for verification.\\"\\"\\"\\n        context_str = json.dumps(context, sort_keys=True)\\n        return hashlib.sha256(context_str.encode()).hexdigest()[:8]\\n\\n    def execute_with_context(self, task: str, context: dict):\\n        context_hash = self.create_context_hash(context)\\n\\n        agentops.record(ActionEvent(\\n            action_type=\\"context_received\\",\\n            params={\\n                \\"context_hash\\": context_hash,\\n                \\"context_keys\\": list(context.keys())\\n            }\\n        ))\\n\\n        result = self.execute(task, context)\\n\\n        # Verify context wasn\'t corrupted\\n        assert self.create_context_hash(context) == context_hash, \\\\\\n            \\"Context was modified during execution!\\"\\n\\n        return result\\n```\\n\\n## Building a Debug Dashboard\\n\\nHere\'s a simple dashboard approach for real-time debugging:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"Debug Dashboard\\"\\n        direction TB\\n        A[Live Event Stream] --\x3e B[Filter Panel]\\n        B --\x3e C[Event Details]\\n        B --\x3e D[Agent Timeline]\\n        B --\x3e E[Error Highlights]\\n\\n        F[Session Browser] --\x3e G[Replay Controls]\\n        G --\x3e H[Step Forward]\\n        G --\x3e I[Step Back]\\n        G --\x3e J[Jump to Error]\\n\\n        K[Metrics Panel] --\x3e L[Latency Graph]\\n        K --\x3e M[Error Rate]\\n        K --\x3e N[Token Usage]\\n    end\\n\\n    style A fill:#0066FF,stroke:#3399FF,color:#fff\\n    style F fill:#22C55E,stroke:#16A34A,color:#fff\\n    style K fill:#F59E0B,stroke:#D97706,color:#fff\\n```\\n\\n## Best Practices Checklist\\n\\nBefore deploying multi-agent systems, ensure you have:\\n\\n- [ ] **Unique IDs** for every agent instance\\n- [ ] **Correlation IDs** for request tracing\\n- [ ] **State checkpoints** at critical points\\n- [ ] **Output validation** for all agent responses\\n- [ ] **Timeout handling** for all async operations\\n- [ ] **Loop protection** with depth limits\\n- [ ] **Error boundaries** around risky operations\\n- [ ] **Comprehensive logging** with structured data\\n\\n## Conclusion\\n\\nDebugging multi-agent systems requires a different mindset than traditional software debugging. The key is comprehensive observability\u2014you need to see everything that happens, when it happens, and how agents interact.\\n\\nWith the right instrumentation and tools, even the most complex multi-agent bugs become tractable. The investment in debugging infrastructure pays dividends every time you avoid a production incident.\\n\\n---\\n\\n*Need help debugging your multi-agent system? [Try AgentOps free](/docs/getting-started/quickstart) and get full visibility into your agent interactions.*"},{"id":"future-of-ai-agents","metadata":{"permalink":"/docusaurus-guide-3/blog/future-of-ai-agents","editUrl":"https://github.com/agentops/agentops-docs/tree/main/blog/2024-01-15-future-of-ai-agents.md","source":"@site/blog/2024-01-15-future-of-ai-agents.md","title":"The Future of AI Agents: Why Observability is Non-Negotiable","description":"Explore why observability has become the cornerstone of successful AI agent deployments and what it means for the future of autonomous systems.","date":"2024-01-15T00:00:00.000Z","tags":[{"inline":true,"label":"ai-agents","permalink":"/docusaurus-guide-3/blog/tags/ai-agents"},{"inline":true,"label":"observability","permalink":"/docusaurus-guide-3/blog/tags/observability"},{"inline":true,"label":"future","permalink":"/docusaurus-guide-3/blog/tags/future"},{"inline":true,"label":"trends","permalink":"/docusaurus-guide-3/blog/tags/trends"}],"readingTime":4,"hasTruncateMarker":true,"authors":[{"name":"Alex Chen","title":"Co-founder & CEO at AgentOps","url":"https://github.com/alexchen","image_url":"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=150&h=150&fit=crop&crop=face","socials":{},"key":null,"page":null}],"frontMatter":{"slug":"future-of-ai-agents","title":"The Future of AI Agents: Why Observability is Non-Negotiable","authors":[{"name":"Alex Chen","title":"Co-founder & CEO at AgentOps","url":"https://github.com/alexchen","image_url":"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=150&h=150&fit=crop&crop=face","imageURL":"https://images.unsplash.com/photo-1507003211169-0a1dd7228f2d?w=150&h=150&fit=crop&crop=face"}],"tags":["ai-agents","observability","future","trends"],"description":"Explore why observability has become the cornerstone of successful AI agent deployments and what it means for the future of autonomous systems.","image":"https://images.unsplash.com/photo-1677442136019-21780ecad995?w=1200&h=630&fit=crop"},"unlisted":false,"prevItem":{"title":"Debugging Multi-Agent Systems: A Practical Guide","permalink":"/docusaurus-guide-3/blog/debugging-multi-agent-systems"}},"content":"![AI Agents Future](https://images.unsplash.com/photo-1677442136019-21780ecad995?w=1200&h=400&fit=crop)\\n\\nThe landscape of AI development has shifted dramatically. What once seemed like science fiction\u2014autonomous AI agents making decisions, executing tasks, and collaborating with each other\u2014is now our reality. But with this power comes unprecedented complexity.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Rise of Autonomous Systems\\n\\nAI agents are no longer simple chatbots responding to predefined prompts. Today\'s agents are sophisticated systems capable of:\\n\\n- **Multi-step reasoning** across complex problem domains\\n- **Tool usage** including web browsing, code execution, and API calls\\n- **Memory and context management** across extended interactions\\n- **Collaboration** with other agents in multi-agent architectures\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"Modern AI Agent Architecture\\"\\n        U[User Request] --\x3e O[Orchestrator Agent]\\n        O --\x3e R[Research Agent]\\n        O --\x3e C[Code Agent]\\n        O --\x3e A[Analysis Agent]\\n        R --\x3e K[(Knowledge Base)]\\n        C --\x3e E[Execution Environment]\\n        A --\x3e D[(Data Store)]\\n        R --\x3e O\\n        C --\x3e O\\n        A --\x3e O\\n        O --\x3e F[Final Response]\\n    end\\n\\n    style O fill:#0066FF,stroke:#3399FF,color:#fff\\n    style R fill:#22C55E,stroke:#16A34A,color:#fff\\n    style C fill:#F59E0B,stroke:#D97706,color:#fff\\n    style A fill:#8B5CF6,stroke:#7C3AED,color:#fff\\n```\\n\\n## The Observability Imperative\\n\\nWith this complexity comes a critical challenge: **how do you understand what your agent is actually doing?**\\n\\nTraditional logging falls short. You need:\\n\\n1. **Real-time visibility** into every decision point\\n2. **Cost tracking** across multiple LLM providers\\n3. **Performance metrics** that reveal bottlenecks\\n4. **Replay capabilities** for debugging production issues\\n\\n> \\"You can\'t improve what you can\'t measure. With AI agents, measurement isn\'t just about performance\u2014it\'s about understanding intent, detecting drift, and ensuring alignment.\\" \u2014 AI Researcher, Stanford HAI\\n\\n## The Hidden Costs of Blind Spots\\n\\nOrganizations deploying AI agents without proper observability face significant risks:\\n\\n| Risk Category | Impact | Probability |\\n|--------------|--------|-------------|\\n| Runaway Costs | High | Very High |\\n| Data Leakage | Critical | Medium |\\n| Hallucination Propagation | High | High |\\n| Performance Degradation | Medium | Very High |\\n| Compliance Violations | Critical | Medium |\\n\\n### Real-World Example: The $50,000 Bug\\n\\nOne of our early customers discovered their agent had been making redundant API calls due to a caching bug. Without observability, this went unnoticed for two weeks.\\n\\n**The result?** Over $50,000 in unnecessary API costs.\\n\\nWith AgentOps, they would have caught this in the first hour through our anomaly detection system.\\n\\n## Building Observable Agents\\n\\nHere\'s how to build observability into your agent architecture from day one:\\n\\n```python\\nimport agentops\\nfrom agentops.decorators import track_agent, record_action\\n\\n# Initialize with comprehensive tracking\\nagentops.init(\\n    api_key=\\"your-api-key\\",\\n    default_tags=[\\"production\\", \\"customer-facing\\"],\\n    auto_start_session=True\\n)\\n\\n@track_agent(name=\\"ResearchAgent\\")\\nclass ResearchAgent:\\n    \\"\\"\\"An agent that researches topics and synthesizes information.\\"\\"\\"\\n\\n    @record_action(action_type=\\"search\\")\\n    async def search(self, query: str) -> list[dict]:\\n        \\"\\"\\"Search for relevant information.\\"\\"\\"\\n        # AgentOps automatically tracks:\\n        # - Execution time\\n        # - Input/output data\\n        # - Any LLM calls made\\n        results = await self.search_engine.query(query)\\n        return results\\n\\n    @record_action(action_type=\\"synthesize\\")\\n    async def synthesize(self, sources: list[dict]) -> str:\\n        \\"\\"\\"Synthesize information from multiple sources.\\"\\"\\"\\n        response = await self.llm.generate(\\n            prompt=self.build_synthesis_prompt(sources)\\n        )\\n        return response.content\\n```\\n\\n## The Three Pillars of Agent Observability\\n\\n```mermaid\\nmindmap\\n  root((Agent Observability))\\n    Monitoring\\n      Real-time metrics\\n      Alert systems\\n      Health checks\\n      SLA tracking\\n    Debugging\\n      Session replay\\n      Step tracing\\n      Error analysis\\n      Root cause detection\\n    Optimization\\n      Cost analysis\\n      Performance profiling\\n      Bottleneck identification\\n      A/B testing\\n```\\n\\n### 1. Comprehensive Monitoring\\n\\nEvery agent interaction should be captured:\\n- LLM calls with full prompt/response logging\\n- Tool invocations with parameters and results\\n- Decision points with reasoning traces\\n- Error states with full context\\n\\n### 2. Intelligent Debugging\\n\\nWhen things go wrong (and they will), you need:\\n- Full session replay with timeline view\\n- Ability to filter by error type, agent, or time range\\n- Correlation of events across distributed systems\\n- Export capabilities for offline analysis\\n\\n### 3. Continuous Optimization\\n\\nUse observability data to:\\n- Identify the most expensive operations\\n- Find opportunities for caching\\n- Optimize prompt engineering\\n- Balance cost vs. quality tradeoffs\\n\\n## Looking Ahead: The Observable Agent Ecosystem\\n\\nThe future of AI agents is inherently observable. We\'re building toward a world where:\\n\\n- **Every agent decision is traceable** back to its inputs and reasoning\\n- **Costs are predictable** through intelligent budgeting and forecasting\\n- **Failures are rare** because issues are caught early through anomaly detection\\n- **Collaboration is seamless** because agent interactions are fully transparent\\n\\n```mermaid\\ntimeline\\n    title Evolution of AI Agent Observability\\n    2022 : Basic logging\\n         : Manual monitoring\\n    2023 : Structured tracing\\n         : Cost tracking\\n         : Session recording\\n    2024 : Real-time analytics\\n         : Anomaly detection\\n         : Multi-agent tracing\\n    2025 : Predictive optimization\\n         : Autonomous remediation\\n         : Cross-system correlation\\n```\\n\\n## Getting Started Today\\n\\nThe best time to implement observability is before you need it. Start with these steps:\\n\\n1. **Instrument your agents** with AgentOps SDK\\n2. **Define your metrics** that matter most\\n3. **Set up alerts** for anomalous behavior\\n4. **Establish baselines** for normal operation\\n5. **Review regularly** and iterate on your approach\\n\\n---\\n\\nReady to make your AI agents observable? [Get started with AgentOps](/docs/getting-started/introduction) in less than 5 minutes.\\n\\n*Have questions or want to share your agent observability journey? Join our [Discord community](https://discord.gg/agentops)!*"}]}}')}}]);